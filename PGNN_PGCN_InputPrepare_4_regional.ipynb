{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f78a61ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import pymetis\n",
    "import networkx as nx\n",
    "import time\n",
    "from networkx.algorithms import community\n",
    "from random import shuffle\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric as tg\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch.nn import init\n",
    "import pdb\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "from torch_geometric.data import Data\n",
    "import torch.optim as optim\n",
    "import pywt\n",
    "from scipy.stats import norm\n",
    "import scipy.interpolate as interp\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49569c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data(file_path):\n",
    "    file = open(file_path,\"rb\")\n",
    "    raw_data = pickle.load(file)\n",
    "    \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6de39fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "    # 查看文件中所有的数据集名称\n",
    "        dataset_names = list(f.keys())\n",
    "        print(\"Datasets in the file:\", dataset_names)\n",
    "        data_dict = {}\n",
    "        for name in dataset_names:\n",
    "            if f[name].shape == (): \n",
    "                data = f[name][()] \n",
    "            else:\n",
    "                data = f[name][:]\n",
    "        \n",
    "            data_dict[name] = data\n",
    "            \n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4937dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renumber_subgraph(nodes, edge_index):\n",
    "    unique_nodes = torch.unique(nodes, sorted = True)\n",
    "    new_node_ids = torch.arange(len(unique_nodes))\n",
    "    node_mapping = {old_id.item(): new_id.item() for old_id, new_id in zip(unique_nodes, new_node_ids)}\n",
    "    \n",
    "    new_edge_index = torch.tensor([\n",
    "        [node_mapping[edge_index[0, i].item()], node_mapping[edge_index[1, i].item()]]\n",
    "        for i in range(edge_index.size(1))\n",
    "    ]).t()\n",
    "    \n",
    "    return new_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3237c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgraph_information(city_names, root_path):\n",
    "    edge_pair_dictionary = {}\n",
    "    max_ach_num = 0\n",
    "    max_subgraph_node_num = 0 \n",
    "    \n",
    "    for name in city_names:\n",
    "        edge_pairs = []\n",
    "        dirs = os.listdir(root_path + name + \"/edge_pair/\")\n",
    "        \n",
    "        #-------------获取每个字图的edge pairs----------\n",
    "        edge_pairs = [torch.tensor(open_data(root_path + name + \"/edge_pair/\" + each_file),dtype=torch.long) for each_file in dirs]\n",
    "        \n",
    "        #------------获取每个子图的节点-----------------\n",
    "        comm_node_list = [torch.unique(edge_pair, sorted = True) for edge_pair in edge_pairs]\n",
    "        edge_pair_dictionary[name + \"_nodes\"] = comm_node_list\n",
    "        \n",
    "        #------------获取每个子图的edge pairs并且新编号--------\n",
    "        renumbered_edge_pairs = [renumber_subgraph(comm_node_list[i], edge_pairs[i]) for i in range(len(edge_pairs))]\n",
    "        edge_pair_dictionary[name + \"_edge_pair\"] = renumbered_edge_pairs \n",
    "        \n",
    "        #--------------计算子图的节点数-----------------\n",
    "        graph_num = [sub_graph_num.shape[0] for sub_graph_num in comm_node_list]\n",
    "        edge_pair_dictionary[name + \"_subgraph_node_num\"] = graph_num\n",
    "        \n",
    "        #---------Calculate the anchor set num of each comm----------------\n",
    "        ach_set_nums = [int(0.5*int(np.log2(node_num.shape[0])))* int(np.log2(node_num.shape[0]))\n",
    "                        for node_num in comm_node_list]\n",
    "        edge_pair_dictionary[name + \"_anchor_set_num\"] = ach_set_nums\n",
    "        \n",
    "        #---------获取每个城市的节点数-----------------\n",
    "        city_node_num = sum([comm_node.shape[0] for comm_node in comm_node_list])\n",
    "        edge_pair_dictionary[name + \"_city_node_num\"] = city_node_num\n",
    "        \n",
    "        if max_ach_num <= max(ach_set_nums):\n",
    "            max_ach_num = max(ach_set_nums)\n",
    "        if max_subgraph_node_num <= max(graph_num):\n",
    "            max_subgraph_node_num = max(graph_num)\n",
    "        \n",
    "    return edge_pair_dictionary, max_ach_num, max_subgraph_node_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c15463c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomData(Data):\n",
    "    def __init__(self, trend, period, target_volume, target_label, edge_pairs, subgraph_node_num, subgraph_nodes, city_node_num, dist_max, dist_argmax):\n",
    "        super(CustomData, self).__init__()\n",
    "        self.trend = trend\n",
    "        self.period = period\n",
    "        self.target_volume = target_volume\n",
    "        self.target_label = target_label\n",
    "        self.edge_pairs = edge_pairs\n",
    "        self.subgraph_node_num = subgraph_node_num\n",
    "        self.subgraph_nodes = subgraph_nodes\n",
    "        self.city_node_num = city_node_num\n",
    "        self.dist_max = dist_max\n",
    "        self.dist_argmax = dist_argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89ac8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_data_prepare(city_names, root_path, purpose):\n",
    "    keys = ['period_template', 'target_template', 'trend_template']\n",
    "    \n",
    "    edge_pair_dictionary, _, max_subgraph_node_num = subgraph_information(city_names, root_path)\n",
    "    \n",
    "    for i in range(len(city_names)):\n",
    "        data_list = []\n",
    "        city_dict = read_data(root_path + city_names[i] + f\"/input_target/{purpose}.h5\")\n",
    "        dist_dict = open_data(root_path + city_names[i] + \"/input_target/dist_dictionary.h5\")\n",
    "        \n",
    "        dist_max = dist_dict['dist_max']\n",
    "        dist_argmax = dist_dict['dist_argmax'].to(torch.int)\n",
    "        edge_pairs = edge_pair_dictionary[city_names[i] + \"_edge_pair\"]\n",
    "        subgraph_node_num = edge_pair_dictionary[city_names[i] + \"_subgraph_node_num\"]\n",
    "        subgraph_nodes = edge_pair_dictionary[city_names[i] + \"_nodes\"]\n",
    "        city_node_num = edge_pair_dictionary[city_names[i] + \"_city_node_num\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "        for j in range(city_dict[\"period_template\"].shape[0]):\n",
    "            trend = torch.tensor(city_dict[\"trend_template\"][j, :, :, :6],dtype=torch.float)\n",
    "            period = torch.tensor(city_dict[\"period_template\"][j, :, :, :6],dtype=torch.float)\n",
    "            targets = torch.tensor(city_dict[\"target_template\"][j, :, : , :],dtype=torch.float)\n",
    "            #target_label = torch.tensor(city_dict[\"target_template\"][j, :, :, 5],dtype=torch.float).unsqueeze(-1)\n",
    "            \n",
    "            \n",
    "            #-------------First we process trend and period------------------\n",
    "            input_indices = [torch.isin(trend[:, :, 0], graph_node).unsqueeze(-1) for graph_node in subgraph_nodes]\n",
    "            target_indices = [torch.isin(targets[:, :, 0], graph_node).unsqueeze(-1) for graph_node in subgraph_nodes]\n",
    "            \n",
    "            \n",
    "            \n",
    "            for k in range(len(input_indices)):\n",
    "                #-------------trend-------------------------\n",
    "                trend_k = torch.zeros((trend.shape[0], max_subgraph_node_num, 4))\n",
    "                trend_k[:,:subgraph_node_num[k],:] = trend[:,:,1:5][input_indices[k].expand_as(trend[:,:,1:5])].view(trend.shape[0], -1, 4)\n",
    "                #-------------period------------------------\n",
    "                period_k = torch.zeros_like(trend_k)\n",
    "                period_k[:,:subgraph_node_num[k],:] = period[:,:,1:5][input_indices[k].expand_as(period[:,:,1:5])].view(period.shape[0], -1, 4)\n",
    "                \n",
    "                #-------------targets---------------------\n",
    "                target_vol_k = torch.zeros((targets.shape[0], max_subgraph_node_num, 4))\n",
    "                target_vol_k[:,:subgraph_node_num[k],:] = targets[:,:,1:5][target_indices[k].expand_as(targets[:,:,1:5])].view(targets.shape[0], -1, 4)\n",
    "                #------------------------------------------\n",
    "                target_label_k = torch.zeros((trend.shape[0],max_subgraph_node_num, 1))\n",
    "                #target_label_k = torch.zeros((targets.shape[0], max_subgraph_node_num, 1))\n",
    "                temporal_tar = trend[:,:,5].unsqueeze(-1)[input_indices[k]].view(trend.shape[0], -1, 1)\n",
    "                target_label_k[:,:subgraph_node_num[k],:] = temporal_tar\n",
    "                #temporal_tar = targets[:,:,5].unsqueeze(-1)\n",
    "                #target_label_k[:,:subgraph_node_num[k],:] = temporal_tar[target_indices[k]].view(targets.shape[0], -1, 1)\n",
    "                #-------------------------------------------\n",
    "   \n",
    "            \n",
    "                custom_data = CustomData(trend_k, period_k, target_vol_k, target_label_k, edge_pairs[k], subgraph_node_num[k],\n",
    "                                     subgraph_nodes[k], city_node_num, dist_max[k,:,:,:], dist_argmax[k,:,:,:])\n",
    "            \n",
    "                data_list.append(custom_data)\n",
    "                \n",
    "\n",
    "        print(f\"{city_names[i]} data is prepared\")\n",
    "            \n",
    "        shuffle(data_list)\n",
    "        torch.save(data_list, root_path + city_names[i] + f'/input_target/{purpose}_regional_level.pt')\n",
    "    print('done')\n",
    "    \n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffbd4ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_names = [\"Barcelona\",\"Antwerp\"]\n",
    "root_path = \"D:/ThesisData/processed data/\"\n",
    "purpose = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4265aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets in the file: ['period_template', 'target_template', 'trend_template']\n",
      "Barcelona data is prepared\n",
      "Datasets in the file: ['period_template', 'target_template', 'trend_template']\n",
      "Antwerp data is prepared\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "data_list = source_data_prepare(city_names, root_path, purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b44beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
