{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1d7b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import pymetis\n",
    "import networkx as nx\n",
    "import time\n",
    "from networkx.algorithms import community\n",
    "from random import shuffle\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric as tg\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch.nn import init\n",
    "import pdb\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch_geometric.data import Data\n",
    "import torch.optim as optim\n",
    "import pywt\n",
    "from scipy.stats import norm\n",
    "from IPython.display import clear_output\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ced79a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data(file_path):\n",
    "    file = open(file_path,\"rb\")\n",
    "    raw_data = pickle.load(file)  \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "005261bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearOperate(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen):\n",
    "        super(NonLinearOperate, self).__init__()  #类NonLinearLayer继承父类nn.Module的初始化方法\n",
    "        self.layer_1 = nn.Linear(input_dimen, hidden_dimen)\n",
    "        self.layer_2 = nn.Linear(hidden_dimen, output_dimen)\n",
    "        self.acti_func = nn.ReLU()\n",
    "        for m in self.modules():#遍历所有子模块\n",
    "#Check if each sub-module is an example of the class nn.Linear\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))#Use Xavier initialization\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = init.constant_(m.bias.data, 0.0)\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.acti_func(x)\n",
    "        x = self.layer_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "958888b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGNN_Layer(nn.Module):\n",
    "    def __init__(self, input_dimen, output_dimen, max_ach_num):\n",
    "        super(PGNN_Layer, self).__init__()\n",
    "        self.input_dimen = input_dimen\n",
    "        self.output_dimen = output_dimen\n",
    "        self.distance_calculate = NonLinearOperate(1, output_dimen, 1)\n",
    "        self.acti_func = nn.ReLU()\n",
    "        self.linear_hidden = nn.Linear(2*input_dimen, output_dimen)\n",
    "        self.out_transition = nn.Linear(output_dimen,1)\n",
    "        self.linear_out_position = nn.Linear(max_ach_num,input_dimen)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))#Use Xavier initialization\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = init.constant_(m.bias.data, 0.0)                \n",
    "                    \n",
    "    def forward(self, node_features, dists_max, dists_argmax):\n",
    "        dists_max = self.distance_calculate(dists_max.unsqueeze(-1)).squeeze()\n",
    "        subset_features = node_features[dists_argmax.flatten(), :]\n",
    "        subset_features = subset_features.reshape(dists_argmax.shape[0], dists_argmax.shape[1], subset_features.shape[1])\n",
    "        messages = subset_features * dists_max.unsqueeze(-1)\n",
    "        feature_self = node_features.unsqueeze(1).repeat(1, dists_max.shape[1],1)\n",
    "        messages = torch.concat((messages, feature_self), dim = -1) #N行M列D维\n",
    "        messages = self.linear_hidden(messages).squeeze()#将输出维度改为Output Dimen,即n*m*output_dimen\n",
    "        messages = self.acti_func(messages) \n",
    "        output_transition = self.out_transition(messages).squeeze(-1) #n * m * output_dimen to n * m\n",
    "        output_position = self.linear_out_position(output_transition)\n",
    "        output_structure = torch.mean(messages, dim=1)#n*output_dimen\n",
    "        \n",
    "        return output_position, output_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "770a8458",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGNN(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen, max_ach_num, layer_num = 1, drop_out = True):\n",
    "        super(PGNN, self).__init__()\n",
    "        self.drop_out = drop_out\n",
    "        self.layer_num = layer_num\n",
    "        self.input_layer = nn.Linear(input_dimen, hidden_dimen)\n",
    "        self.last_layer = nn.Linear(hidden_dimen, input_dimen)\n",
    "        self.max_ach_num = max_ach_num\n",
    "        if self.layer_num == 1:\n",
    "            self.gnn_operate_1 = PGNN_Layer(hidden_dimen, output_dimen, max_ach_num)#输出维度是node_num * hidden_dimen\n",
    "            \n",
    "        if self.layer_num > 1:\n",
    "            self.gnn_hidden = nn.ModuleList([PGNN_Layer(hidden_dimen, hidden_dimen, max_ach_num) for i in range(0, layer_num)])\n",
    "            self.gnn_output_layer = PGNN_Layer(hidden_dimen, output_dimen)\n",
    "                \n",
    "        \n",
    "    def forward(self, x, dist_max_sets, dist_argmax_sets):\n",
    "        \n",
    "        x = self.input_layer(x)\n",
    "        if self.layer_num == 1:\n",
    "            x_position, x = self.gnn_operate_1(x, dist_max_sets[0,:,:], dist_argmax_sets[0,:,:])\n",
    "            if self.drop_out:\n",
    "                x = F.dropout(x, training=self.training)\n",
    "            x_position = self.last_layer(x_position)\n",
    "            #print(f\"pgnn output_layer size: {x_position.shape}\")\n",
    "            return x_position\n",
    "    \n",
    "        if self.layer_num > 1:\n",
    "            for i in range(self.layer_num):\n",
    "                _, x = self.gnn_hidden[i](x, dist_max_sets[i,:,:], dist_argmax_sets[i,:,:])\n",
    "                if self.drop_out:\n",
    "                    x = F.dropout(x, training=self.training)\n",
    "                  \n",
    "            _ = F.normalize(_, p=2, dim=-1)\n",
    "            x_position = self.last_layer(_)    \n",
    "            \n",
    "            \n",
    "            return x_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61361329",
   "metadata": {},
   "outputs": [],
   "source": [
    "class P_GCN(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen, max_ach_num, layer_num = 1, drop_out = True):\n",
    "        super(P_GCN, self).__init__()\n",
    "        self.max_ach_num = max_ach_num\n",
    "        self.layer_num = layer_num\n",
    "        self.drop_out = drop_out\n",
    "        self.input_layer = nn.Linear(input_dimen, hidden_dimen)\n",
    "        self.p_gcn_block = nn.Sequential(PGNN_Layer(hidden_dimen, hidden_dimen, max_ach_num), \n",
    "                                         GCNConv(hidden_dimen, hidden_dimen, add_self_loops=True))\n",
    "        self.acti_func = nn.ReLU()   \n",
    "        \n",
    "        if layer_num == 1:\n",
    "            self.gcn_p_layers = self.p_gcn_block\n",
    "        if layer_num > 1:\n",
    "            self.gcn_p_layers = nn.ModuleList([self.p_gcn_block for i in range(num_layer)])\n",
    "            \n",
    "        self.output_layer = nn.Linear(hidden_dimen, output_dimen)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = init.constant_(m.bias.data, 0.0)   \n",
    "        \n",
    "        \n",
    "    def forward(self, x, edge_index, dist_max, dist_argmax):#GCN_P_input shape: node_num, 4\n",
    "        \n",
    "        x_ = self.input_layer(x)\n",
    "        \n",
    "        if self.layer_num == 1:\n",
    "            x_position, _ = self.gcn_p_layers[0](x_, dist_max[0,:,:], dist_argmax[0,:,:])\n",
    "            if self.drop_out:\n",
    "                x_position = F.dropout(x_position, training=self.training)\n",
    "            x = self.gcn_p_layers[1](x_position, edge_index)\n",
    "            if self.drop_out:\n",
    "                x = F.dropout(x, training=self.training)\n",
    "            x = self.acti_func(x + x_)\n",
    "            \n",
    "        else:\n",
    "            for i in range(self.layer_num):\n",
    "                x_position, _ = self.gcn_p_layers[i][0](x_, dist_max[i,:,:], dist_argmax[i,:,:])\n",
    "                if self.drop_out:\n",
    "                    x_position = F.dropout(x_position, training=self.training)\n",
    "                x = self.gcn_p_layers[i][1](x_position, edge_index)\n",
    "                if self.drop_out:\n",
    "                    x = F.dropout(x, training=self.training)\n",
    "                x = self.acti_func(x + x_)\n",
    "                x_ = x\n",
    "                    \n",
    "        x = self.acti_func(self.output_layer(x))\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bfc4a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入数据shape: node_num, 4, time_step(20)\n",
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels_1, hidden_channels_2, out_channels):\n",
    "        super(CNN_1D, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = input_channels, out_channels = hidden_channels_1, kernel_size = 3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_channels_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels = hidden_channels_1, out_channels = hidden_channels_2, kernel_size = 3, padding=0),\n",
    "            nn.BatchNorm1d(hidden_channels_2),\n",
    "            nn.ReLU(), #len: 18\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),#(18-2)/2 +1 =9\n",
    "            nn.ConvTranspose1d(in_channels= hidden_channels_2,\n",
    "                               out_channels=out_channels,\n",
    "                               kernel_size=4,\n",
    "                               stride= 2, \n",
    "                               padding=0))\n",
    "\n",
    "        self.fc1 = nn.Linear(out_channels, 4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        #output shape: change from batch_num, out_channels, t-step to batch_num, t-step, out_channels\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = self.fc1(out)\n",
    "        #output shape: batch_num, t-step, 4\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "248cd318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, pred_len, output_dimen = 4, num_layers = 2):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_dimen = hidden_dimen\n",
    "        self.output_dimen = output_dimen\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_layer_1 = nn.Linear(input_dimen, hidden_dimen)\n",
    "        self.gru_layers = nn.GRU(hidden_dimen, hidden_dimen, num_layers, batch_first = True)\n",
    "        self.linear_layer_2 = nn.Linear(hidden_dimen, output_dimen)\n",
    "        self.k = pred_len #Futre k-step to predict \n",
    "        \n",
    "         \n",
    "    #gru输入格式：node_num, t-steps, hidden_dimen\n",
    "    def forward(self, pgnn_t_step_outs, extractor_outputs):  \n",
    "        x = torch.cat((pgnn_t_step_outs, extractor_outputs), dim = -1)\n",
    "        #x shape: node_num, t-steps, 2*4\n",
    "        batch_size, seq_len, feat_dim = x.size()\n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_dimen)\n",
    "        x = self.linear_layer_1(x)\n",
    "        outputs, _ = self.gru_layers(x, h_0)#outputs shape: batch_size, seq_len, hidden dim. \n",
    "        layer_2_input = outputs[:, -self.k:, :]\n",
    "        x = self.linear_layer_2(layer_2_input) #x shape: batch_size, k steps, 4.\n",
    "        #linear_layer_2 trasnfer last dimension from hidden dim to 4\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1b5730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T_Step_PGNN(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen, max_ach_num, layer_num = 1):\n",
    "        super(T_Step_PGNN, self).__init__()\n",
    "        self.pgnn_model = PGNN(input_dimen, hidden_dimen, output_dimen, max_ach_num)\n",
    "        \n",
    "    def forward(self, t_step_inputs, subgraph_nodes, dist_max, dist_argmax): #t-step-inputs shape: 20, max_subgraph_nodenumber,4\n",
    "        pgnn_template = torch.zeros((t_step_inputs.shape[0], t_step_inputs.shape[1], t_step_inputs.shape[2]))\n",
    "        \n",
    "        pgnn_outputs = torch.empty((0,subgraph_nodes.shape[0],t_step_inputs.shape[2]))\n",
    "        subgraph_node_num = subgraph_nodes.shape[0]\n",
    "            \n",
    "        for t in range(t_step_inputs.shape[0]):\n",
    "            pgnn_t_step = self.pgnn_model(t_step_inputs[t,:subgraph_node_num,:], dist_max[:,:,:], dist_argmax[:,:,:])\n",
    "            pgnn_outputs = torch.cat((pgnn_outputs, pgnn_t_step.unsqueeze(0)), dim=0)\n",
    "        \n",
    "        pgnn_template[:,:subgraph_node_num,:] = pgnn_outputs\n",
    "        \n",
    "        return pgnn_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f5886a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Extractor(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen, max_ach_num,\n",
    "                input_channels, hidden_channels_1, hidden_channels_2, out_channels, layer_num = 1):\n",
    "        super(Feature_Extractor, self).__init__()\n",
    "        self.pgcn_model = P_GCN(input_dimen, hidden_dimen, output_dimen, max_ach_num)\n",
    "        self.cnn_1D = CNN_1D(input_channels, hidden_channels_1, hidden_channels_2, out_channels)\n",
    "        \n",
    "        \n",
    "    def forward(self, t_step_inputs, edge_index, subgraph_nodes, dist_max, dist_argmax): \n",
    "        pgcn_template = torch.zeros((t_step_inputs.shape[0], t_step_inputs.shape[1], t_step_inputs.shape[2]))\n",
    "       \n",
    "        pgcn_outputs = torch.empty((0,subgraph_nodes.shape[0],t_step_inputs.shape[2]))\n",
    "        subgraph_node_num = subgraph_nodes.shape[0]\n",
    "            \n",
    "            \n",
    "        for t in range(t_step_inputs.shape[0]):\n",
    "            pgcn_t_step = self.pgcn_model(t_step_inputs[t,:subgraph_node_num,:], edge_index, dist_max[:,:,:], dist_argmax[:,:,:])\n",
    "            pgcn_outputs = torch.cat((pgcn_outputs, pgcn_t_step.unsqueeze(0)), dim=0)\n",
    "            \n",
    "        pgcn_template[:,:subgraph_node_num,:] = pgcn_outputs\n",
    "            \n",
    "            \n",
    "        #shape changed as: node_num, 4, time_step\n",
    "        pgcn_template =pgcn_template.permute(1,2,0)\n",
    "        extractor_outputs = self.cnn_1D(pgcn_template).permute(1, 0, 2)\n",
    "        #cnn_output shape changed from node_num, t-step, 4 to t-step, node_num, 4 \n",
    "        \n",
    "        return extractor_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48624601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_pooling(extractor_outputs, k):\n",
    "    norms = torch.norm(extractor_outputs, p=2, dim= -1)\n",
    "    #print(\"norm shape\", norms.shape)\n",
    "    _, sorted_indices = torch.sort(norms, dim= -1, descending=True)\n",
    "    sorted_outputs = torch.gather(extractor_outputs, dim=1, index=sorted_indices.unsqueeze(-1).expand(-1, -1, extractor_outputs.size(-1)))\n",
    "    k_nodes_outputs = sorted_outputs[:, : k , :]\n",
    "    \n",
    "    return k_nodes_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56690bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, out_dimen, k, seq_len=20, layer_num = 1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.k = k\n",
    "        self.layer_num = layer_num\n",
    "        nn.utils.parametrizations.spectral_norm(nn.Linear(hidden_dimen, out_dimen))\n",
    "        self.input_layer = nn.Sequential(nn.utils.parametrizations.spectral_norm(nn.Linear(input_dimen * seq_len, hidden_dimen)), nn.BatchNorm1d(num_features = hidden_dimen),\n",
    "                                         nn.ReLU(), nn.Flatten(start_dim=0,end_dim=-1),\n",
    "                                         nn.utils.parametrizations.spectral_norm(nn.Linear(k * hidden_dimen, hidden_dimen)), nn.ReLU())\n",
    "        self.acti_func = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        if layer_num == 1:\n",
    "            self.linear_layers = nn.utils.parametrizations.spectral_norm(nn.Linear(hidden_dimen, out_dimen))\n",
    "        elif layer_num > 1:\n",
    "            self.linear_layers = nn.ModuleList([nn.utils.parametrizations.spectral_norm(nn.Linear(hidden_dimen, hidden_dimen)), nn.ReLU()] * (layer_num - 1))\n",
    "            self.linear_layers.append(nn.utils.parametrizations.spectral_norm(nn.Linear(hidden_dimen, out_dimen)))\n",
    "        \n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = init.constant_(m.bias.data, 0.0)\n",
    "        \n",
    "        \n",
    "    def forward(self, extractor_outputs):\n",
    "        k_nodes_outputs = sort_pooling(extractor_outputs, self.k)\n",
    "        flattened_k_nodes = torch.reshape(k_nodes_outputs.permute(1,0,2),(self.k, -1))\n",
    "        x = self.input_layer(flattened_k_nodes)\n",
    "        \n",
    "        if self.layer_num == 1:\n",
    "            x = self.linear_layers(self.acti_func(x))#Shape: out_dimen\n",
    "            \n",
    "        else:\n",
    "            for layer in self.linear_layers:\n",
    "                x = layer(x)\n",
    "    \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6aeeb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomData(Data):\n",
    "    def __init__(self, trend, period, target_volume, target_label, edge_pairs, subgraph_node_num, subgraph_nodes,city_node_num, dist_max, dist_argmax):\n",
    "        super(CustomData, self).__init__()\n",
    "        self.trend = trend\n",
    "        self.period = period\n",
    "        self.target_volume = target_volume\n",
    "        self.target_label = target_label\n",
    "        self.edge_pairs = edge_pairs\n",
    "        self.subgraph_node_num = subgraph_node_num\n",
    "        self.subgraph_nodes = subgraph_nodes\n",
    "        self.city_node_num = city_node_num\n",
    "        self.dist_max = dist_max\n",
    "        self.dist_argmax = dist_argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddad7009",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dimen, hidden_dimen, output_dimen = 4, 8 ,4\n",
    "max_ach_num = 50\n",
    "input_channels, hidden_channels_1, hidden_channels_2, out_channels = 4, 8, 16, 8\n",
    "gru_input_dimen = 8\n",
    "num_class = 3 \n",
    "layer_num = 2\n",
    "\n",
    "k = 60\n",
    "pred_len = 10\n",
    "seq_len =20\n",
    "alpha = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a92d7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------PGNN----------------------------\n",
    "pgnn_model = T_Step_PGNN(input_dimen, hidden_dimen, output_dimen, max_ach_num)\n",
    "\n",
    "\n",
    "    \n",
    "#-------------------Feature Extractor----------------------   \n",
    "feature_extractor = Feature_Extractor(input_dimen, hidden_dimen, output_dimen, max_ach_num,\n",
    "                                      input_channels, hidden_channels_1, hidden_channels_2, out_channels)\n",
    "\n",
    "#-------------------Discriminator-----------------------------\n",
    "discriminator = Discriminator(input_dimen, hidden_dimen, k, seq_len, num_class, layer_num)\n",
    "\n",
    "#------------------Predictor-----------------------------------\n",
    "predictor = GRU(gru_input_dimen, hidden_dimen, pred_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5af6397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_d_1_calculate(target_data, source_data):\n",
    "    cost_matrix = cdist(target_data.numpy(), source_data.numpy(), metric='euclidean')\n",
    "\n",
    "    # 使用 linear_sum_assignment 来解决最小化运输成本问题\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "    # 计算 Wasserstein-1 距离\n",
    "    wasserstein_1_distance = cost_matrix[row_ind, col_ind].sum() / len(row_ind)\n",
    "    \n",
    "    return torch.tensor(wasserstein_1_distance, dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "500bbb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_regression = nn.MSELoss(reduction='sum')\n",
    "\n",
    "optimizer_extractor = optim.Adam(list(feature_extractor.parameters())+list(pgnn_model.parameters())+list(predictor.parameters()), lr=0.0008)\n",
    "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=0.0009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f59d148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556fbdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 20\n",
    "plt.ion() \n",
    "#---------collect train set value-------------\n",
    "train_regress_losses = []\n",
    "train_class_losses = []\n",
    "#--------collect vali set value---------------\n",
    "vali_regress_losses = []\n",
    "vali_class_losses = []\n",
    "#---------------------------------------------\n",
    "epoch_numbers = []\n",
    "\n",
    "for h in range(epoch_num):\n",
    "    train_regress_loss =0.0\n",
    "    train_class_loss = 0.0\n",
    "    \n",
    "    for b in range(batch_num):\n",
    "        batch = target_data[b: b + batch_size]\n",
    "        source_batch = source_data[b: b + batch_size]\n",
    "        \n",
    "        \n",
    "        all_subgraph_nodes = sum([batch[i].subgraph_node_num for i in range(len(batch))])\n",
    "        \n",
    "        #-----------------------------------------\n",
    "        one_batch_extractor = []\n",
    "        one_batch_predictor = []\n",
    "        one_batch_discriminator = []\n",
    "        #-----------------------------------------\n",
    "        one_batch_source_extractor = []\n",
    "        one_batch_source_discrim = []\n",
    "        #-----------------------------------------\n",
    "        target_volumes = []\n",
    "        \n",
    "        \n",
    "        #Each Batch Covers batch_size data files\n",
    "        for i in range(len(batch)):\n",
    "            #extractor outputs Shape: T-step, Node_num, 4\n",
    "            #------------target city--------------------\n",
    "            extractor_outputs = feature_extractor(batch[i].trend, batch[i].edge_pairs, batch[i].subgraph_nodes, batch[i].dist_max, batch[i].dist_argmax)    \n",
    "            pgnn_period_outputs = pgnn_model(batch[i].period, batch[i].subgraph_nodes, batch[i].dist_max, batch[i].dist_argmax)\n",
    "            discriminator_out = discriminator(extractor_outputs.detach())\n",
    "            \n",
    "            #------------surce city---------------------\n",
    "            source_extractor_outputs = feature_extractor(source_batch[i].trend, source_batch[i].edge_pairs, source_batch[i].subgraph_nodes, source_batch[i].dist_max, source_batch[i].dist_argmax) \n",
    "            source_discriminator_out = discriminator(source_extractor_outputs.detach())\n",
    "                \n",
    "            \n",
    "            predictor_output = predictor(extractor_outputs.permute(1, 0, 2), pgnn_period_outputs.permute(1, 0, 2))\n",
    "            predictor_output = predictor_output.permute(1, 0, 2)\n",
    "                                      \n",
    "            #-------------------------------------------------\n",
    "            one_batch_extractor.append(extractor_outputs)\n",
    "            one_batch_discriminator.append(discriminator_out)\n",
    "            one_batch_predictor.append(predictor_output)\n",
    "            #-------------------------------------------------    \n",
    "            one_batch_source_extractor.append(source_extractor_outputs)\n",
    "            one_batch_source_discrim.append(source_discriminator_out)\n",
    "            #--------------------------------------------------\n",
    "            \n",
    "            \n",
    "            \n",
    "            target_volumes.append(batch[i].target_volume)  \n",
    "            #target_labels.append(target_label)\n",
    "                    \n",
    "        #----------------------------------------------------------------\n",
    "        one_batch_extractor = torch.stack(one_batch_extractor)  \n",
    "        one_batch_discriminator = torch.stack(one_batch_discriminator)\n",
    "        one_batch_predictor = torch.stack(one_batch_predictor)\n",
    "        #----------------------------------------------------------------\n",
    "        one_batch_source_extractor = torch.stack(one_batch_source_extractor)  \n",
    "        one_batch_source_discrim = torch.stack(one_batch_source_discrim)\n",
    "    \n",
    "        target_volumes = torch.stack(target_volumes)                  \n",
    "        #target_labels = torch.tensor(target_labels).to(torch.long)  \n",
    "        \n",
    "        \n",
    "        #---------Update the Para of Discriminator-----------\n",
    "        optimizer_discriminator.zero_grad()\n",
    "        loss_discriminator = -1 * wasserstein_d_1_calculate(one_batch_discriminator, one_batch_source_discrim)\n",
    "        loss_discriminator.backward()\n",
    "        optimizer_discriminator.step()\n",
    "        #------------------------------------------------------\n",
    "\n",
    "        \n",
    "        #--------Update Feature Extractor Para-------------\n",
    "        one_batch_discriminator = torch.empty((0, discriminator_out.shape[0]))\n",
    "        one_batch_source_discrim = torch.empty((0, source_discriminator_out.shape[0]))\n",
    "        \n",
    "        for i in range(one_batch_extractor.shape[0]):\n",
    "            discriminator_out = discriminator(one_batch_extractor[i,:,:,:])\n",
    "            one_batch_discriminator = torch.cat((one_batch_discriminator, discriminator_out.unsqueeze(0)), dim = 0)\n",
    "            \n",
    "            source_discriminator_out = discriminator(one_batch_source_discrim[i,:,:,:])\n",
    "            one_batch_source_discrim = torch.cat((one_batch_source_discrim, source_discriminator_out.unsqueeze(0)), dim = 0)\n",
    "        \n",
    "        \n",
    "        loss_discriminator = wasserstein_d_1_calculate(one_batch_discriminator, one_batch_source_discrim)\n",
    "        loss_predictor = criterion_regression(one_batch_predictor, target_volumes) / all_subgraph_nodes\n",
    "        #----------------------------------------------\n",
    "        train_regress_loss += loss_predictor.item()\n",
    "        train_class_loss += loss_discriminator.item()\n",
    "        #-----------------------------------------------\n",
    "        \n",
    "        \n",
    "        print(f\"After batch {b}, regression loss: {loss_predictor}; disriminator loss: {loss_discriminator}\")      \n",
    "        \n",
    "        \n",
    "        optimizer_extractor.zero_grad()\n",
    "        loss_feat_ext = loss_predictor + alpha * loss_discriminator \n",
    "        loss_feat_ext.backward(retain_graph=True)\n",
    "        optimizer_extractor.step()      \n",
    "        \n",
    "        \n",
    "    train_regress_loss /= batch_num\n",
    "    train_class_loss /= batch_num   \n",
    "    train_regress_losses.append(train_regress_loss)\n",
    "    train_class_losses.append(train_class_loss)\n",
    "        \n",
    "    \n",
    "    \n",
    "    feature_extractor.eval()\n",
    "    pgnn_model.eval()\n",
    "    discriminator.eval()\n",
    "    predictor.eval()\n",
    "    \n",
    "    vali_regress_loss = 0.0\n",
    "    vali_class_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for b in range(vali_batch_num):\n",
    "            batch = vali_data[b: b + batch_size]\n",
    "            source_batch = source_vali_data[b: b + batch_size]\n",
    "            \n",
    "            all_subgraph_nodes = sum([batch[i].subgraph_node_num for i in range(len(batch))])\n",
    "        \n",
    "        \n",
    "            #-----------------------------------------\n",
    "            one_batch_extractor = []\n",
    "            one_batch_predictor = []\n",
    "            one_batch_discriminator = []\n",
    "            #-----------------------------------------\n",
    "            one_batch_source_extractor = []\n",
    "            one_batch_source_discrim = []\n",
    "            #-----------------------------------------        \n",
    "        \n",
    "        \n",
    "        \n",
    "            target_volumes = []  \n",
    "            for i in range(len(batch)):\n",
    "                #------------target city--------------------\n",
    "                extractor_outputs = feature_extractor(batch[i].trend, batch[i].edge_pairs, batch[i].subgraph_nodes, batch[i].dist_max, batch[i].dist_argmax)    \n",
    "                pgnn_period_outputs = pgnn_model(batch[i].period, batch[i].subgraph_nodes, batch[i].dist_max, batch[i].dist_argmax)\n",
    "                discriminator_out = discriminator(extractor_outputs.detach())\n",
    "            \n",
    "                #------------surce city---------------------\n",
    "                source_extractor_outputs = feature_extractor(source_batch[i].trend, source_batch[i].edge_pairs, source_batch[i].subgraph_nodes, source_batch[i].dist_max, source_batch[i].dist_argmax) \n",
    "                source_discriminator_out = discriminator(source_extractor_outputs.detach())\n",
    "                #--------------------------------------------\n",
    "            \n",
    "                predictor_output = predictor(extractor_outputs.permute(1, 0, 2), pgnn_period_outputs.permute(1, 0, 2))\n",
    "                predictor_output = predictor_output.permute(1, 0, 2)\n",
    "                                      \n",
    "                #-------------------------------------------------\n",
    "                one_batch_extractor.append(extractor_outputs)\n",
    "                one_batch_discriminator.append(discriminator_out)\n",
    "                one_batch_predictor.append(predictor_output)\n",
    "                #-------------------------------------------------    \n",
    "                one_batch_source_extractor.append(source_extractor_outputs)\n",
    "                one_batch_source_discrim.append(source_discriminator_out)\n",
    "                #--------------------------------------------------                \n",
    "                target_volumes.append(batch[i].target_volume)  \n",
    "        \n",
    "            #----------------------------------------------------------------\n",
    "            one_batch_extractor = torch.stack(one_batch_extractor)  \n",
    "            one_batch_discriminator = torch.stack(one_batch_discriminator)\n",
    "            one_batch_predictor = torch.stack(one_batch_predictor)\n",
    "            #----------------------------------------------------------------\n",
    "            one_batch_source_extractor = torch.stack(one_batch_source_extractor)  \n",
    "            one_batch_source_discrim = torch.stack(one_batch_source_discrim)\n",
    "            #-----------------------------------------------------------------\n",
    "            target_volumes = torch.stack(target_volumes)                                 \n",
    "            \n",
    "            \n",
    "            loss_discriminator = wasserstein_d_1_calculate(one_batch_discriminator, one_batch_source_discrim)\n",
    "            loss_predictor = criterion_regression(one_batch_predictor, target_volumes) / all_subgraph_nodes\n",
    "                \n",
    "            vali_regress_loss +=  loss_predictor.item()\n",
    "            vali_class_loss += loss_discriminator.item()\n",
    "            \n",
    "    vali_regress_loss /= vali_batch_num\n",
    "    vali_class_loss /= vali_batch_num\n",
    "    vali_regress_losses.append(vali_regress_loss)\n",
    "    vali_class_losses.append(vali_class_loss)\n",
    "    \n",
    "    #------------------------------------------------------------------\n",
    "    epoch_numbers.append(h+1)\n",
    "    \n",
    "    clear_output(wait=True)  #清除上一次的输出\n",
    "    fig, ax1 = plt.subplots(figsize=(10,5))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ax1.plot(epoch_numbers, train_regress_losses, label='Train set')\n",
    "    ax1.plot(epoch_numbers, vali_regress_losses, label='Validation set')\n",
    "    ax1.set_yticks(np.arange(0, 3.5, 0.5)) \n",
    "    ax1.set_xlabel('Epoch number')\n",
    "    ax1.set_ylabel('MSE loss')\n",
    "    ax1.legend(loc = 'upper right')\n",
    "\n",
    "    \n",
    "    plt.show()\n",
    "    plt.pause(0.1)\n",
    "    \n",
    "        \n",
    "plt.ioff()  #关闭交互模式\n",
    "plt.show()      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
