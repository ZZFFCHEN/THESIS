{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c1fedc3-ffe3-4b80-8c6f-52eec65f16f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric as tg\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import init\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset, random_split\n",
    "from torch_geometric.data import Data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2e40306-3187-4ac2-9793-be3df69fb825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, file_path):\n",
    "    with open(file_path , 'wb') as f:\n",
    "        pickle.dump(data,f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c388bb13-cf6c-4e77-babf-1d0417b93bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, dist, dist_arg, masks, min_vals, max_vals, mask_dist_max, class_labels):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.dist = dist\n",
    "        self.dist_arg = dist_arg\n",
    "        self.masks = masks\n",
    "        self.min_vals = min_vals\n",
    "        self.max_vals = max_vals\n",
    "        self.mask_dist_max = mask_dist_max\n",
    "        self.class_labels = class_labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 获取输入和对应的目标数据\n",
    "        x = self.inputs[idx]\n",
    "        y = self.targets[idx]\n",
    "        d_ = self.dist[idx]\n",
    "        d_arg = self.dist_arg[idx]\n",
    "        mask = self.masks[idx]\n",
    "        min_ = self.min_vals[idx]\n",
    "        max_ = self.max_vals[idx]\n",
    "        mask_d_ = self.mask_dist_max[idx]\n",
    "        class_ = self.class_labels[idx]\n",
    "        return x, y, d_, d_arg, mask, min_, max_, mask_d_, class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d5bc078-4fb9-4efc-8e26-e56c73a9e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearOperate(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen):\n",
    "        super(NonLinearOperate, self).__init__()  #类NonLinearLayer继承父类nn.Module的初始化方法\n",
    "        self.layer_1 = nn.Linear(input_dimen, hidden_dimen)\n",
    "        self.layer_2 = nn.Linear(hidden_dimen, output_dimen)\n",
    "        self.acti_func = nn.ReLU()\n",
    "        for m in self.modules():#遍历所有子模块\n",
    "        #Check if each sub-module is an example of the class nn.Linear\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = init.kaiming_normal_(m.weight.data, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = init.constant_(m.bias.data, 0.0)\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.acti_func(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.acti_func(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe1afb58-259f-40b0-a836-79d4e59dc2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGNN_Layer(nn.Module):\n",
    "    def __init__(self, hidden_dimen, output_dimen, anchor_num, drop_out = True):\n",
    "        super(PGNN_Layer, self).__init__()\n",
    "        self.drop_out = drop_out\n",
    "        self.output_dimen = output_dimen\n",
    "        #self.distance_calculate = NonLinearOperate(1, output_dimen, 1)\n",
    "        self.acti_func = nn.LeakyReLU()\n",
    "        self.linear_hidden = nn.Linear(2 * anchor_num, hidden_dimen)\n",
    "        self.out_layer = nn.Linear(hidden_dimen, output_dimen)\n",
    "        \n",
    "        self.linear_structure = nn.Linear(hidden_dimen, 1)\n",
    "\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = init.kaiming_uniform_(m.weight.data, nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = init.constant_(m.bias.data, 0.0)                \n",
    "                    \n",
    "    def forward(self, node_features, dists_max, dists_argmax, mask_dist_max):\n",
    "        # node feature SHAPE: Batch-size, 28, Max_node_num\n",
    "        # Dist_max SHAPE: Batch-size, 28, Max_node_num, 36\n",
    "        # mask_dist_max SHAPE: Batch-size, 28, Max_node_num, 36\n",
    "        batch_size, num_days, max_node_num = node_features.size()\n",
    "        #dists_max = self.distance_calculate(dists_max.unsqueeze(-1)).squeeze()\n",
    "\n",
    "        indices_expanded = dists_argmax.flatten(start_dim=2)\n",
    "        batch_indices = torch.arange(batch_size).view(-1, 1, 1).expand(-1, num_days, indices_expanded.shape[-1])\n",
    "        day_indices = torch.arange(num_days).view(1, -1, 1).expand(batch_size, -1, indices_expanded.shape[-1]) \n",
    "        \n",
    "        subset_features = node_features[batch_indices, day_indices, indices_expanded]\n",
    "\n",
    "\n",
    "        #---------------------------------------------------------------\n",
    "        subset_features = subset_features.reshape(subset_features.shape[0], subset_features.shape[1], dists_argmax.shape[2], dists_argmax.shape[3])\n",
    "        #message SHAPE: Batch-size, 28, Max_node_num, 36\n",
    "        messages = subset_features * dists_max * mask_dist_max\n",
    "        feature_self = node_features.unsqueeze(-1).repeat(1, 1, 1, dists_max.shape[-1])\n",
    "        \n",
    "        #messages SHAPE: Batch-size, 28, Max_node_num * (36*2)\n",
    "        messages = torch.concat((messages, feature_self), dim = -1)\n",
    "        #---------------------------------------------------------------\n",
    "        #INPUT DIMEN:Batch-size, 28, Max_node_num, 72\n",
    "        messages = self.linear_hidden(messages)\n",
    "        if self.drop_out:\n",
    "            messages = F.dropout(messages, training=self.training, p=0.2)\n",
    "        #SHAPE: Batch-size, 28, Max_node_num, hidden_dimen\n",
    "        messages = self.acti_func(messages) \n",
    "        \n",
    "        #SHAPE: Batch-size, 28, Max_node_num, Output_dimen\n",
    "        output = self.out_layer(messages)\n",
    "        if self.drop_out:\n",
    "            output = F.dropout(output, training=self.training, p =0.2)\n",
    "        output = self.acti_func(output) \n",
    "        \n",
    "        output_structure = self.linear_structure(messages)\n",
    "        if self.drop_out:\n",
    "            output_structure = F.dropout(output_structure, training=self.training, p=0.2)\n",
    "        output_structure = self.acti_func(output_structure).unsqueeze(-1)\n",
    "        \n",
    "        \n",
    "        return output, output_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef29d36e-c726-4b26-9b13-f9619c1667a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGNN(nn.Module):\n",
    "    def __init__(self, hidden_dimen, output_dimen, anchor_num, layer_num = 1, drop_out = True):\n",
    "        super(PGNN, self).__init__()\n",
    "        self.drop_out = drop_out\n",
    "        self.layer_num = layer_num\n",
    "        self.anchor_num = anchor_num\n",
    "        if self.layer_num == 1:\n",
    "            self.pgnn_operate_1 = PGNN_Layer(hidden_dimen, output_dimen, anchor_num, drop_out)\n",
    "        \n",
    "            \n",
    "        if self.layer_num > 1:\n",
    "            self.pgnn_operate_1 = nn.ModuleList([PGNN_Layer(hidden_dimen, output_dimen, anchor_num, drop_out) for i in range(0, layer_num)])\n",
    "           \n",
    "                \n",
    "        \n",
    "    def forward(self, x, dist_max, dist_argmax, mask_dist_max):\n",
    "\n",
    "        if self.layer_num == 1:\n",
    "            x_position, x = self.pgnn_operate_1(x, dist_max, dist_argmax, mask_dist_max)\n",
    "    \n",
    "            return x_position\n",
    "\n",
    "        \n",
    "        if self.layer_num > 1:\n",
    "            for i in range(self.layer_num):\n",
    "                _, x = self.pgnn_operate_1[i](x, dist_max, dist_argmax, mask_dist_max)\n",
    "            x_position = _   \n",
    "\n",
    "            return x_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c96663-8b0d-4ab3-a601-5e61e171af7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self, output_dimen, hidden_channels_1, hidden_channels_2):\n",
    "        super(CNN_1D, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = output_dimen, out_channels = hidden_channels_1, kernel_size=7,  stride=1, padding=0),\n",
    "            nn.LeakyReLU(), #len 22 days\n",
    "            nn.Conv1d(in_channels = hidden_channels_1, out_channels = hidden_channels_2, kernel_size=7,  stride=1, padding=0),\n",
    "            nn.LeakyReLU(), #len 16 days\n",
    "            nn.Conv1d(in_channels = hidden_channels_2, out_channels = hidden_channels_2, kernel_size = 6, stride = 2, padding=1),\n",
    "            nn.LeakyReLU())  #len: 7 days\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d) or isinstance(m, nn.ConvTranspose1d):\n",
    "                # 对卷积层使用 Kaiming 正态初始化\n",
    "                nn.init.kaiming_uniform_(m.weight.data, nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "       \n",
    "        \n",
    "#PGNN处理后数据形状 : Batch_num, 28, Max_node_num, output_dimen ------> (Batch_num * Max_node_num), output_dimen, 28\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, max_node_num, out_dim = x.size()\n",
    "        x= x.permute(0, 2, 3, 1)\n",
    "        x = x.flatten(start_dim=0, end_dim=1)\n",
    "        #output shape: (Batch_num * Max_node_num), hidden_channels_2, 7\n",
    "        out = self.conv_layer(x) \n",
    "        #----------------------------------------------------\n",
    "        #output shape: (Batch_num * Max_node_num), 7, hidden_channels_2\n",
    "        out = out.permute(0, 2, 1)\n",
    "        #------------------------------------------------------\n",
    "\n",
    "       \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "057f91b8-1428-4bf9-aebe-eba749689e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, output_seq_len, num_nodes, drop_out = True):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers = 1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size * output_seq_len)\n",
    "        self.num_nodes = num_nodes\n",
    "        self.output_seq_len = output_seq_len\n",
    "        self.output_size = output_size\n",
    "        self.acti_func = nn.ReLU()\n",
    "        self.drop_out = drop_out\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = init.constant_(m.bias.data, 0.0) \n",
    "          \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch_num * Max_node_num), 7, hidden_size\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        if self.drop_out:\n",
    "            lstm_out = F.dropout(lstm_out, training=self.training, p=0.2)\n",
    "        lstm_out = self.acti_func(lstm_out)\n",
    "        # lstm_out shape: (Batch_num * Max_node_num), hidden_size \n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        out = self.fc(lstm_out)  \n",
    "        out= self.acti_func(out)\n",
    "        #out shape: Batch_num, Max_node_num, output_seq_len\n",
    "        out = out.view(-1, self.num_nodes, self.output_seq_len).permute(0, 2, 1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7034733f-05cd-4c91-a2b0-7c2cab36fef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, output_seq_len, num_nodes, drop_out = True):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size * output_seq_len)\n",
    "        self.num_nodes = num_nodes\n",
    "        self.output_seq_len = output_seq_len\n",
    "        self.output_size = output_size\n",
    "        self.acti_func = nn.ReLU()\n",
    "        self.drop_out = drop_out\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = init.constant_(m.bias.data, 0.0)  \n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch_num * Max_node_num), 7, hidden_size\n",
    "        lstm_out, _ = self.rnn(x)\n",
    "        if self.drop_out:\n",
    "            lstm_out = F.dropout(lstm_out, training=self.training, p=0.2)\n",
    "        lstm_out = self.acti_func(lstm_out)\n",
    "        # lstm_out shape: (Batch_num * Max_node_num), hidden_size \n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        out = self.fc(lstm_out)  \n",
    "        out= self.acti_func(out)\n",
    "        #out shape: Batch_num, Max_node_num, output_seq_len\n",
    "        out = out.view(-1, self.num_nodes, self.output_seq_len).permute(0, 2, 1)\n",
    "         \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "497230d6-5f1a-4096-a97b-328e74442993",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "#PGNN\n",
    "anchor_num = 50\n",
    "num_nodes = 1466    \n",
    "hidden_dimen = 128  \n",
    "output_dimen = 64\n",
    "#-------------------------------------------------\n",
    "#CNN\n",
    "hidden_channels_1, hidden_channels_2 = 128, 64\n",
    "#-------------------------------------------------\n",
    "input_size = hidden_channels_2  # LSTM 输入的维度\n",
    "hidden_size = 128   # LSTM 隐藏层的维度\n",
    "output_seq_len = 14  # 预测14天\n",
    "output_size = 1\n",
    "\n",
    "#-------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6050b07-2e87-4299-8d5b-ac6ef14f54f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b3cdf05-b859-47f3-90fe-741a333419bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm = LSTMModel(input_size, hidden_size, output_size, output_seq_len, num_nodes)\n",
    "lstm = RNNModel(input_size, hidden_size, output_size, output_seq_len, num_nodes)\n",
    "lstm_optimizer = optim.NAdam(lstm.parameters(), lr= 1e-4, weight_decay=1e-5) #Nadam 4e-4\n",
    "lstm_scheduler = optim.lr_scheduler.StepLR(lstm_optimizer, step_size=8, gamma=0.6)\n",
    "#------------------------------------------------------------------------------------------\n",
    "pgnn = PGNN(hidden_dimen, output_dimen, anchor_num)\n",
    "cnn_pgnn_optimizer = optim.NAdam(list(pgnn.parameters()), lr= 2e-4, weight_decay=1e-5) \n",
    "cnn_pgnn_scheduler = optim.lr_scheduler.StepLR(cnn_pgnn_optimizer, step_size=8, gamma=0.55)\n",
    "cnn = CNN_1D(output_dimen, hidden_channels_1, hidden_channels_2)\n",
    "cnn_optimizer = optim.NAdam(list(cnn.parameters()), lr= 2e-4, weight_decay=1e-5) #Nadam 6e-4\n",
    "cnn_scheduler = optim.lr_scheduler.StepLR(cnn_optimizer, step_size=8, gamma=0.55)\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "criterion_MSE = nn.MSELoss(reduction='none')  # 使用均方误差作为损失函数\n",
    "criterion_MAE = nn.L1Loss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48a554d7-0c1e-4c9a-9039-62c321f93355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('D:/ThesisData/processed data/ModelPara/source_pgnn_cnn_rnn_14days.pth')\n",
    "pgnn.load_state_dict(checkpoint['pgnn_state_dict'])\n",
    "cnn.load_state_dict(checkpoint['cnn_state_dict'])\n",
    "lstm.load_state_dict(checkpoint['rnn_state_dict'])\n",
    "#cnn_pgnn_optimizer.load_state_dict(checkpoint['cnn_pgnn_optimizer_state_dict'])\n",
    "#cnn_optimizer.load_state_dict(checkpoint['cnn_optimizer_state_dict'])\n",
    "#lstm_optimizer.load_state_dict(checkpoint['lstm_optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "616b0abc-87f4-4e47-89ad-0deea2fa3834",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(cnn.conv_layer):\n",
    "    if isinstance(layer, nn.Conv1d) and i < 2:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False \n",
    "\n",
    "for param in pgnn.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d51b0862-e1ed-4c3c-bb5f-537b0d6152a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = torch.load(\"D:/ThesisData/processed data/TargetDomain/NEW/train_data_14days.h5\")\n",
    "vali_barcelona = torch.load(\"D:/ThesisData/processed data/TargetDomain/NEW/Barcelona_vali_data_14days.h5\")\n",
    "test_barcelona = torch.load(\"D:/ThesisData/processed data/TargetDomain/NEW/Barcelona_test_data_14days.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ade987-5d65-4acb-895d-7d4e99312bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7cbfc2e-b515-4e53-859a-3d49ed8c40c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 14\n",
    "train_loader = DataLoader(target_train, batch_size=batch_size, shuffle=True)\n",
    "vali_loader = DataLoader(vali_barcelona, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_barcelona, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a3a1c01-7a3f-460a-a7a4-5c0c2b9d66af",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05d80872-84e1-4c80-bafa-c6917e07ac3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, Train Set Inversed Values: MSE=2741.1, MAE=40.6\n",
      "----Validation Barcelona: MSE=1058.9, MAE=20.2\n",
      "----Test Barcelona: MSE=1617.1, MAE=26.1\n",
      "-----------------------------------------------\n",
      "epoch 2, Train Set Inversed Values: MSE=2791.7, MAE=41.0\n",
      "----Validation Barcelona: MSE=1065.8, MAE=20.2\n",
      "----Test Barcelona: MSE=1627.0, MAE=26.2\n",
      "-----------------------------------------------\n",
      "epoch 3, Train Set Inversed Values: MSE=2631.1, MAE=40.0\n",
      "----Validation Barcelona: MSE=1086.9, MAE=20.4\n",
      "----Test Barcelona: MSE=1670.9, MAE=26.5\n",
      "-----------------------------------------------\n",
      "epoch 4, Train Set Inversed Values: MSE=2694.5, MAE=40.3\n",
      "----Validation Barcelona: MSE=1075.0, MAE=20.4\n",
      "----Test Barcelona: MSE=1634.9, MAE=26.2\n",
      "-----------------------------------------------\n",
      "epoch 5, Train Set Inversed Values: MSE=2752.3, MAE=40.6\n",
      "----Validation Barcelona: MSE=1075.8, MAE=20.4\n",
      "----Test Barcelona: MSE=1634.5, MAE=26.2\n",
      "-----------------------------------------------\n",
      "epoch 6, Train Set Inversed Values: MSE=2607.6, MAE=39.7\n",
      "----Validation Barcelona: MSE=1098.7, MAE=20.6\n",
      "----Test Barcelona: MSE=1687.1, MAE=26.6\n",
      "-----------------------------------------------\n",
      "epoch 7, Train Set Inversed Values: MSE=2855.6, MAE=40.9\n",
      "----Validation Barcelona: MSE=1035.4, MAE=20.4\n",
      "----Test Barcelona: MSE=1534.5, MAE=25.6\n",
      "-----------------------------------------------\n",
      "epoch 8, Train Set Inversed Values: MSE=2799.8, MAE=41.3\n",
      "----Validation Barcelona: MSE=1096.5, MAE=20.6\n",
      "----Test Barcelona: MSE=1696.2, MAE=26.7\n",
      "-----------------------------------------------\n",
      "epoch 9, Train Set Inversed Values: MSE=2750.8, MAE=40.5\n",
      "----Validation Barcelona: MSE=1074.3, MAE=20.6\n",
      "----Test Barcelona: MSE=1637.7, MAE=26.3\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 32\u001b[0m\n\u001b[0;32m     28\u001b[0m cnn_pgnn_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     29\u001b[0m cnn_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 32\u001b[0m \u001b[43mloss_mse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(lstm\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m     36\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(cnn\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\Cuda_env\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\Cuda_env\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for h in range(epoch_num):\n",
    "    lstm.train()\n",
    "    pgnn.train()\n",
    "    cnn.train()\n",
    "    \n",
    "    \n",
    "    for batch_input, batch_labels, batch_dist, batch_dist_arg, batch_masks, batch_min, batch_max, batch_dist_mask, _ in train_loader:\n",
    "\n",
    "        \n",
    "        batch_input, batch_labels, batch_masks = batch_input.squeeze(), batch_labels.squeeze(), batch_masks.squeeze()\n",
    "        batch_min, batch_max = batch_min.squeeze(), batch_max.squeeze()\n",
    "  \n",
    "        \n",
    "\n",
    "        \n",
    "        pgnn_output = pgnn(batch_input, batch_dist, batch_dist_arg, batch_dist_mask)\n",
    "        cnn_output = cnn(pgnn_output)\n",
    "        \n",
    "        batch_outputs = lstm(cnn_output)\n",
    "      \n",
    "        \n",
    "        loss_mse = criterion_MSE(batch_outputs, batch_labels) * batch_masks\n",
    "        loss_mae = criterion_MAE(batch_outputs, batch_labels) * batch_masks\n",
    "        loss_mse = loss_mse.sum() / batch_masks.sum()\n",
    "        loss_mae = loss_mae.sum() / batch_masks.sum()\n",
    "        \n",
    "        lstm_optimizer.zero_grad()\n",
    "        cnn_pgnn_optimizer.zero_grad()\n",
    "        cnn_optimizer.zero_grad()\n",
    "\n",
    "       \n",
    "        loss_mse.backward()\n",
    "\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(lstm.parameters(), 1.0)\n",
    "        torch.nn.utils.clip_grad_norm_(cnn.parameters(), 1.0)\n",
    "        torch.nn.utils.clip_grad_norm_(pgnn.parameters(), 1.0)\n",
    "\n",
    "        \n",
    "        lstm_optimizer.step()\n",
    "        cnn_pgnn_optimizer.step()\n",
    "        cnn_optimizer.step()\n",
    "        \n",
    "        del loss_mse\n",
    "        del loss_mae\n",
    "        #----------------------------------------------------------------------\n",
    "        inverse_outputs = batch_outputs * (batch_max - batch_min) + batch_min\n",
    "        inverse_labels = batch_labels * (batch_max - batch_min) + batch_min\n",
    "\n",
    "        \n",
    "        loss_mse = criterion_MSE(inverse_outputs, inverse_labels) * batch_masks\n",
    "        loss_mae = criterion_MAE(inverse_outputs, inverse_labels) * batch_masks\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss_mse = loss_mse.sum() / batch_masks.sum()\n",
    "        loss_mae = loss_mae.sum() / batch_masks.sum()    \n",
    "        \n",
    "        del batch_outputs\n",
    "        del batch_labels\n",
    "        del inverse_outputs\n",
    "        del inverse_labels\n",
    "        del batch_min\n",
    "        del batch_max\n",
    "\n",
    "    \n",
    "    print(f\"epoch {h + 1}, Train Set Inversed Values: MSE={loss_mse.item():.1f}, MAE={loss_mae.item():.1f}\")  \n",
    "    \n",
    "    \n",
    "#--------------------------------------------------\n",
    "    lstm.eval()\n",
    "    pgnn.eval()\n",
    "    cnn.eval()  \n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch_input, batch_labels, batch_dist, batch_dist_arg, batch_masks, batch_min, batch_max, batch_dist_mask, _ in vali_loader:\n",
    "            batch_input, batch_labels, batch_masks = batch_input.squeeze(), batch_labels.squeeze(), batch_masks.squeeze()\n",
    "            batch_min, batch_max = batch_min.squeeze(), batch_max.squeeze()\n",
    "\n",
    "\n",
    "            pgnn_output = pgnn(batch_input, batch_dist, batch_dist_arg, batch_dist_mask)\n",
    "            cnn_output = cnn(pgnn_output)\n",
    "            batch_outputs = lstm(cnn_output)           \n",
    "\n",
    "      \n",
    "            del pgnn_output\n",
    "            del cnn_output\n",
    "\n",
    "\n",
    "            #----------------------------------------------------------------------\n",
    "            Barcelona_vali_outputs = batch_outputs * (batch_max - batch_min) + batch_min\n",
    "            Barcelona_vali_labels = batch_labels * (batch_max - batch_min) + batch_min            \n",
    "            #----------------------------------------------------------------------\n",
    "        \n",
    "\n",
    "            loss_mse = criterion_MSE(Barcelona_vali_outputs, Barcelona_vali_labels) * batch_masks\n",
    "            loss_mae = criterion_MAE(Barcelona_vali_outputs, Barcelona_vali_labels) * batch_masks\n",
    "            loss_mse = loss_mse.sum() / batch_masks.sum()\n",
    "            loss_mae = loss_mae.sum() / batch_masks.sum()   \n",
    "\n",
    "          \n",
    "        print(f\"----Validation Barcelona: MSE={loss_mse.item():.1f}, MAE={loss_mae.item():.1f}\")\n",
    "\n",
    "\n",
    "        for batch_input, batch_labels, batch_dist, batch_dist_arg, batch_masks, batch_min, batch_max, batch_dist_mask, _ in test_loader:\n",
    "            batch_input, batch_labels, batch_masks = batch_input.squeeze(), batch_labels.squeeze(), batch_masks.squeeze()\n",
    "            batch_min, batch_max = batch_min.squeeze(), batch_max.squeeze()\n",
    "\n",
    "\n",
    "            pgnn_output = pgnn(batch_input, batch_dist, batch_dist_arg, batch_dist_mask)\n",
    "            cnn_output = cnn(pgnn_output)\n",
    "            batch_outputs = lstm(cnn_output)           \n",
    "\n",
    "      \n",
    "            del pgnn_output\n",
    "            del cnn_output\n",
    "\n",
    "\n",
    "            #----------------------------------------------------------------------\n",
    "            Barcelona_test_outputs = batch_outputs * (batch_max - batch_min) + batch_min\n",
    "            Barcelona_test_labels = batch_labels * (batch_max - batch_min) + batch_min            \n",
    "            #----------------------------------------------------------------------\n",
    "        \n",
    "\n",
    "            loss_mse = criterion_MSE(Barcelona_test_outputs, Barcelona_test_labels) * batch_masks\n",
    "            loss_mae = criterion_MAE(Barcelona_test_outputs, Barcelona_test_labels) * batch_masks\n",
    "            loss_mse = loss_mse.sum() / batch_masks.sum()\n",
    "            loss_mae = loss_mae.sum() / batch_masks.sum()   \n",
    "\n",
    "          \n",
    "        print(f\"----Test Barcelona: MSE={loss_mse.item():.1f}, MAE={loss_mae.item():.1f}\")\n",
    "        print(\"-----------------------------------------------\")\n",
    "\n",
    "\n",
    "    if h == epoch_num-1: #epoch_num-1:\n",
    "    #if loss_mse <= 2030 or h == epoch_num-1:\n",
    "        pgnn_cnn_results = {\"Barcelona_vali\": Barcelona_vali_outputs.detach().numpy(), \"Barcelona_vali_label\": Barcelona_vali_labels.detach().numpy(),\n",
    "                            \"Barcelona_test\": Barcelona_test_outputs.detach().numpy(), \"Barcelona_test_label\": Barcelona_test_labels.detach().numpy(),\n",
    "                            \"Barcelona_node_num\": 1273}\n",
    "\n",
    "        save_data(pgnn_cnn_results,\"D:/ThesisData/processed data/TargetDomain/NEW/results/pgnn_cnn_rnn_finetune_results.h5\" )    \n",
    "        break\n",
    "\n",
    "        \n",
    "    lstm_scheduler.step()\n",
    "    cnn_pgnn_scheduler.step()\n",
    "    cnn_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753523aa-d3eb-4fdf-8aea-0e3634578381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0267fe-731f-4478-9618-8e38e65daafc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
