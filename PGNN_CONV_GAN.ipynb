{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f47ce19-0e23-43c9-8bb0-759c35821ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric as tg\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch.nn import init\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aad3683b-782c-4e42-8e89-8dd1db10007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, file_path):\n",
    "    with open(file_path , 'wb') as f:\n",
    "        pickle.dump(data,f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c1b7723-a4a9-4f97-bb4e-5a9edda4157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, dist, dist_arg, masks, min_vals, max_vals, mask_dist_max, class_labels):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.dist = dist\n",
    "        self.dist_arg = dist_arg\n",
    "        self.masks = masks\n",
    "        self.min_vals = min_vals\n",
    "        self.max_vals = max_vals\n",
    "        self.mask_dist_max = mask_dist_max\n",
    "        self.class_labels = class_labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 获取输入和对应的目标数据\n",
    "        x = self.inputs[idx]\n",
    "        y = self.targets[idx]\n",
    "        d_ = self.dist[idx]\n",
    "        d_arg = self.dist_arg[idx]\n",
    "        mask = self.masks[idx]\n",
    "        min_ = self.min_vals[idx]\n",
    "        max_ = self.max_vals[idx]\n",
    "        mask_d_ = self.mask_dist_max[idx]\n",
    "        class_ = self.class_labels[idx]\n",
    "        return x, y, d_, d_arg, mask, min_, max_, mask_d_, class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3ddbd94-ec72-48bc-8dab-f8bd5a19a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_pooling(x, k, num_nodes):\n",
    "    # x SHAPE: (Batch_num * Max_node_num), (hidden_channels_2)\n",
    "    #-------------------> Batch_num, Max_node_num, (hidden_channels_2)\n",
    "    x = x.reshape(-1, num_nodes, x.shape[-1])\n",
    "    norms = torch.norm(x, p=2, dim= -1)\n",
    "    #------------------------------------------------------------\n",
    "    _, sorted_indices = torch.sort(norms, dim= -1, descending=True)\n",
    "    \n",
    "    sorted_x = torch.gather(x, dim=1, index=sorted_indices.unsqueeze(-1).expand(-1, -1, x.size(-1)))\n",
    "    top_k_x = sorted_x[:, :k , :]\n",
    "    # top_k_x SHAPE: Batch_num, K, (hidden_channels_2)\n",
    "    \n",
    "    return top_k_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99543c20-74be-4b04-91f5-f7cfc5896e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_input = grad_output.neg() * ctx.alpha  # 梯度取反\n",
    "        return grad_input, None\n",
    "\n",
    "class GradientReversalLayer(nn.Module):\n",
    "    def __init__(self, alpha=None):\n",
    "        super(GradientReversalLayer, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dae5469-07e8-4884-8129-6454255238d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class GradientReversalLayer(Function):\n",
    "    #def forward(ctx, x, alpha):\n",
    "        #ctx.alpha = alpha\n",
    "        #return x.view_as(x)\n",
    "    \n",
    "    \n",
    "    #def backward(ctx, grad_output):\n",
    "        #output = grad_output.neg() * ctx.alpha  # 反转梯度\n",
    "        #return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d21c3a2f-6205-4c90-b283-562cc9423423",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Domain_Classifier(nn.Module):\n",
    "    def __init__(self, hidden_channels_2, class_hidden_1, class_hidden_2, k, num_nodes, alpha, drop_out = True):\n",
    "        super(Domain_Classifier, self).__init__()\n",
    "        self.k = k\n",
    "        self.drop_out = drop_out\n",
    "        self.num_nodes = num_nodes\n",
    "        self.fc_1 = nn.Sequential(nn.Linear(k * hidden_channels_2, class_hidden_1), nn.LeakyReLU())  \n",
    "        #self.fc_2 = nn.Sequential(nn.Linear(class_hidden_1, class_hidden_2), nn.LeakyReLU()) \n",
    "        self.fc_3 = nn.Sequential(nn.Linear(class_hidden_1, 1), nn.LeakyReLU())  \n",
    "        self.grl = GradientReversalLayer(alpha) \n",
    "        \n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                module.weight.data = init.kaiming_uniform_(module.weight.data, nonlinearity='leaky_relu')\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data = init.constant_(module.bias.data, 0.0)                    \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x shape: (Batch_num * Max_node_num), 7, hidden_channels_2\n",
    "        x = x \n",
    "        x = x[:, -1, :]\n",
    "        #---------------------------------------------------------\n",
    "        # top_k_x SHAPE: Batch_num, K, hidden_channels_2\n",
    "        x = sort_pooling(x, self.k, self.num_nodes) \n",
    "        #------------------------------------------------------------\n",
    "        x = x.flatten(start_dim=1, end_dim=2)\n",
    "        #-----------------------------------------------------------\n",
    "        #Shape: Batch_num, hidden_dimen\n",
    "        #-----------------------------------------------------------\n",
    "        x = self.grl(x)\n",
    "        #-----------------------------------------------------------\n",
    "        x = self.fc_1(x)\n",
    "        if self.drop_out:\n",
    "            x = F.dropout(x, training=self.training, p = 0.2)\n",
    "        #------------------------------------------------------------\n",
    "        #x = self.fc_2(x)\n",
    "        #if self.drop_out:\n",
    "            #x = F.dropout(x, training=self.training, p = 0.2)\n",
    "\n",
    "        \n",
    "        # SHAPE: Batch_num, 1\n",
    "        x = self.fc_3(x) \n",
    "      \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76c59dcf-7b19-427e-8e2a-4431f4c41fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearOperate(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen):\n",
    "        super(NonLinearOperate, self).__init__()  #类NonLinearLayer继承父类nn.Module的初始化方法\n",
    "        self.layer_1 = nn.Linear(input_dimen, hidden_dimen)\n",
    "        self.layer_2 = nn.Linear(hidden_dimen, output_dimen)\n",
    "        self.acti_func = nn.ReLU()\n",
    "        for m in self.modules():#遍历所有子模块\n",
    "        #Check if each sub-module is an example of the class nn.Linear\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = init.kaiming_normal_(m.weight.data, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = init.constant_(m.bias.data, 0.0)\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.acti_func(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.acti_func(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af39d3cb-1ddd-4998-b515-8c06576557ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGNN_Layer(nn.Module):\n",
    "    def __init__(self, hidden_dimen, output_dimen, anchor_num, drop_out = True):\n",
    "        super(PGNN_Layer, self).__init__()\n",
    "        self.drop_out = drop_out\n",
    "        self.output_dimen = output_dimen\n",
    "        #self.distance_calculate = NonLinearOperate(1, output_dimen, 1)\n",
    "        self.acti_func = nn.LeakyReLU()\n",
    "        self.linear_hidden = nn.Linear(2 * anchor_num, hidden_dimen)\n",
    "        self.out_layer = nn.Linear(hidden_dimen, output_dimen)\n",
    "        \n",
    "        self.linear_structure = nn.Linear(hidden_dimen, 1)\n",
    "\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = init.kaiming_uniform_(m.weight.data, nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = init.constant_(m.bias.data, 0.0)                \n",
    "                    \n",
    "    def forward(self, node_features, dists_max, dists_argmax, mask_dist_max):\n",
    "        # node feature SHAPE: Batch-size, 28, Max_node_num\n",
    "        # Dist_max SHAPE: Batch-size, 28, Max_node_num, 36\n",
    "        # mask_dist_max SHAPE: Batch-size, 28, Max_node_num, 36\n",
    "        batch_size, num_days, max_node_num = node_features.size()\n",
    "        #dists_max = self.distance_calculate(dists_max.unsqueeze(-1)).squeeze()\n",
    "\n",
    "        indices_expanded = dists_argmax.flatten(start_dim=2)\n",
    "        batch_indices = torch.arange(batch_size).view(-1, 1, 1).expand(-1, num_days, indices_expanded.shape[-1])\n",
    "        day_indices = torch.arange(num_days).view(1, -1, 1).expand(batch_size, -1, indices_expanded.shape[-1]) \n",
    "        \n",
    "        subset_features = node_features[batch_indices, day_indices, indices_expanded]\n",
    "\n",
    "\n",
    "        #---------------------------------------------------------------\n",
    "        subset_features = subset_features.reshape(subset_features.shape[0], subset_features.shape[1], dists_argmax.shape[2], dists_argmax.shape[3])\n",
    "        #message SHAPE: Batch-size, 28, Max_node_num, 36\n",
    "        messages = subset_features * dists_max * mask_dist_max\n",
    "        feature_self = node_features.unsqueeze(-1).repeat(1, 1, 1, dists_max.shape[-1])\n",
    "        \n",
    "        #messages SHAPE: Batch-size, 28, Max_node_num * (36*2)\n",
    "        messages = torch.concat((messages, feature_self), dim = -1)\n",
    "        #---------------------------------------------------------------\n",
    "        #INPUT DIMEN:Batch-size, 28, Max_node_num, 72\n",
    "        messages = self.linear_hidden(messages)\n",
    "        if self.drop_out:\n",
    "            messages = F.dropout(messages, training=self.training, p=0.2)\n",
    "        #SHAPE: Batch-size, 28, Max_node_num, hidden_dimen\n",
    "        messages = self.acti_func(messages) \n",
    "        \n",
    "        #SHAPE: Batch-size, 28, Max_node_num, Output_dimen\n",
    "        output = self.out_layer(messages)\n",
    "        if self.drop_out:\n",
    "            output = F.dropout(output, training=self.training, p =0.2)\n",
    "        output = self.acti_func(output) \n",
    "        \n",
    "        output_structure = self.linear_structure(messages)\n",
    "        if self.drop_out:\n",
    "            output_structure = F.dropout(output_structure, training=self.training, p=0.2)\n",
    "        output_structure = self.acti_func(output_structure).unsqueeze(-1)\n",
    "        \n",
    "        \n",
    "        return output, output_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3fb1978-f3fc-4711-967a-257e9b0397d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGNN(nn.Module):\n",
    "    def __init__(self, hidden_dimen, output_dimen, anchor_num, layer_num = 1, drop_out = True):\n",
    "        super(PGNN, self).__init__()\n",
    "        self.drop_out = drop_out\n",
    "        self.layer_num = layer_num\n",
    "        self.anchor_num = anchor_num\n",
    "        if self.layer_num == 1:\n",
    "            self.pgnn_operate_1 = PGNN_Layer(hidden_dimen, output_dimen, anchor_num, drop_out)\n",
    "        \n",
    "            \n",
    "        if self.layer_num > 1:\n",
    "            self.pgnn_operate_1 = nn.ModuleList([PGNN_Layer(hidden_dimen, output_dimen, anchor_num, drop_out) for i in range(0, layer_num)])\n",
    "           \n",
    "                \n",
    "        \n",
    "    def forward(self, x, dist_max, dist_argmax, mask_dist_max):\n",
    "\n",
    "        if self.layer_num == 1:\n",
    "            x_position, x = self.pgnn_operate_1(x, dist_max, dist_argmax, mask_dist_max)\n",
    "    \n",
    "            return x_position\n",
    "\n",
    "        \n",
    "        if self.layer_num > 1:\n",
    "            for i in range(self.layer_num):\n",
    "                _, x = self.pgnn_operate_1[i](x, dist_max, dist_argmax, mask_dist_max)\n",
    "            x_position = _   \n",
    "\n",
    "            return x_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c58ef978-9007-4c4b-be01-efe5b4c5789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PGNN处理后数据形状 : Batch_num, 28, Max_node_num, output_dimen ------> (Batch_num * Max_node_num), output_dimen, 28\n",
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self, output_dimen, hidden_channels_1, hidden_channels_2):\n",
    "        super(CNN_1D, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = output_dimen, out_channels = hidden_channels_1, kernel_size=7,  stride=1, padding=0),\n",
    "            nn.LeakyReLU(), #len 22 days\n",
    "            nn.Conv1d(in_channels = hidden_channels_1, out_channels = hidden_channels_2, kernel_size=7,  stride=1, padding=0),\n",
    "            nn.LeakyReLU(), #len 16 days\n",
    "            nn.Conv1d(in_channels = hidden_channels_2, out_channels = hidden_channels_2, kernel_size = 6, stride = 2, padding=1),\n",
    "            nn.LeakyReLU())  #len: 7 days\n",
    "\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d) or isinstance(m, nn.ConvTranspose1d):\n",
    "                # 对卷积层使用 Kaiming 正态初始化\n",
    "                nn.init.kaiming_uniform_(m.weight.data, nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "       \n",
    "        \n",
    "#PGNN处理后数据形状 : Batch_num, 28, Max_node_num, output_dimen ------> (Batch_num * Max_node_num), output_dimen, 28\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, max_node_num, out_dim = x.size()\n",
    "        x= x.permute(0, 2, 3, 1)\n",
    "        x = x.flatten(start_dim=0, end_dim=1)\n",
    "        #output shape: (Batch_num * Max_node_num), hidden_channels_2, 7\n",
    "        out = self.conv_layer(x) \n",
    "        #----------------------------------------------------\n",
    "        #output shape: (Batch_num * Max_node_num), 7, hidden_channels_2\n",
    "        out = out.permute(0, 2, 1)\n",
    "        #------------------------------------------------------\n",
    "       \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "537f6210-fc95-4891-9dee-663f4f287750",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, output_seq_len, num_nodes, drop_out = True):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers = 1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size * output_seq_len)\n",
    "        self.num_nodes = num_nodes\n",
    "        self.output_seq_len = output_seq_len\n",
    "        self.output_size = output_size\n",
    "        self.acti_func = nn.ReLU()\n",
    "        self.drop_out = drop_out\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = init.constant_(m.bias.data, 0.0) \n",
    "          \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch_num * Max_node_num), 7, hidden_size\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        if self.drop_out:\n",
    "            lstm_out = F.dropout(lstm_out, training=self.training, p=0.2)\n",
    "        lstm_out = self.acti_func(lstm_out)\n",
    "        # lstm_out shape: (Batch_num * Max_node_num), hidden_size \n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        out = self.fc(lstm_out)  \n",
    "        out= self.acti_func(out)\n",
    "        #out shape: Batch_num, Max_node_num, output_seq_len\n",
    "        out = out.view(-1, self.num_nodes, self.output_seq_len).permute(0, 2, 1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6ab117-6d0f-4996-b076-8e17ebefeaa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8b14a73d-f57d-41fb-a833-8a6460370b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "#PGNN\n",
    "anchor_num = 50\n",
    "num_nodes = 1466\n",
    "hidden_dimen = 128\n",
    "output_dimen = 64\n",
    "#-------------------------------------------------\n",
    "#CNN\n",
    "hidden_channels_1, hidden_channels_2 = 128, 64\n",
    "#-------------------------------------------------\n",
    "input_size = hidden_channels_2  # LSTM 输入的维度\n",
    "hidden_size = 128   # LSTM 隐藏层的维度\n",
    "output_seq_len = 14  # 预测14天\n",
    "output_size = 1\n",
    "\n",
    "#-------------------------------------------------\n",
    "#Domain Classifier\n",
    "class_hidden_1, class_hidden_2 = 258, 128\n",
    "k = 500\n",
    "alpha = 0.35\n",
    "beta= 0.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ee058a0e-8558-477f-bf96-0252704559c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "classifier = Domain_Classifier(hidden_channels_2, class_hidden_1, class_hidden_2, k, num_nodes, alpha)\n",
    "classifier_optimizer = optim.NAdam(classifier.parameters(), lr=3e-4, weight_decay=1e-5)\n",
    "classifier_scheduler = optim.lr_scheduler.StepLR(classifier_optimizer, step_size = 11, gamma = 0.55)\n",
    "criterion_BCE = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a6aa7951-ee33-49e5-b25c-deb51d2ab78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTMModel(input_size, hidden_size, output_size, output_seq_len, num_nodes)\n",
    "lstm_optimizer = optim.NAdam(lstm.parameters(), lr= 2e-4, weight_decay=1e-5) #Nadam 4e-4\n",
    "lstm_scheduler = optim.lr_scheduler.StepLR(lstm_optimizer, step_size=11, gamma=0.6)\n",
    "#------------------------------------------------------------------------------------------\n",
    "pgnn = PGNN(hidden_dimen, output_dimen, anchor_num)\n",
    "cnn_pgnn_optimizer = optim.NAdam(list(pgnn.parameters()), lr= 2e-4, weight_decay=1e-5) #Nadam 6e-4\n",
    "cnn_pgnn_scheduler = optim.lr_scheduler.StepLR(cnn_pgnn_optimizer, step_size=11, gamma=0.55)\n",
    "#------------------------------------------------------------------------------------------\n",
    "cnn = CNN_1D(output_dimen, hidden_channels_1, hidden_channels_2)\n",
    "cnn_optimizer = optim.NAdam(list(cnn.parameters()), lr= 2e-4, weight_decay=1e-5) #Nadam 6e-4\n",
    "cnn_scheduler = optim.lr_scheduler.StepLR(cnn_optimizer, step_size=11, gamma=0.55)\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "criterion_MSE = nn.MSELoss(reduction='none')  # 使用均方误差作为损失函数\n",
    "criterion_MAE = nn.L1Loss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7f31a679-97a7-4de1-b1e8-9793cdef4235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('D:/ThesisData/processed data/ModelPara/source_pgnn_cnn_lstm_14days.pth')\n",
    "pgnn.load_state_dict(checkpoint['pgnn_state_dict'])\n",
    "cnn.load_state_dict(checkpoint['cnn_state_dict'])\n",
    "lstm.load_state_dict(checkpoint['rnn_state_dict'])\n",
    "#cnn_pgnn_optimizer.load_state_dict(checkpoint['cnn_pgnn_optimizer_state_dict'])\n",
    "#cnn_optimizer.load_state_dict(checkpoint['cnn_optimizer_state_dict'])\n",
    "#lstm_optimizer.load_state_dict(checkpoint['lstm_optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e4317c-a1e5-4673-89a3-c08da8410f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "93997b7d-29f5-45d0-925f-11bc8b8e52d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train = torch.load(\"D:/ThesisData/processed data/SourceDomain/NEW/train_data_14days.h5\")\n",
    "target_train = torch.load(\"D:/ThesisData/processed data/TargetDomain/NEW/train_data_14days.h5\")\n",
    "vali_barcelona = torch.load(\"D:/ThesisData/processed data/TargetDomain/NEW/Barcelona_vali_data_14days.h5\")\n",
    "test_barcelona = torch.load(\"D:/ThesisData/processed data/TargetDomain/NEW/Barcelona_test_data_14days.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c935686d-20cf-4958-8a36-bff602a6e340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319ac721-3de6-409e-a331-d2db556cadf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f1c434e2-48d7-4a20-bd7f-aed8418a0586",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(source_train) // 4))  # 前半部分数据的索引\n",
    "subset_source = Subset(source_train, indices) \n",
    "combined_train = ConcatDataset([subset_source, target_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "67d55f16-bde0-43ee-865a-0f1b99429033",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 14\n",
    "train_loader = DataLoader(combined_train, batch_size=batch_size, shuffle=True)\n",
    "vali_loader = DataLoader(vali_barcelona, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_barcelona, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd77ed-b760-4a0f-85de-b5a0cd3f9db4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d30e8776-25ac-4d5c-9f0f-757d2255616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7d2c958b-4911-4466-a880-403d8f7cc512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train domain classifier:\n",
      "epoch 1, Train Domain Classifier: BCE=0.694\n",
      "train domain classifier:\n",
      "epoch 2, Train Domain Classifier: BCE=8.843\n",
      "train domain classifier:\n",
      "epoch 3, Train Domain Classifier: BCE=0.693\n",
      "--------------------------------------------\n",
      "train feature extractor:\n",
      "epoch 4, Train Barcelona: MSE=4571.6, MAE=48.4\n",
      "----Validation Barcelona: MSE=1063.5, MAE=19.9\n",
      "----Test Barcelona: MSE=1647.0, MAE=26.3\n",
      "--------------------------------------------\n",
      "train domain classifier:\n",
      "epoch 5, Train Domain Classifier: BCE=0.685\n",
      "train domain classifier:\n",
      "epoch 6, Train Domain Classifier: BCE=0.657\n",
      "train domain classifier:\n",
      "epoch 7, Train Domain Classifier: BCE=0.742\n",
      "--------------------------------------------\n",
      "train feature extractor:\n",
      "epoch 8, Train Barcelona: MSE=4631.2, MAE=50.0\n",
      "----Validation Barcelona: MSE=1005.3, MAE=19.8\n",
      "----Test Barcelona: MSE=1507.0, MAE=25.5\n",
      "--------------------------------------------\n",
      "train domain classifier:\n",
      "epoch 9, Train Domain Classifier: BCE=23.020\n",
      "train domain classifier:\n",
      "epoch 10, Train Domain Classifier: BCE=0.668\n",
      "train domain classifier:\n",
      "epoch 11, Train Domain Classifier: BCE=0.624\n",
      "--------------------------------------------\n",
      "train feature extractor:\n",
      "epoch 12, Train Barcelona: MSE=4981.8, MAE=52.3\n",
      "----Validation Barcelona: MSE=982.1, MAE=19.4\n",
      "----Test Barcelona: MSE=1450.7, MAE=24.8\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for h in range(epoch_num):\n",
    "\n",
    "\n",
    "    lstm.train()\n",
    "    pgnn.train()\n",
    "    cnn.train()\n",
    "    classifier.train()  \n",
    "\n",
    "    \n",
    "    if (h + 1) % 4 != 0:\n",
    "        for batch_input, batch_labels, batch_dist, batch_dist_arg, batch_masks, batch_min, batch_max, batch_dist_mask, batch_class_labels in train_loader:\n",
    "\n",
    "        \n",
    "            batch_input, batch_labels, batch_masks = batch_input.squeeze(), batch_labels.squeeze(), batch_masks.squeeze()\n",
    "            batch_min, batch_max = batch_min.squeeze(), batch_max.squeeze()\n",
    "  \n",
    "        \n",
    "            pgnn_output = pgnn(batch_input, batch_dist, batch_dist_arg, batch_dist_mask)\n",
    "            cnn_output = cnn(pgnn_output)\n",
    "            #---------------------------------------------------------------------\n",
    "            #class_input = GradientReversalLayer.apply(cnn_output, alpha)\n",
    "            batch_class = classifier(cnn_output)\n",
    "            #---------------------------------------------------------------------\n",
    "            loss_bce = criterion_BCE(batch_class, batch_class_labels)\n",
    "            #---------------------------------------------------------------------\n",
    "        \n",
    "\n",
    "            #---------------------------------------------------------------------\n",
    "            classifier_optimizer.zero_grad()\n",
    "            loss_bce.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(classifier.parameters(), 1.0)\n",
    "            \n",
    "\n",
    "        \n",
    "        print(\"train domain classifier:\")\n",
    "        print(f\"epoch {h + 1}, Train Domain Classifier: BCE={loss_bce.item():.3f}\")  \n",
    "        \n",
    "    \n",
    "    else:\n",
    "        for batch_input, batch_labels, batch_dist, batch_dist_arg, batch_masks, batch_min, batch_max, batch_dist_mask, batch_class_labels in train_loader:\n",
    "\n",
    "            \n",
    "            batch_input, batch_labels, batch_masks = batch_input.squeeze(), batch_labels.squeeze(), batch_masks.squeeze()\n",
    "            batch_min, batch_max = batch_min.squeeze(), batch_max.squeeze()\n",
    "\n",
    "\n",
    "            pgnn_output = pgnn(batch_input, batch_dist, batch_dist_arg, batch_dist_mask)\n",
    "            cnn_output = cnn(pgnn_output)\n",
    "            batch_outputs = lstm(cnn_output)  \n",
    "            #---------------------------------------------------------------------\n",
    "            batch_class = classifier(cnn_output)\n",
    "            #---------------------------------------------------------------------\n",
    "            loss_bce = criterion_BCE(batch_class, batch_class_labels)\n",
    "            #---------------------------------------------------------------------\n",
    "            loss_mse = criterion_MSE(batch_outputs, batch_labels) * batch_masks\n",
    "            loss_mae = criterion_MAE(batch_outputs, batch_labels) * batch_masks\n",
    "            loss_mse = loss_mse.sum() / batch_masks.sum()\n",
    "            loss_mae = loss_mae.sum() / batch_masks.sum()      \n",
    "            #----------------------------------------------------------------------\n",
    "        \n",
    "\n",
    "            #----------------------------------------------------------------------\n",
    "            lstm_optimizer.zero_grad()\n",
    "            cnn_optimizer.zero_grad()\n",
    "            cnn_pgnn_optimizer.zero_grad()\n",
    "            #----------------------------------------------------------------------\n",
    "            \n",
    "    \n",
    "            #----------------------------------------------------------------------\n",
    "            loss_total = loss_mse + beta * loss_bce\n",
    "            loss_total.backward()\n",
    "\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(classifier.parameters(), 1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(lstm.parameters(), 1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(cnn.parameters(), 1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(pgnn.parameters(), 1.0)\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "        #----------------------------------------------------------------------\n",
    "        inverse_outputs = batch_outputs * (batch_max - batch_min) + batch_min\n",
    "        inverse_labels = batch_labels * (batch_max - batch_min) + batch_min\n",
    "\n",
    "        \n",
    "        loss_mse = criterion_MSE(inverse_outputs, inverse_labels) * batch_masks\n",
    "        loss_mae = criterion_MAE(inverse_outputs, inverse_labels) * batch_masks\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss_mse = loss_mse.sum() / batch_masks.sum()\n",
    "        loss_mae = loss_mae.sum() / batch_masks.sum()    \n",
    "        \n",
    "        del batch_outputs\n",
    "        del batch_labels\n",
    "        del inverse_outputs\n",
    "        del inverse_labels\n",
    "        del batch_min\n",
    "        del batch_max\n",
    "\n",
    "        if (h + 1) % 4 == 0: \n",
    "            print(\"--------------------------------------------\")\n",
    "            print(\"train feature extractor:\")\n",
    "            print(f\"epoch {h + 1}, Train Barcelona: MSE={loss_mse.item():.1f}, MAE={loss_mae.item():.1f}\")            \n",
    "       \n",
    "\n",
    "        \n",
    "#--------------------------------------------------\n",
    "    lstm.eval()\n",
    "    pgnn.eval()\n",
    "    cnn.eval()  \n",
    "    classifier.eval()\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if (h + 1) % 4 == 0: \n",
    "            for batch_input, batch_labels, batch_dist, batch_dist_arg, batch_masks, batch_min, batch_max, batch_dist_mask, _ in vali_loader:\n",
    "\n",
    "                batch_input, batch_labels, batch_masks = batch_input.squeeze(), batch_labels.squeeze(), batch_masks.squeeze()\n",
    "                batch_min, batch_max = batch_min.squeeze(), batch_max.squeeze()\n",
    "\n",
    "                pgnn_output = pgnn(batch_input, batch_dist, batch_dist_arg, batch_dist_mask)\n",
    "                cnn_output = cnn(pgnn_output)\n",
    "        \n",
    "                batch_outputs = lstm(cnn_output)\n",
    "          \n",
    "    \n",
    "                #----------------------------------------------------------------------\n",
    "                Barcelona_vali_outputs = batch_outputs * (batch_max - batch_min) + batch_min\n",
    "                Barcelona_vali_labels = batch_labels * (batch_max - batch_min) + batch_min\n",
    "\n",
    "                loss_mse = criterion_MSE(Barcelona_vali_outputs, Barcelona_vali_labels) * batch_masks\n",
    "                loss_mae = criterion_MAE(Barcelona_vali_outputs, Barcelona_vali_labels) * batch_masks\n",
    "                loss_mse_ = loss_mse.sum() / batch_masks.sum()\n",
    "                loss_mae = loss_mae.sum() / batch_masks.sum()    \n",
    "\n",
    "            \n",
    "            print(f\"----Validation Barcelona: MSE={loss_mse_.item():.1f}, MAE={loss_mae.item():.1f}\")\n",
    "         \n",
    "\n",
    "            for batch_input, batch_labels, batch_dist, batch_dist_arg, batch_masks, batch_min, batch_max, batch_dist_mask, _ in test_loader:\n",
    "\n",
    "                batch_input, batch_labels, batch_masks = batch_input.squeeze(), batch_labels.squeeze(), batch_masks.squeeze()\n",
    "                batch_min, batch_max = batch_min.squeeze(), batch_max.squeeze()\n",
    "\n",
    "                pgnn_output = pgnn(batch_input, batch_dist, batch_dist_arg, batch_dist_mask)\n",
    "                cnn_output = cnn(pgnn_output)\n",
    "        \n",
    "                batch_outputs = lstm(cnn_output)\n",
    "          \n",
    "    \n",
    "                #----------------------------------------------------------------------\n",
    "                Barcelona_test_outputs = batch_outputs * (batch_max - batch_min) + batch_min\n",
    "                Barcelona_test_labels = batch_labels * (batch_max - batch_min) + batch_min\n",
    "\n",
    "                loss_mse = criterion_MSE(Barcelona_test_outputs, Barcelona_test_labels) * batch_masks\n",
    "                loss_mae = criterion_MAE(Barcelona_test_outputs, Barcelona_test_labels) * batch_masks\n",
    "                loss_mse = loss_mse.sum() / batch_masks.sum()\n",
    "                loss_mae = loss_mae.sum() / batch_masks.sum()    \n",
    "\n",
    "            \n",
    "            print(f\"----Test Barcelona: MSE={loss_mse.item():.1f}, MAE={loss_mae.item():.1f}\")\n",
    "            print(\"--------------------------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            if h == epoch_num-1 or loss_mse_ <=1000:\n",
    "            #if h == epoch_num-1 or (loss_mse_ <=1293 and h > 0):\n",
    "                pgnn_cnn_results = {\"Barcelona_vali\": Barcelona_vali_outputs.detach().numpy(), \"Barcelona_vali_label\": Barcelona_vali_labels.detach().numpy(),\n",
    "                                    \"Barcelona_test\": Barcelona_test_outputs.detach().numpy(), \"Barcelona_test_label\": Barcelona_test_labels.detach().numpy(),\n",
    "                                    \"Barcelona_node_num\": 1273}\n",
    "\n",
    "                save_data(pgnn_cnn_results,\"D:/ThesisData/processed data/TargetDomain/NEW/results/pgnn_cnn_lstm_gan_results.h5\" )    \n",
    "                break\n",
    "\n",
    "    classifier_optimizer.step()\n",
    "    lstm_optimizer.step()\n",
    "    cnn_optimizer.step()\n",
    "    cnn_pgnn_optimizer.step()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c99e573f-217c-4c5b-ace7-925da202e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'pgnn_state_dict': pgnn.state_dict(),\n",
    "    'lstm_state_dict': lstm.state_dict(),\n",
    "    'cnn_state_dict': cnn.state_dict(),\n",
    "    'classifier_state_dict': classifier.state_dict(),\n",
    "    'lstm_optimizer_state_dict': lstm_optimizer.state_dict(),\n",
    "    'cnn_pgnn_optimizer_state_dict': cnn_pgnn_optimizer.state_dict(),\n",
    "    'classifier_optimizer_state_dict': classifier_optimizer.state_dict(),\n",
    "}, 'D:/ThesisData/processed data/ModelPara/source_pgnn_cnn_lstm_gan.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0af130-0a6a-4289-a1d2-4fbd25f1e851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bf695b-c4b0-47ca-a2f3-6c99baeb89d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
