{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fddd1cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import pymetis\n",
    "import networkx as nx\n",
    "import time\n",
    "from networkx.algorithms import community\n",
    "from random import shuffle\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pdb\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "from torch_geometric.data import Data\n",
    "import pywt\n",
    "from scipy.stats import norm\n",
    "import scipy.interpolate as interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a95d9296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, file_path):\n",
    "    with open(file_path , 'wb') as f:\n",
    "        pickle.dump(data,f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b4400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_hdf5(data, file_path):\n",
    "    with h5py.File(file_path, 'w') as f:\n",
    "        for key, value in data.items():\n",
    "            f.create_dataset(key, data=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b7b2527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data(file_path):\n",
    "    file = open(file_path,\"rb\")\n",
    "    raw_data = pickle.load(file)  \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "395a0673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_transform(data, level = 4):\n",
    "    volume = data[:,:,:]\n",
    "    time_steps = volume.shape[0]\n",
    "    #volume shape:天数，点数，点特征\n",
    "    trend_patterns = []\n",
    "    periodic_patterns = []\n",
    "    \n",
    "    for i in range(volume.shape[1]):\n",
    "        #对每一个节点的四个方向进行小波变换\n",
    "        #pywt.wavedec输出是一个包含几个数组的列表 ,提取低频/逼近系数\n",
    "        four_dir_coeffs = [pywt.wavedec(volume[:, i, j], wavelet= 'db1', level = level) for j in range(volume.shape[2])]\n",
    "        #重构信号仅使用逼近系数\n",
    "        freq_coeffs = []\n",
    "        \n",
    "        for each_dir_coeffs in four_dir_coeffs:\n",
    "            approximation = each_dir_coeffs[0]\n",
    "            details = each_dir_coeffs[1:] \n",
    "            coeffs_trend_only = [approximation] + [np.zeros_like(detail) for detail in details]\n",
    "            freq_coeffs.append(pywt.waverec(coeffs_trend_only, wavelet= 'db1'))\n",
    "\n",
    "        trend_patterns.append(np.transpose(np.array(freq_coeffs)))\n",
    "        #重构去除趋势后的周期部分\n",
    "        four_dir_period = [pywt.waverec([np.zeros_like(each_dir_coeffs[0])] + each_dir_coeffs[1:], wavelet= 'db1') for each_dir_coeffs in four_dir_coeffs]\n",
    "        periodic_patterns.append(np.transpose(np.array(four_dir_period)))  \n",
    "     \n",
    "    #Shape: node_num, time_step, 4\n",
    "    trend_patterns = np.array(trend_patterns)\n",
    "    periodic_patterns = np.array(periodic_patterns) \n",
    "    \n",
    "    \n",
    "    #Change FROM node_num, time_step, 4 TO time step, node_num, 4\n",
    "    return np.transpose(np.array(trend_patterns), (1,0,2)),  np.transpose(np.array(periodic_patterns), (1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd76a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_len, pred_len):\n",
    "    sequences = []\n",
    "    for i in range(data.shape[0] - seq_len - pred_len + 1):\n",
    "        input_seq = data[i:i + seq_len, :, :]\n",
    "        target_seq = data[i + seq_len:i + seq_len + pred_len, :, :]\n",
    "        sequences.append((input_seq, target_seq))\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def sequence_concatenate(sequences):\n",
    "    for i in range(len(sequences)):\n",
    "        input_seq, target_seq = sequences[i]\n",
    "        input_seq = np.expand_dims(input_seq, axis=0)\n",
    "        target_seq = np.expand_dims(target_seq, axis=0)\n",
    "        if i == 0:\n",
    "            input_result = input_seq\n",
    "            target_result = target_seq\n",
    "        else:\n",
    "            input_result = np.concatenate((input_result, input_seq), axis=0)\n",
    "            target_result = np.concatenate((target_result, target_seq), axis=0)\n",
    "    return input_result, target_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "134985bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_target_divide(city_name, file_root_path, seq_len, pred_len):\n",
    "    scaler = MinMaxScaler()\n",
    "    file_path = file_root_path + city_name + \"/volume_data/volume_file.h5\"\n",
    "    data = open_data(file_path)\n",
    "    city_node_num = data.shape[1]\n",
    "    #----------只取横跨疫情前后90天的数据（从2020年2月1日起的120天）-----------------------------------\n",
    "    #----------对于Target City Barcelona,只提取45天的数据\n",
    "    if city_name != \"Barcelona\":\n",
    "        data = data[29:29+100, :, :]\n",
    "    else:\n",
    "        data = data[29:29+50, :, :]\n",
    "    \n",
    "    max_data = np.max(data[:,:,1:])\n",
    "    print(f'Before filtering, max volume value: {max_data}')\n",
    "    \n",
    "    #filter the data, if larger than 250, set as 250; if smaller than 10, set as 0.\n",
    "    upper_bound, lower_bound = 250., 10.\n",
    "    data[:, :, 1:][data[:, :, 1:] > upper_bound] = upper_bound\n",
    "    data[:, :, 1:][data[:, :, 1:] < lower_bound] = 0.\n",
    "    \n",
    "    print(f\"After filtering, max volume value: {np.max(data[:,:,1:])}\")\n",
    "\n",
    "    day_num, node_num, feature_num = data.shape[0], data.shape[1], data.shape[2]-1\n",
    "    #---------归一化数据-----------------\n",
    "    volume_data = data[ :, :, 1:]\n",
    "    nodes_id = np.expand_dims(data[:,:,0], axis= -1)\n",
    "    \n",
    "    norm_volume_data = scaler.fit_transform(np.reshape(volume_data, (-1, feature_num)))\n",
    "    norm_volume_data = np.reshape(norm_volume_data, (day_num, node_num, feature_num))\n",
    "    norm_volume_data = np.concatenate((nodes_id, norm_volume_data), axis = -1)\n",
    "    \n",
    "    min_vals = scaler.data_min_\n",
    "    max_vals = scaler.data_max_\n",
    "    \n",
    "    sequences = create_sequences(norm_volume_data, seq_len, pred_len)\n",
    "    input_result, target_result = sequence_concatenate(sequences)\n",
    "    \n",
    "    #Input shape: 100//seq , seq, Node_num, [Node_ID+ Node_Feature_num(5)]\n",
    "    return input_result, target_result, city_node_num, min_vals, max_vals\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e9ac439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Wavelet_Transform(city_name, input_result, target_result, max_len):\n",
    "    target_template = np.zeros((target_result.shape[0], target_result.shape[1], max_len, target_result.shape[3] +1))\n",
    "    \n",
    "    trend_template = np.zeros((input_result.shape[0], input_result.shape[1], max_len, input_result.shape[3]))\n",
    "    period_template = np.zeros((input_result.shape[0], input_result.shape[1], max_len, input_result.shape[3])) \n",
    "    \n",
    "    for i in range(input_result.shape[0]):\n",
    "        #----------Use Wavelet Transform to extract TREND AND PERIODIC FEATURES---\n",
    "        trend_parts, periodic_parts = wavelet_transform(input_result[i, :, :, 1:], level = 4)\n",
    "        #----------------------------------------------------------------------------\n",
    "        trend_template[i, :, :input_result.shape[2], 1:] = trend_parts\n",
    "        trend_template[i, :, :input_result.shape[2], 0] = input_result[i, :, :, 0]\n",
    "        \n",
    "        \n",
    "        #----------------------------------------------------------------------------\n",
    "        period_template[i, :, :input_result.shape[2], 1:] = periodic_parts\n",
    "        period_template[i, :, :input_result.shape[2], 0] = input_result[i, :, :, 0]\n",
    "        \n",
    "        \n",
    "        #------------------Target covers 4-Direction Volumes AND Target LABLE--------------------------\n",
    "        target_template[i, :, :input_result.shape[2], 1:5] = target_result[i, :, :, 1:5]\n",
    "        target_template[i, :, :input_result.shape[2], 0] = target_result[i, :, :, 0]\n",
    "        \n",
    "        if city_name == \"Barcelona\":\n",
    "            target_template[i, :, :input_result.shape[2], 5] = 0.0\n",
    "            \n",
    "        if city_name == \"Antwerp\":\n",
    "            target_template[i, :, :input_result.shape[2], 5] = 1.0\n",
    "            \n",
    "        elif city_name == \"Bangkok\":      \n",
    "            target_template[i, :, :input_result.shape[2], 5] = 1.0\n",
    "    \n",
    "        #elif city_name == \"Moscow\":\n",
    "            #target_template[i, :, :input_result.shape[2], 5] = 2.0\n",
    "        \n",
    "    return trend_template, period_template, target_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90d871f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c6464bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering, max volume value: 380.0\n",
      "After filtering, max volume value: 250.0\n",
      "One city finished\n"
     ]
    }
   ],
   "source": [
    "seq_len, pred_len = 20, 10\n",
    "city_names = [\"Barcelona\", \"Antwerp\",\"Bangkok\"]\n",
    "file_root_path = \"D:ThesisData/processed data/\"\n",
    "max_len = 21842  #The Node Num of Bangkok 21464------Mscow:21842\n",
    "\n",
    "input_result, target_result, city_node_num, min_vals, max_vals = input_target_divide(city_names[2], file_root_path, seq_len, pred_len)\n",
    "trend_template, period_template, target_template = Wavelet_Transform(city_names[2], input_result, target_result, max_len)\n",
    "\n",
    "print(\"One city finished\")\n",
    "city_dictionary = {\"trend_template\": trend_template, \"period_template\": period_template, \"target_template\": target_template,\n",
    "                   \"city_node_num\":city_node_num, \"min_vals\": min_vals, \"max_vals\": max_vals}\n",
    "\n",
    "save_data_hdf5(city_dictionary, file_root_path + city_names[2] + \"/input_target/input_target.h5\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7680d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(361, 111260, 5)\n"
     ]
    }
   ],
   "source": [
    "#file_root_path = \"D:ThesisData/processed data/\"\n",
    "#file_path = file_root_path + \"Antwerp\" + \"/volume_data/volume_file.h5\"\n",
    "#data = open_data(file_path)\n",
    "#print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e4259f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab0d3db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
