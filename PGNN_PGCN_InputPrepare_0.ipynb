{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fddd1cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import pymetis\n",
    "import networkx as nx\n",
    "import time\n",
    "from networkx.algorithms import community\n",
    "from random import shuffle\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric as tg\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch.nn import init\n",
    "import pdb\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "from torch_geometric.data import Data\n",
    "import torch.optim as optim\n",
    "import pywt\n",
    "from scipy.stats import norm\n",
    "import scipy.interpolate as interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a95d9296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, file_path):\n",
    "    with open(file_path , 'wb') as f:\n",
    "        pickle.dump(data,f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b4400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_hdf5(data, file_path):\n",
    "    with h5py.File(file_path, 'w') as f:\n",
    "        for key, value in data.items():\n",
    "            f.create_dataset(key, data=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b7b2527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data(file_path):\n",
    "    file = open(file_path,\"rb\")\n",
    "    raw_data = pickle.load(file)  \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "395a0673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_transform(data, level = 4):\n",
    "    volume = data[:,:,:]\n",
    "    time_steps = volume.shape[0]\n",
    "    #volume shape:天数，点数，点特征\n",
    "    trend_patterns = []\n",
    "    periodic_patterns = []\n",
    "    \n",
    "    for i in range(volume.shape[1]):\n",
    "        #对每一个节点的四个方向进行小波变换\n",
    "        #pywt.wavedec输出是一个包含几个数组的列表 ,提取低频/逼近系数\n",
    "        four_dir_coeffs = [pywt.wavedec(volume[:, i, j], wavelet= 'db1', level = level) for j in range(volume.shape[2])]\n",
    "        #重构信号仅使用逼近系数\n",
    "        freq_coeffs = []\n",
    "        \n",
    "        for each_dir_coeffs in four_dir_coeffs:\n",
    "            approximation = each_dir_coeffs[0]\n",
    "            details = each_dir_coeffs[1:] \n",
    "            coeffs_trend_only = [approximation] + [np.zeros_like(detail) for detail in details]\n",
    "            freq_coeffs.append(pywt.waverec(coeffs_trend_only, wavelet= 'db1'))\n",
    "\n",
    "        trend_patterns.append(np.transpose(np.array(freq_coeffs)))\n",
    "        #重构去除趋势后的周期部分\n",
    "        four_dir_period = [pywt.waverec([np.zeros_like(each_dir_coeffs[0])] + each_dir_coeffs[1:], wavelet= 'db1') for each_dir_coeffs in four_dir_coeffs]\n",
    "        periodic_patterns.append(np.transpose(np.array(four_dir_period)))  \n",
    "     \n",
    "    #Shape: node_num, time_step, 4\n",
    "    trend_patterns = np.array(trend_patterns)\n",
    "    periodic_patterns = np.array(periodic_patterns) \n",
    "    \n",
    "    \n",
    "    #Change FROM node_num, time_step, 4 TO time step, node_num, 4\n",
    "    return np.transpose(np.array(trend_patterns), (1,0,2)),  np.transpose(np.array(periodic_patterns), (1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd76a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_len, pred_len):\n",
    "    sequences = []\n",
    "    for i in range(data.shape[0] - seq_len - pred_len + 1):\n",
    "        input_seq = data[i:i + seq_len, :, :]\n",
    "        target_seq = data[i + seq_len:i + seq_len + pred_len, :, :]\n",
    "        sequences.append((input_seq, target_seq))\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def sequence_concatenate(sequences):\n",
    "    for i in range(len(sequences)):\n",
    "        input_seq, target_seq = sequences[i]\n",
    "        input_seq = np.expand_dims(input_seq, axis=0)\n",
    "        target_seq = np.expand_dims(target_seq, axis=0)\n",
    "        if i == 0:\n",
    "            input_result = input_seq\n",
    "            target_result = target_seq\n",
    "        else:\n",
    "            input_result = np.concatenate((input_result, input_seq), axis=0)\n",
    "            target_result = np.concatenate((target_result, target_seq), axis=0)\n",
    "    return input_result, target_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "134985bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_target_divide(city_name, file_root_path, seq_len, pred_len):\n",
    "    scaler = MinMaxScaler()\n",
    "    file_path = file_root_path + city_name + \"/volume_data/volume_file.h5\"\n",
    "    data = open_data(file_path)\n",
    "    city_node_num = data.shape[1]\n",
    "    #----------只取横跨疫情前后100天的数据（从2020年2月1日起的100天）-----------------------------------\n",
    "    data = data[181+30:181+30+100, :, :]\n",
    "    day_num, node_num, feature_num = data.shape[0], data.shape[1], data.shape[2]-1\n",
    "    #---------归一化数据-----------------\n",
    "    volume_data = data[ :, :, 1:]\n",
    "    nodes_id = np.expand_dims(data[:,:,0], axis= -1)\n",
    "    \n",
    "    norm_volume_data = scaler.fit_transform(np.reshape(volume_data, (day_num * node_num, feature_num)))\n",
    "    norm_volume_data = np.reshape(norm_volume_data, (day_num, node_num, feature_num))\n",
    "    norm_volume_data = np.concatenate((nodes_id, norm_volume_data), axis = -1)\n",
    "    \n",
    "    sequences = create_sequences(norm_volume_data, seq_len, pred_len)\n",
    "    input_result, target_result = sequence_concatenate(sequences)\n",
    "    \n",
    "    #Input shape: 100//seq , seq, Node_num, [Node_ID+ Node_Feature_num(5)]\n",
    "    return input_result, target_result, city_node_num\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e9ac439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Wavelet_Transform(city_name, input_result, target_result, max_len):\n",
    "    target_template = np.zeros((target_result.shape[0], target_result.shape[1], max_len, target_result.shape[3] + 1))\n",
    "    \n",
    "    trend_template = np.zeros((input_result.shape[0], input_result.shape[1], max_len, input_result.shape[3] + 1) )\n",
    "    period_template = np.zeros((input_result.shape[0], input_result.shape[1], max_len, input_result.shape[3] + 1)) \n",
    "    for i in range(input_result.shape[0]):\n",
    "        #----------Use Wavelet Transform to extract TREND AND PERIODIC FEATURES---\n",
    "        trend_parts, periodic_parts = wavelet_transform(input_result[i, :, :, 1:], level = 4)\n",
    "        #----------------------------------------------------------------------------\n",
    "        trend_template[i, :, :input_result.shape[2], 1:5] = trend_parts\n",
    "        trend_template[i, :, :input_result.shape[2], 0] = input_result[i, :, :, 0]\n",
    "        \n",
    "        \n",
    "        #----------------------------------------------------------------------------\n",
    "        period_template[i, :, :input_result.shape[2], 1:5] = periodic_parts\n",
    "        period_template[i, :, :input_result.shape[2], 0] = input_result[i, :, :, 0]\n",
    "        \n",
    "        \n",
    "        #------------------Target covers 4-Direction Volumes AND Target LABLE--------------------------\n",
    "        target_template[i, :, :input_result.shape[2], 1:5] = target_result[i, :, :, 1:5]\n",
    "        target_template[i, :, :input_result.shape[2], 0] = target_result[i, :, :, 0]\n",
    "        \n",
    "        if city_name == \"Barcelona\":\n",
    "            trend_template[i, :, :input_result.shape[2], 5] = 1.0\n",
    "            period_template[i, :, :input_result.shape[2], 5] = 1.0\n",
    "            \n",
    "            target_template[i, :, :input_result.shape[2], 5] = 1.0\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            trend_template[i, :, :input_result.shape[2], 5] = 0.0\n",
    "            period_template[i, :, :input_result.shape[2], 5] = 0.0\n",
    "            \n",
    "            target_template[i, :, :input_result.shape[2], 5] = 0.0\n",
    "        \n",
    "    return trend_template, period_template, target_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90d871f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c6464bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One city finished\n"
     ]
    }
   ],
   "source": [
    "seq_len, pred_len = 20, 10\n",
    "city_names = [\"Barcelona\", \"Antwerp\"]\n",
    "file_root_path = \"D:ThesisData/processed data/\"\n",
    "max_len = 111260#The Node Num of Antwerp\n",
    "\n",
    "input_result, target_result, city_node_num = input_target_divide(city_names[1], file_root_path, seq_len, pred_len)\n",
    "trend_template, period_template, target_template = Wavelet_Transform(city_names[1], input_result, target_result, max_len)\n",
    "print(\"One city finished\")\n",
    "city_dictionary = {\"trend_template\": trend_template, \"period_template\": period_template, \"target_template\": target_template, \"city_node_num\":city_node_num}\n",
    "save_data_hdf5(city_dictionary, file_root_path + city_names[1] + \"/input_target/input_target.h5\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7680d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722c4f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab0d3db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
