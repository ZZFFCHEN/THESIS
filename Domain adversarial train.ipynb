{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a36fde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import pymetis\n",
    "import networkx as nx\n",
    "import time\n",
    "from networkx.algorithms import community\n",
    "from random import shuffle\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric as tg\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch.nn import init\n",
    "import pdb\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch_geometric.data import Data\n",
    "import torch.optim as optim\n",
    "import pywt\n",
    "from scipy.stats import norm\n",
    "import scipy.interpolate as interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2aa081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data(file_path):\n",
    "    file = open(file_path,\"rb\")\n",
    "    raw_data = pickle.load(file)  \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fb26fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearOperate(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen):\n",
    "        super(NonLinearOperate, self).__init__()  #类NonLinearLayer继承父类nn.Module的初始化方法\n",
    "        self.layer_1 = nn.Linear(input_dimen, hidden_dimen)\n",
    "        self.layer_2 = nn.Linear(hidden_dimen, output_dimen)\n",
    "        self.acti_func = nn.ReLU()\n",
    "        for m in self.modules():#遍历所有子模块\n",
    "#Check if each sub-module is an example of the class nn.Linear\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))#Use Xavier initialization\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = init.constant_(m.bias.data, 0.0)\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.acti_func(x)\n",
    "        x = self.layer_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61dad852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGNN_Layer(nn.Module):\n",
    "    def __init__(self, input_dimen, output_dimen, max_ach_num):\n",
    "        super(PGNN_Layer, self).__init__()\n",
    "        self.input_dimen = input_dimen\n",
    "        self.output_dimen = output_dimen\n",
    "        self.distance_calculate = NonLinearOperate(1, output_dimen, 1)\n",
    "        self.acti_func = nn.ReLU()\n",
    "        self.linear_hidden = nn.Linear(2*input_dimen, output_dimen)\n",
    "        self.out_transition = nn.Linear(output_dimen,1)\n",
    "        self.linear_out_position = nn.Linear(max_ach_num,input_dimen)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))#Use Xavier initialization\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = init.constant_(m.bias.data, 0.0)                \n",
    "                    \n",
    "    def forward(self, node_features, dists_max, dists_argmax):\n",
    "        dists_max = self.distance_calculate(dists_max.unsqueeze(-1)).squeeze()\n",
    "        subset_features = node_features[dists_argmax.flatten(), :]\n",
    "        subset_features = subset_features.reshape(dists_argmax.shape[0], dists_argmax.shape[1], subset_features.shape[1])\n",
    "        messages = subset_features * dists_max.unsqueeze(-1)\n",
    "        feature_self = node_features.unsqueeze(1).repeat(1, dists_max.shape[1],1)\n",
    "        messages = torch.concat((messages, feature_self), dim = -1) #N行M列D维\n",
    "        messages = self.linear_hidden(messages).squeeze()#将输出维度改为Output Dimen,即n*m*output_dimen\n",
    "        messages = self.acti_func(messages) \n",
    "        output_transition = self.out_transition(messages).squeeze(-1) #n * m * output_dimen to n * m\n",
    "        output_position = self.linear_out_position(output_transition)\n",
    "        output_structure = torch.mean(messages, dim=1)#n*output_dimen\n",
    "        \n",
    "        return output_position, output_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9331a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGNN(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen, max_ach_num, layer_num = 1, drop_out = True):\n",
    "        super(PGNN, self).__init__()\n",
    "        self.drop_out = drop_out\n",
    "        self.layer_num = layer_num\n",
    "        self.input_layer = nn.Linear(input_dimen, hidden_dimen)\n",
    "        self.last_layer = nn.Linear(hidden_dimen, input_dimen)\n",
    "        self.max_ach_num = max_ach_num\n",
    "        if self.layer_num == 1:\n",
    "            self.gnn_operate_1 = PGNN_Layer(hidden_dimen, output_dimen, max_ach_num)#输出维度是node_num * hidden_dimen\n",
    "            \n",
    "        if self.layer_num > 1:\n",
    "            self.gnn_hidden = nn.ModuleList([PGNN_Layer(hidden_dimen, hidden_dimen, max_ach_num) for i in range(0, layer_num)])\n",
    "            self.gnn_output_layer = PGNN_Layer(hidden_dimen, output_dimen)\n",
    "                \n",
    "        \n",
    "    def forward(self, x, dist_max_sets, dist_argmax_sets):\n",
    "        \n",
    "        x = self.input_layer(x)\n",
    "        if self.layer_num == 1:\n",
    "            x_position, x = self.gnn_operate_1(x, dist_max_sets[0,:,:], dist_argmax_sets[0,:,:])\n",
    "            if self.drop_out:\n",
    "                x = F.dropout(x, training=self.training)\n",
    "            x_position = self.last_layer(x_position)\n",
    "            #print(f\"pgnn output_layer size: {x_position.shape}\")\n",
    "            return x_position\n",
    "    \n",
    "        if self.layer_num > 1:\n",
    "            for i in range(self.layer_num):\n",
    "                _, x = self.gnn_hidden[i](x, dist_max_sets[i,:,:], dist_argmax_sets[i,:,:])\n",
    "                if self.drop_out:\n",
    "                    x = F.dropout(x, training=self.training)\n",
    "                  \n",
    "            _ = F.normalize(_, p=2, dim=-1)\n",
    "            x_position = self.last_layer(_)    \n",
    "            \n",
    "            \n",
    "            return x_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab1f6bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class P_GCN(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen, max_ach_num, layer_num = 1, drop_out = True):\n",
    "        super(P_GCN, self).__init__()\n",
    "        self.max_ach_num = max_ach_num\n",
    "        self.layer_num = layer_num\n",
    "        self.drop_out = drop_out\n",
    "        self.input_layer = nn.Linear(input_dimen, hidden_dimen)\n",
    "        self.p_gcn_block = nn.Sequential(PGNN_Layer(hidden_dimen, hidden_dimen, max_ach_num), \n",
    "                                         GCNConv(hidden_dimen, hidden_dimen, add_self_loops=True))\n",
    "        self.acti_func = nn.ReLU()   \n",
    "        \n",
    "        if layer_num == 1:\n",
    "            self.gcn_p_layers = self.p_gcn_block\n",
    "        if layer_num > 1:\n",
    "            self.gcn_p_layers = nn.ModuleList([self.p_gcn_block for i in range(num_layer)])\n",
    "        self.output_layer = nn.Linear(hidden_dimen, output_dimen)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = init.constant_(m.bias.data, 0.0)   \n",
    "        \n",
    "        \n",
    "    def forward(self, x, edge_index, dist_max, dist_argmax):#GCN_P_input shape: node_num, 4\n",
    "        \n",
    "        x_ = self.input_layer(x)\n",
    "        \n",
    "        if self.layer_num == 1:\n",
    "            x_position, _ = self.gcn_p_layers[0](x_, dist_max[0,:,:], dist_argmax[0,:,:])\n",
    "            if self.drop_out:\n",
    "                x_position = F.dropout(x_position, training=self.training)\n",
    "            x = self.gcn_p_layers[1](x_position, edge_index)\n",
    "            if self.drop_out:\n",
    "                x = F.dropout(x, training=self.training)\n",
    "            x = self.acti_func(x + x_)\n",
    "            \n",
    "        else:\n",
    "            for i in range(self.layer_num):\n",
    "                x_position, _ = self.gcn_p_layers[i][0](x_, dist_max[i,:,:], dist_argmax[i,:,:])\n",
    "                if self.drop_out:\n",
    "                    x_position = F.dropout(x_position, training=self.training)\n",
    "                x = self.gcn_p_layers[i][1](x_position, edge_index)\n",
    "                if self.drop_out:\n",
    "                    x = F.dropout(x, training=self.training)\n",
    "                x = self.acti_func(x + x_)\n",
    "                x_ = x\n",
    "                    \n",
    "        x = self.acti_func(self.output_layer(x))\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82093d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入数据shape: node_num, 4, time_step(20)\n",
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels_1, hidden_channels_2, out_channels):\n",
    "        super(CNN_1D, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = input_channels, out_channels = hidden_channels_1, kernel_size = 3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_channels_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels = hidden_channels_1, out_channels = hidden_channels_2, kernel_size = 3, padding=0),\n",
    "            nn.BatchNorm1d(hidden_channels_2),\n",
    "            nn.ReLU(), #len: 18\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),#(18-2)/2 +1 =9\n",
    "            nn.ConvTranspose1d(in_channels= hidden_channels_2,\n",
    "                               out_channels=out_channels,\n",
    "                               kernel_size=4,\n",
    "                               stride= 2, \n",
    "                               padding=0))\n",
    "\n",
    "        self.fc1 = nn.Linear(out_channels, 4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        #output shape: change from batch_num, out_channels, t-step to batch_num, t-step, out_channels\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = self.fc1(out)\n",
    "        #output shape: batch_num, t-step, 4\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922b78b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "063afe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, pred_len, output_dimen = 4, num_layers = 2):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_dimen = hidden_dimen\n",
    "        self.output_dimen = output_dimen\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_layer_1 = nn.Linear(input_dimen, hidden_dimen)\n",
    "        self.gru_layers = nn.GRU(hidden_dimen, hidden_dimen, num_layers, batch_first = True)\n",
    "        self.linear_layer_2 = nn.Linear(hidden_dimen, output_dimen)\n",
    "        self.k = pred_len #Futre k-step to predict \n",
    "        \n",
    "         \n",
    "    #gru输入格式：node_num, t-steps, hidden_dimen\n",
    "    def forward(self, pgnn_t_step_outs, extractor_outputs):  \n",
    "        x = torch.cat((pgnn_t_step_outs, extractor_outputs), dim = -1)\n",
    "        batch_size, seq_len, feat_dim = x.size()\n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_dimen)\n",
    "        x = self.linear_layer_1(x)\n",
    "        outputs, _ = self.gru_layers(x, h_0)#outputs shape: batch_size, seq_len, hidden dim. \n",
    "        layer_2_input = outputs[:, -self.k:, :]\n",
    "        x = self.linear_layer_2(layer_2_input) #x shape: batch_size, k steps, 4.\n",
    "        #linear_layer_2 trasnfer last dimension from hidden dim to 4\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6974333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T_Step_PGNN(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen, max_ach_num, layer_num = 1):\n",
    "        super(T_Step_PGNN, self).__init__()\n",
    "        self.pgnn_model = PGNN(input_dimen, hidden_dimen, output_dimen, max_ach_num)\n",
    "        \n",
    "    def forward(self, p_gnn_input_sets, subgraph_nodes, dist_max, dist_argmax): \n",
    "        p_gnn_template = torch.zeros((p_gnn_input_sets.shape[0], p_gnn_input_sets.shape[1], 4))\n",
    "        indices = [torch.isin(p_gnn_input_sets[:, :, 0], graph_node).unsqueeze(-1).expand_as(p_gnn_template) for graph_node in subgraph_nodes]\n",
    "        for i in range(len(indices)):\n",
    "            subgraph = p_gnn_input_sets[:,:,1:5][indices[i]].view(p_gnn_input_sets.shape[0], -1, 4)\n",
    "            p_gnn_outputs = torch.empty((0,subgraph.shape[1],subgraph.shape[2]))\n",
    "            subgraph_node_num = subgraph_nodes[i].shape[0]\n",
    "            \n",
    "            for t in range(subgraph.shape[0]):\n",
    "                p_gnn_t_step = self.pgnn_model(subgraph[t,:,:], dist_max[i,:,:subgraph_node_num,:], dist_argmax[i,:,:subgraph_node_num,:])\n",
    "                p_gnn_outputs = torch.cat((p_gnn_outputs, p_gnn_t_step.unsqueeze(0)), dim=0)\n",
    "            \n",
    "            p_gnn_template[indices[i]] = p_gnn_outputs.view(-1)\n",
    "        \n",
    "        return pgcn_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08eb1ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Extractor(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen, max_ach_num,\n",
    "                input_channels, hidden_channels_1, hidden_channels_2, out_channels, layer_num = 1):\n",
    "        super(Feature_Extractor, self).__init__()\n",
    "        self.pgcn_model = P_GCN(input_dimen, hidden_dimen, output_dimen, max_ach_num)\n",
    "        self.cnn_1D = CNN_1D(input_channels, hidden_channels_1, hidden_channels_2, out_channels)\n",
    "        \n",
    "        \n",
    "    def forward(self, p_gcn_input_sets, edge_index, subgraph_nodes, dist_max, dist_argmax): \n",
    "        pgcn_template = torch.zeros((p_gcn_input_sets.shape[0], p_gcn_input_sets.shape[1], 4))\n",
    "        indices = [torch.isin(p_gcn_input_sets[:, :, 0], graph_node).unsqueeze(-1).expand_as(pgcn_template) for graph_node in subgraph_nodes]\n",
    "       \n",
    "            \n",
    "        for i in range(len(indices)):\n",
    "            subgraph = p_gcn_input_sets[:,:,1:5][indices[i]].view(p_gcn_input_sets.shape[0], -1, 4)\n",
    "            print(subgraph.shape[1])\n",
    "            p_gcn_outputs = torch.empty((0,subgraph.shape[1],subgraph.shape[2]))\n",
    "            subgraph_node_num = subgraph_nodes[i].shape[0]\n",
    "            \n",
    "            \n",
    "            for t in range(subgraph.shape[0]):\n",
    "                p_gcn_t_step = self.pgcn_model(subgraph[t,:,:], edge_index[i], dist_max[i,:,:subgraph_node_num,:], dist_argmax[i,:,:subgraph_node_num,:])\n",
    "                p_gcn_outputs = torch.cat((p_gcn_outputs, p_gcn_t_step.unsqueeze(0)), dim=0)\n",
    "            \n",
    "            pgcn_template[indices[i]] = p_gcn_outputs.view(-1)\n",
    "            del p_gcn_outputs\n",
    "            \n",
    "        #shape changed as: node_num, 4, time_step\n",
    "        pgcn_template =pgcn_template.permute(1,2,0)\n",
    "        extractor_outputs = self.cnn_1D(pgcn_template).permute(1, 0, 2)\n",
    "        #cnn_output shape changed from node_num, t-step, 4 to t-step, node_num, 4 \n",
    "        \n",
    "        return extractor_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61761d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_pooling(extractor_outputs, k):\n",
    "    norms = torch.norm(extractor_outputs, p=2, dim= -1)\n",
    "    _, sorted_indices = torch.sort(norms, dim= -1, descending=True)\n",
    "    sorted_outputs = torch.gather(extractor_outputs, dim=1, index=sorted_indices.unsqueeze(-1).expand(-1, -1, extractor_outputs.size(-1)))\n",
    "    k_nodes_outputs = sorted_outputs[:, : k , :]\n",
    "    \n",
    "    return k_nodes_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79da8eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, k, layer_num = 1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.k = k\n",
    "        self.layer_num = layer_num\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features = input_dimen)\n",
    "        self.input_layer = nn.Linear(input_dimen, hidden_dimen)\n",
    "    \n",
    "        if layer_num == 1:\n",
    "            self.linear_layers = nn.Linear(hidden_dimen, 1)\n",
    "        else:\n",
    "            self.linear_layers = nn.ModuleList([nn.Linear(hidden_dimen, hidden_dimen), nn.ReLU()] * (layer_num - 1))\n",
    "            self.linear_layers.append(nn.Linear(hidden_dimen, 1))\n",
    "        \n",
    "        self.acti_func = nn.ReLU()\n",
    "        self.classifier = nn.Sigmoid() \n",
    "        \n",
    "    def forward(self, extractor_outputs):\n",
    "        k_nodes_outputs = sort_pooling(extractor_outputs, self.k)\n",
    "        norm_x = self.batch_norm(k_nodes_outputs)\n",
    "        x = self.input_layer(norm_x)\n",
    "        \n",
    "        if self.layer_num == 1:\n",
    "            x = self.linear_layers(self.acti_func(x))\n",
    "        else:\n",
    "            x = self.linear_layers(x)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e9466ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renumber_subgraph(nodes, edge_index):\n",
    "    unique_nodes = torch.unique(nodes, sorted = True)\n",
    "    new_node_ids = torch.arange(len(unique_nodes))\n",
    "    node_mapping = {old_id.item(): new_id.item() for old_id, new_id in zip(unique_nodes, new_node_ids)}\n",
    "    \n",
    "    new_edge_index = torch.tensor([\n",
    "        [node_mapping[edge_index[0, i].item()], node_mapping[edge_index[1, i].item()]]\n",
    "        for i in range(edge_index.size(1))\n",
    "    ]).t()\n",
    "    \n",
    "    return new_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f604037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomData(Data):\n",
    "    def __init__(self, trend, period, target_volume, target_label, edge_pairs, subgraph_node_num, subgraph_nodes,city_node_num, dist_max, dist_argmax):\n",
    "        super(CustomData, self).__init__()\n",
    "        self.trend = trend\n",
    "        self.period = period\n",
    "        self.target_volume = target_volume\n",
    "        self.target_label = target_label\n",
    "        self.edge_pairs = edge_pairs\n",
    "        self.subgraph_node_num = subgraph_node_num\n",
    "        self.subgraph_nodes = subgraph_nodes\n",
    "        self.city_node_num = city_node_num\n",
    "        self.dist_max = dist_max\n",
    "        self.dist_argmax = dist_argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6491431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dimen, hidden_dimen, output_dimen = 4, 8 ,4\n",
    "max_ach_num = 72\n",
    "input_channels, hidden_channels_1, hidden_channels_2, out_channels = 4, 8, 16, 8\n",
    "k = 60\n",
    "pred_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6aa6525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------PGNN----------------------------\n",
    "pgnn_model = T_Step_PGNN(input_dimen, hidden_dimen, output_dimen, max_ach_num)\n",
    "    \n",
    "#-------------------Feature Extractor----------------------   \n",
    "feature_extractor = Feature_Extractor(input_dimen, hidden_dimen, output_dimen, max_ach_num,\n",
    "                                      input_channels, hidden_channels_1, hidden_channels_2, out_channels)\n",
    "\n",
    "#-------------------Discriminator-----------------------------\n",
    "discriminator = Discriminator(input_dimen, hidden_dimen, k, layer_num = 2)\n",
    "\n",
    "#------------------Predictor-----------------------------------\n",
    "predictor = GRU(input_dimen, hidden_dimen, pred_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08fa5c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_discriminator = nn.BCELoss()\n",
    "criterion_regression = nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "optimizer_extractor = optim.Adam(feature_extractor.parameters(), lr=0.0008)\n",
    "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=0.0006)\n",
    "optimizer_pgnn = optim.Adam(pgnn_model.parameters(), lr=0.0008)\n",
    "optimizer_predictor = optim.Adam(predictor.parameters(), lr=0.0008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "418a21c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "purpose = \"test\"\n",
    "batch_size = 4\n",
    "root_path = \"D:/ThesisData/processed data/\"\n",
    "file_path = root_path + f'/SourceDomain/loaders/{purpose}_list.pt'\n",
    "data = torch.load(file_path)\n",
    "batch_num = len(data)//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8d23298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8578343a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c44336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted trend batch ID:\n",
      "111260\n",
      "tensor(0.)\n",
      "sorted period batch ID:\n",
      "111260\n",
      "tensor(0.)\n",
      "-----------------------\n",
      "sorted batch nodes\n",
      "91160\n",
      "tensor(13)\n",
      "3617\n",
      "4693\n",
      "3873\n",
      "3736\n",
      "3992\n",
      "4011\n",
      "4565\n",
      "4430\n",
      "4165\n",
      "4704\n",
      "4795\n",
      "4099\n",
      "4826\n",
      "4148\n",
      "3755\n",
      "3858\n",
      "3240\n",
      "4129\n",
      "3587\n",
      "4131\n",
      "4257\n",
      "4549\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 1\n",
    "\n",
    "for h in range(epoch_num):\n",
    "    for b in range(batch_num):\n",
    "        batch = data[b: b + batch_size]\n",
    "        city_nodes = sum([batch[i].city_node_num for i in range(len(batch))])\n",
    "        \n",
    "        one_batch_discriminator = []\n",
    "        one_batch_predictor = []\n",
    "        target_volumes = []\n",
    "        target_labels = []\n",
    "        \n",
    "        #Each Batch Covers batch_size data files\n",
    "        for i in range(len(batch)):\n",
    "    \n",
    "            #extractor outputs Shape: T-step, Node_num, 4\n",
    "\n",
    "            batch_id = batch[i].trend[0,:,0]\n",
    "            sorted_batch_id, _ = torch.sort(batch_id)\n",
    "            \n",
    "            batch_nodes = torch.cat(batch[i].subgraph_nodes, dim = 0)\n",
    "            sorted_batch_nodes, _ = torch.sort(batch_nodes)\n",
    "            \n",
    "            batch_id_ = batch[i].period[0,:,0]\n",
    "            sorted_batch_id_, _ = torch.sort(batch_id_)\n",
    "           \n",
    "            \n",
    "            print(\"sorted trend batch ID:\")\n",
    "            print(sorted_batch_id.shape[0])\n",
    "            print(torch.min(sorted_batch_id))\n",
    "            print(\"sorted period batch ID:\")\n",
    "            print(sorted_batch_id_.shape[0])\n",
    "            print(torch.min(sorted_batch_id_))\n",
    "            \n",
    "            print(\"-----------------------\")\n",
    "            print(\"sorted batch nodes\")\n",
    "            print(sorted_batch_nodes.shape[0])\n",
    "            print(torch.min(sorted_batch_nodes))\n",
    "            \n",
    "            \n",
    "            extractor_outputs = feature_extractor(batch[i].trend, batch[i].edge_pairs, batch[i].subgraph_nodes, batch[i].dist_max, batch[i].dist_argmax)    \n",
    "            \n",
    "            \n",
    "            pgnn_outputs = pgnn_model(batch[i].period, batch[i].subgraph_nodes, batch[i].dist_max, batch[i].dist_argmax)\n",
    "                   \n",
    "            discriminator_out = discriminator(extractor_outputs.detach())\n",
    "            \n",
    "                                      \n",
    "            predictor_output = predictor(pgnn_outputs.permute(1, 0, 2), extractor_template.permute(1, 0, 2))\n",
    "            predictor_output = predictor_output.permute(1, 0, 2)\n",
    "                                      \n",
    "            one_batch_predictor.append(predictor_output)\n",
    "            one_batch_discriminator.append(discriminator_out)\n",
    "            \n",
    "            target_volumes.append(batch[i].target_volume)            \n",
    "            target_labels.append(batch[i].target_label[:,:k])\n",
    "                                      \n",
    "        one_batch_predictor = torch.stack(one_batch_predictor)\n",
    "        one_batch_discriminator = torch.stack(one_batch_discriminator)\n",
    "        target_volumes = torch.stack(target_volumes)                  \n",
    "        target_labels = torch.stack(target_labels)\n",
    "                                      \n",
    "                                      \n",
    "        #--------Update PGNN Para-------------------\n",
    "        optimizer_pgnn.zero_grad()\n",
    "        loss_pgnn = criterion_regression(one_batch_predictor, target_volume) / city_nodes\n",
    "        loss_pgnn.backward()\n",
    "        optimizer_pgnn.step()\n",
    "\n",
    "        \n",
    "        #---------Update the Para of Discriminator-----------\n",
    "        optimizer_discriminator.zero_grad()\n",
    "        loss_discriminator = criterion_discriminator(one_batch_discriminator, target_labels)\n",
    "        loss_discriminator.backward()\n",
    "        optimizer_discriminator.step()\n",
    "\n",
    "\n",
    "        #--------Update Feature Extractor Para-------------\n",
    "        #--------Firstly, Use UPDATED DISCRIMINATOR TO CLASSIFIED DOMAIN DATA---------\n",
    "        discriminator_out = discriminator(extractor_outputs)\n",
    "        loss_discriminator = criterion_discriminator(discriminator_out, target_labels)\n",
    "        #--------Secondly, update the PARAs of FEATURE EXTRACTOR------------\n",
    "        optimizer_extractor.zero_grad()\n",
    "        loss_feat_ext = alpha * loss_pgnn + (1-alpha) * (-loss_discriminator) \n",
    "        loss_feat_ext.backward()\n",
    "        optimizer_extractor.step()\n",
    "\n",
    "\n",
    "        #---------Update GRU Paras AND NOT INFLUENCE PGNN--------\n",
    "        optimizer_predictor.zero_grad()\n",
    "        gru_k_step_outs = predictor(pgnn_outputs.permute(1, 0, 2).detach(), extractor_template.permute(1, 0, 2).detach()).permute(1, 0, 2)\n",
    "        loss_gru = criterion_regression(gru_k_step_outs, target_volumes)/ city_nodes\n",
    "        loss_gru.backward()\n",
    "        optimizer_predictor.step()\n",
    "\n",
    "        print(f\"After batch {b}, regression loss: {loss_gru}, disriminator loss: {loss_discriminator}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427c9848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d54c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
