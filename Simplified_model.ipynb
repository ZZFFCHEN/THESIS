{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3af0571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import pymetis\n",
    "import networkx as nx\n",
    "import time\n",
    "from networkx.algorithms import community\n",
    "from random import shuffle\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric as tg\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch.nn import init\n",
    "import pdb\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch_geometric.data import Data\n",
    "import torch.optim as optim\n",
    "import pywt\n",
    "from scipy.stats import norm\n",
    "import scipy.interpolate as interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b92ec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data(file_path):\n",
    "    file = open(file_path,\"rb\")\n",
    "    raw_data = pickle.load(file)  \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "079df26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearOperate(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen):\n",
    "        super(NonLinearOperate, self).__init__()  #类NonLinearLayer继承父类nn.Module的初始化方法\n",
    "        self.layer_1 = nn.Linear(input_dimen, hidden_dimen)\n",
    "        self.layer_2 = nn.Linear(hidden_dimen, output_dimen)\n",
    "        self.acti_func = nn.ReLU()\n",
    "        for m in self.modules():#遍历所有子模块\n",
    "#Check if each sub-module is an example of the class nn.Linear\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))#Use Xavier initialization\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = init.constant_(m.bias.data, 0.0)\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.acti_func(x)\n",
    "        x = self.layer_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b1e6315",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGNN_Layer(nn.Module):\n",
    "    def __init__(self, input_dimen, output_dimen, max_ach_num):\n",
    "        super(PGNN_Layer, self).__init__()\n",
    "        self.input_dimen = input_dimen\n",
    "        self.output_dimen = output_dimen\n",
    "        self.distance_calculate = NonLinearOperate(1, output_dimen, 1)\n",
    "        self.acti_func = nn.ReLU()\n",
    "        self.linear_hidden = nn.Linear(2*input_dimen, output_dimen)\n",
    "        self.out_transition = nn.Linear(output_dimen,1)\n",
    "        self.linear_out_position = nn.Linear(max_ach_num,input_dimen)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))#Use Xavier initialization\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = init.constant_(m.bias.data, 0.0)                \n",
    "                    \n",
    "    def forward(self, node_features, dists_max, dists_argmax):\n",
    "        dists_max = self.distance_calculate(dists_max.unsqueeze(-1)).squeeze()\n",
    "        subset_features = node_features[dists_argmax.flatten(), :]\n",
    "        subset_features = subset_features.reshape(dists_argmax.shape[0], dists_argmax.shape[1], subset_features.shape[1])\n",
    "        messages = subset_features * dists_max.unsqueeze(-1)\n",
    "        feature_self = node_features.unsqueeze(1).repeat(1, dists_max.shape[1],1)\n",
    "        messages = torch.concat((messages, feature_self), dim = -1) #N行M列D维\n",
    "        messages = self.linear_hidden(messages).squeeze()#将输出维度改为Output Dimen,即n*m*output_dimen\n",
    "        messages = self.acti_func(messages) \n",
    "        output_transition = self.out_transition(messages).squeeze(-1) #n * m * output_dimen to n * m\n",
    "        output_position = self.linear_out_position(output_transition)\n",
    "        output_structure = torch.mean(messages, dim=1)#n*output_dimen\n",
    "        \n",
    "        return output_position, output_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b16685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGNN(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen, max_ach_num, layer_num = 1, drop_out = True):\n",
    "        super(PGNN, self).__init__()\n",
    "        self.drop_out = drop_out\n",
    "        self.layer_num = layer_num\n",
    "        self.input_layer = nn.Linear(input_dimen, hidden_dimen)\n",
    "        self.last_layer = nn.Linear(hidden_dimen, input_dimen)\n",
    "        self.max_ach_num = max_ach_num\n",
    "        if self.layer_num == 1:\n",
    "            self.gnn_operate_1 = PGNN_Layer(hidden_dimen, output_dimen, max_ach_num)#输出维度是node_num * hidden_dimen\n",
    "            \n",
    "        if self.layer_num > 1:\n",
    "            self.gnn_hidden = nn.ModuleList([PGNN_Layer(hidden_dimen, hidden_dimen, max_ach_num) for i in range(0, layer_num)])\n",
    "            self.gnn_output_layer = PGNN_Layer(hidden_dimen, output_dimen)\n",
    "                \n",
    "        \n",
    "    def forward(self, x, dist_max_sets, dist_argmax_sets):\n",
    "        \n",
    "        x = self.input_layer(x)\n",
    "        if self.layer_num == 1:\n",
    "            x_position, x = self.gnn_operate_1(x, dist_max_sets[0,:,:], dist_argmax_sets[0,:,:])\n",
    "            if self.drop_out:\n",
    "                x = F.dropout(x, training=self.training)\n",
    "            x_position = self.last_layer(x_position)\n",
    "            #print(f\"pgnn output_layer size: {x_position.shape}\")\n",
    "            return x_position\n",
    "    \n",
    "        if self.layer_num > 1:\n",
    "            for i in range(self.layer_num):\n",
    "                _, x = self.gnn_hidden[i](x, dist_max_sets[i,:,:], dist_argmax_sets[i,:,:])\n",
    "                if self.drop_out:\n",
    "                    x = F.dropout(x, training=self.training)\n",
    "                  \n",
    "            _ = F.normalize(_, p=2, dim=-1)\n",
    "            x_position = self.last_layer(_)    \n",
    "            \n",
    "            \n",
    "            return x_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad389c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class P_GCN(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen, max_ach_num, layer_num = 1, drop_out = True):\n",
    "        super(P_GCN, self).__init__()\n",
    "        self.max_ach_num = max_ach_num\n",
    "        self.layer_num = layer_num\n",
    "        self.drop_out = drop_out\n",
    "        self.input_layer = nn.Linear(input_dimen, hidden_dimen)\n",
    "        self.p_gcn_block = nn.Sequential(PGNN_Layer(hidden_dimen, hidden_dimen, max_ach_num), \n",
    "                                         GCNConv(hidden_dimen, hidden_dimen, add_self_loops=True))\n",
    "        self.acti_func = nn.ReLU()   \n",
    "        \n",
    "        if layer_num == 1:\n",
    "            self.gcn_p_layers = self.p_gcn_block\n",
    "        if layer_num > 1:\n",
    "            self.gcn_p_layers = nn.ModuleList([self.p_gcn_block for i in range(num_layer)])\n",
    "            \n",
    "        self.output_layer = nn.Linear(hidden_dimen, output_dimen)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = init.constant_(m.bias.data, 0.0)   \n",
    "        \n",
    "        \n",
    "    def forward(self, x, edge_index, dist_max, dist_argmax):#GCN_P_input shape: node_num, 4\n",
    "        \n",
    "        x_ = self.input_layer(x)\n",
    "        \n",
    "        if self.layer_num == 1:\n",
    "            x_position, _ = self.gcn_p_layers[0](x_, dist_max[0,:,:], dist_argmax[0,:,:])\n",
    "            if self.drop_out:\n",
    "                x_position = F.dropout(x_position, training=self.training)\n",
    "            x = self.gcn_p_layers[1](x_position, edge_index)\n",
    "            if self.drop_out:\n",
    "                x = F.dropout(x, training=self.training)\n",
    "            x = self.acti_func(x + x_)\n",
    "            \n",
    "        else:\n",
    "            for i in range(self.layer_num):\n",
    "                x_position, _ = self.gcn_p_layers[i][0](x_, dist_max[i,:,:], dist_argmax[i,:,:])\n",
    "                if self.drop_out:\n",
    "                    x_position = F.dropout(x_position, training=self.training)\n",
    "                x = self.gcn_p_layers[i][1](x_position, edge_index)\n",
    "                if self.drop_out:\n",
    "                    x = F.dropout(x, training=self.training)\n",
    "                x = self.acti_func(x + x_)\n",
    "                x_ = x\n",
    "                    \n",
    "        x = self.acti_func(self.output_layer(x))\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8ea773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入数据shape: node_num, 4, time_step(20)\n",
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels_1, hidden_channels_2, out_channels):\n",
    "        super(CNN_1D, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = input_channels, out_channels = hidden_channels_1, kernel_size = 3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_channels_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels = hidden_channels_1, out_channels = hidden_channels_2, kernel_size = 3, padding=0),\n",
    "            nn.BatchNorm1d(hidden_channels_2),\n",
    "            nn.ReLU(), #len: 18\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),#(18-2)/2 +1 =9\n",
    "            nn.ConvTranspose1d(in_channels= hidden_channels_2,\n",
    "                               out_channels=out_channels,\n",
    "                               kernel_size=4,\n",
    "                               stride= 2, \n",
    "                               padding=0))\n",
    "\n",
    "        self.fc1 = nn.Linear(out_channels, 4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        #output shape: change from batch_num, out_channels, t-step to batch_num, t-step, out_channels\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = self.fc1(out)\n",
    "        #output shape: batch_num, t-step, 4\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b765e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, pred_len, output_dimen = 4, num_layers = 2):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_dimen = hidden_dimen\n",
    "        self.output_dimen = output_dimen\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_layer_1 = nn.Linear(input_dimen, hidden_dimen)\n",
    "        self.gru_layers = nn.GRU(hidden_dimen, hidden_dimen, num_layers, batch_first = True)\n",
    "        self.linear_layer_2 = nn.Linear(hidden_dimen, output_dimen)\n",
    "        self.k = pred_len #Futre k-step to predict \n",
    "        \n",
    "         \n",
    "    #gru输入格式：node_num, t-steps, hidden_dimen\n",
    "    def forward(self, pgnn_t_step_outs, extractor_outputs):  \n",
    "        x = torch.cat((pgnn_t_step_outs, extractor_outputs), dim = -1)\n",
    "        #x shape: node_num, t-steps, 2*4\n",
    "        batch_size, seq_len, feat_dim = x.size()\n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_dimen)\n",
    "        x = self.linear_layer_1(x)\n",
    "        outputs, _ = self.gru_layers(x, h_0)#outputs shape: batch_size, seq_len, hidden dim. \n",
    "        layer_2_input = outputs[:, -self.k:, :]\n",
    "        x = self.linear_layer_2(layer_2_input) #x shape: batch_size, k steps, 4.\n",
    "        #linear_layer_2 trasnfer last dimension from hidden dim to 4\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b593d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T_Step_PGNN(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen, max_ach_num, layer_num = 1):\n",
    "        super(T_Step_PGNN, self).__init__()\n",
    "        self.pgnn_model = PGNN(input_dimen, hidden_dimen, output_dimen, max_ach_num)\n",
    "        \n",
    "    def forward(self, t_step_inputs, subgraph_nodes, dist_max, dist_argmax): #t-step-inputs shape: 20, max_subgraph_nodenumber,4\n",
    "        pgnn_template = torch.zeros((t_step_inputs.shape[0], t_step_inputs.shape[1], t_step_inputs.shape[2]))\n",
    "        \n",
    "        pgnn_outputs = torch.empty((0,subgraph_nodes.shape[0],t_step_inputs.shape[2]))\n",
    "        subgraph_node_num = subgraph_nodes.shape[0]\n",
    "            \n",
    "        for t in range(t_step_inputs.shape[0]):\n",
    "            pgnn_t_step = self.pgnn_model(t_step_inputs[t,:subgraph_node_num,:], dist_max[:,:subgraph_node_num,:], dist_argmax[:,:subgraph_node_num,:])\n",
    "            pgnn_outputs = torch.cat((pgnn_outputs, pgnn_t_step.unsqueeze(0)), dim=0)\n",
    "        \n",
    "        pgnn_template[:,:subgraph_node_num,:] = pgnn_outputs\n",
    "        \n",
    "        return pgnn_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e5d2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "150cfefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Extractor(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen, max_ach_num,\n",
    "                input_channels, hidden_channels_1, hidden_channels_2, out_channels, layer_num = 1):\n",
    "        super(Feature_Extractor, self).__init__()\n",
    "        self.pgcn_model = P_GCN(input_dimen, hidden_dimen, output_dimen, max_ach_num)\n",
    "        self.cnn_1D = CNN_1D(input_channels, hidden_channels_1, hidden_channels_2, out_channels)\n",
    "        \n",
    "        \n",
    "    def forward(self, t_step_inputs, edge_index, subgraph_nodes, dist_max, dist_argmax): \n",
    "        pgcn_template = torch.zeros((t_step_inputs.shape[0], t_step_inputs.shape[1], t_step_inputs.shape[2]))\n",
    "       \n",
    "        pgcn_outputs = torch.empty((0,subgraph_nodes.shape[0],t_step_inputs.shape[2]))\n",
    "        subgraph_node_num = subgraph_nodes.shape[0]\n",
    "            \n",
    "            \n",
    "        for t in range(t_step_inputs.shape[0]):\n",
    "            pgcn_t_step = self.pgcn_model(t_step_inputs[t,:subgraph_node_num,:], edge_index, dist_max[:,:subgraph_node_num,:], dist_argmax[:,:subgraph_node_num,:])\n",
    "            pgcn_outputs = torch.cat((pgcn_outputs, pgcn_t_step.unsqueeze(0)), dim=0)\n",
    "            \n",
    "        pgcn_template[:,:subgraph_node_num,:] = pgcn_outputs\n",
    "            \n",
    "            \n",
    "        #shape changed as: node_num, 4, time_step\n",
    "        pgcn_template =pgcn_template.permute(1,2,0)\n",
    "        extractor_outputs = self.cnn_1D(pgcn_template).permute(1, 0, 2)\n",
    "        #cnn_output shape changed from node_num, t-step, 4 to t-step, node_num, 4 \n",
    "        \n",
    "        return extractor_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d353898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_pooling(extractor_outputs, k):\n",
    "    #print(extractor_outputs.shape)\n",
    "    norms = torch.norm(extractor_outputs, p=2, dim= -1)\n",
    "    #print(\"norm shape\", norms.shape)\n",
    "    _, sorted_indices = torch.sort(norms, dim= -1, descending=True)\n",
    "    sorted_outputs = torch.gather(extractor_outputs, dim=1, index=sorted_indices.unsqueeze(-1).expand(-1, -1, extractor_outputs.size(-1)))\n",
    "    k_nodes_outputs = sorted_outputs[:, : k , :]\n",
    "    #print(\"sort shape:\")\n",
    "    #print(k_nodes_outputs.shape)\n",
    "    \n",
    "    return k_nodes_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c856106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, k, layer_num = 1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.k = k\n",
    "        self.layer_num = layer_num\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features = input_dimen)\n",
    "        self.input_layer = nn.Linear(input_dimen, hidden_dimen)\n",
    "    \n",
    "        if layer_num == 1:\n",
    "            self.linear_layers = nn.Linear(hidden_dimen, 1)\n",
    "        elif layer_num > 1:\n",
    "            self.linear_layers = nn.ModuleList([nn.Linear(hidden_dimen, hidden_dimen), nn.ReLU()] * (layer_num - 1))\n",
    "            self.linear_layers.append(nn.Linear(hidden_dimen, 1))\n",
    "        \n",
    "        self.acti_func = nn.ReLU()\n",
    "        self.classifier = nn.Sigmoid() \n",
    "        \n",
    "    def forward(self, extractor_outputs):\n",
    "        k_nodes_outputs = sort_pooling(extractor_outputs, self.k)\n",
    "        flattened_k_nodes_outputs = torch.reshape(k_nodes_outputs, (-1, k_nodes_outputs.shape[2]))\n",
    "        norm_x = self.batch_norm(flattened_k_nodes_outputs)\n",
    "        x = self.input_layer(norm_x.reshape(k_nodes_outputs.shape[0],-1,k_nodes_outputs.shape[2]))\n",
    "        \n",
    "        if self.layer_num == 1:\n",
    "            x = self.linear_layers(self.acti_func(x))\n",
    "        else:\n",
    "            for layer in self.linear_layers:\n",
    "                x = layer(x)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c505851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomData(Data):\n",
    "    def __init__(self, trend, period, target_volume, target_label, edge_pairs, subgraph_node_num, subgraph_nodes,city_node_num, dist_max, dist_argmax):\n",
    "        super(CustomData, self).__init__()\n",
    "        self.trend = trend\n",
    "        self.period = period\n",
    "        self.target_volume = target_volume\n",
    "        self.target_label = target_label\n",
    "        self.edge_pairs = edge_pairs\n",
    "        self.subgraph_node_num = subgraph_node_num\n",
    "        self.subgraph_nodes = subgraph_nodes\n",
    "        self.city_node_num = city_node_num\n",
    "        self.dist_max = dist_max\n",
    "        self.dist_argmax = dist_argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fc7e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dimen, hidden_dimen, output_dimen = 4, 8 ,4\n",
    "max_ach_num = 72\n",
    "input_channels, hidden_channels_1, hidden_channels_2, out_channels = 4, 8, 16, 8\n",
    "gru_input_dimen = 8\n",
    "k = 60\n",
    "pred_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15d2fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------PGNN----------------------------\n",
    "pgnn_model = T_Step_PGNN(input_dimen, hidden_dimen, output_dimen, max_ach_num)\n",
    "\n",
    "\n",
    "    \n",
    "#-------------------Feature Extractor----------------------   \n",
    "feature_extractor = Feature_Extractor(input_dimen, hidden_dimen, output_dimen, max_ach_num,\n",
    "                                      input_channels, hidden_channels_1, hidden_channels_2, out_channels)\n",
    "\n",
    "#-------------------Discriminator-----------------------------\n",
    "discriminator = Discriminator(input_dimen, hidden_dimen, k, layer_num = 2)\n",
    "\n",
    "#------------------Predictor-----------------------------------\n",
    "predictor = GRU(gru_input_dimen, hidden_dimen, pred_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "528164f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_discriminator = nn.BCELoss()\n",
    "criterion_regression = nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "optimizer_extractor = optim.Adam(feature_extractor.parameters(), lr=0.0008)\n",
    "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=0.0006)\n",
    "optimizer_pgnn = optim.Adam(pgnn_model.parameters(), lr=0.0008)\n",
    "optimizer_predictor = optim.Adam(predictor.parameters(), lr=0.0008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e056fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "purpose = \"train\"\n",
    "root_path = \"D:/ThesisData/processed data/\"\n",
    "file_path = root_path + f'Barcelona/input_target/{purpose}_regional_level.pt'\n",
    "data = torch.load(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dde3fdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "924\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "batch_size = 4\n",
    "batch_num = len(data)//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca8f364b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 5873, 4])\n",
      "torch.Size([20, 5873, 4])\n",
      "torch.Size([20, 5873, 4])\n",
      "torch.Size([20, 5873, 4])\n",
      "torch.Size([20, 5873, 4])\n",
      "torch.Size([20, 5873, 4])\n",
      "torch.Size([20, 5873, 4])\n",
      "torch.Size([20, 5873, 4])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([4, 10, 60, 1])) that is different to the input size (torch.Size([4, 20, 60, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m#---------Update the Para of Discriminator-----------\u001b[39;00m\n\u001b[0;32m     49\u001b[0m optimizer_discriminator\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 50\u001b[0m loss_discriminator \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion_discriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone_batch_discriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m loss_discriminator\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     52\u001b[0m optimizer_discriminator\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\Cuda_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\Cuda_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\Cuda_env\\lib\\site-packages\\torch\\nn\\modules\\loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\Cuda_env\\lib\\site-packages\\torch\\nn\\functional.py:3118\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3116\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m-> 3118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3121\u001b[0m     )\n\u001b[0;32m   3123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3124\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([4, 10, 60, 1])) that is different to the input size (torch.Size([4, 20, 60, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "epoch_num = 1\n",
    "\n",
    "for h in range(epoch_num):\n",
    "    for b in range(batch_num):\n",
    "        batch = data[b: b + batch_size]\n",
    "        all_subgraph_nodes = sum([batch[i].subgraph_node_num for i in range(len(batch))])\n",
    "        \n",
    "        one_batch_discriminator = []\n",
    "        one_batch_extractor = []\n",
    "        one_batch_predictor = []\n",
    "        \n",
    "        target_volumes = []\n",
    "        target_labels = []\n",
    "        \n",
    "        #Each Batch Covers batch_size data files\n",
    "        for i in range(len(batch)):\n",
    "    \n",
    "            #extractor outputs Shape: T-step, Node_num, 4\n",
    "            extractor_outputs = feature_extractor(batch[i].trend, batch[i].edge_pairs, batch[i].subgraph_nodes, batch[i].dist_max, batch[i].dist_argmax)    \n",
    "            pgnn_period_outputs = pgnn_model(batch[i].period, batch[i].subgraph_nodes, batch[i].dist_max, batch[i].dist_argmax)\n",
    "                \n",
    "                \n",
    "            discriminator_out = discriminator(extractor_outputs.detach())\n",
    "            \n",
    "            print(extractor_outputs.shape)    \n",
    "            print(pgnn_period_outputs.shape)\n",
    "            predictor_output = predictor(extractor_outputs.permute(1, 0, 2), pgnn_period_outputs.permute(1, 0, 2))\n",
    "            predictor_output = predictor_output.permute(1, 0, 2)\n",
    "                                      \n",
    "            \n",
    "            one_batch_extractor.append(extractor_outputs)\n",
    "            one_batch_discriminator.append(discriminator_out)\n",
    "            one_batch_predictor.append(predictor_output)\n",
    "            \n",
    "            \n",
    "            target_volumes.append(batch[i].target_volume)            \n",
    "            target_labels.append(batch[i].target_label[:,:k,:])\n",
    "                     \n",
    "        one_batch_extractor = torch.stack(one_batch_extractor)  \n",
    "        one_batch_discriminator = torch.stack(one_batch_discriminator)\n",
    "        one_batch_predictor = torch.stack(one_batch_predictor)\n",
    "        \n",
    "       \n",
    "        target_volumes = torch.stack(target_volumes)                  \n",
    "        target_labels = torch.stack(target_labels)\n",
    "                                                                   \n",
    "        \n",
    "        #---------Update the Para of Discriminator-----------\n",
    "        optimizer_discriminator.zero_grad()\n",
    "        loss_discriminator = criterion_discriminator(one_batch_discriminator, target_labels)\n",
    "        loss_discriminator.backward()\n",
    "        optimizer_discriminator.step()\n",
    "        #------------------------------------------------------\n",
    "\n",
    "        \n",
    "        #--------Update Feature Extractor Para-------------\n",
    "        one_batch_discriminator = torch.empty_like(discriminator_out.unsuqeeze(0))\n",
    "        for i in range(one_batch_extractor.shape[0]):\n",
    "            discriminator_out = discriminator(one_batch_extractor[i,:,:,:])\n",
    "            one_batch_discriminator = torch.cat((one_batch_discriminator,discriminator_out.unsuqeeze(0)), dim = 0)\n",
    "        \n",
    "        loss_discriminator = criterion_discriminator(one_batch_discriminator, target_labels)\n",
    "        loss_predictor = criterion_regression(one_batch_predictor, target_volumes) / all_subgraph_nodes\n",
    "        \n",
    "        optimizer_extractor.zero_grad()\n",
    "        loss_feat_ext = alpha * loss_predictor + (1-alpha) * (-loss_discriminator) \n",
    "        loss_feat_ext.backward()\n",
    "        optimizer_extractor.step()\n",
    "\n",
    "        \n",
    "        #--------Update PGNN Para-------------------\n",
    "        optimizer_pgnn.zero_grad()\n",
    "        loss_predictor.backward()\n",
    "        optimizer_pgnn.step()        \n",
    "        \n",
    "\n",
    "        #---------Update GRU Paras AND NOT INFLUENCE PGNN--------\n",
    "        optimizer_predictor.zero_grad()\n",
    "        loss_predictor.backward()\n",
    "        optimizer_predictor.step()\n",
    "\n",
    "        print(f\"After batch {b}, regression loss: {loss_gru}, disriminator loss: {loss_discriminator}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439e0bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
