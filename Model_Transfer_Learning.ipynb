{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "e1d7b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import time\n",
    "from random import shuffle\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric as tg\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "import torch.autograd as autograd\n",
    "from torch.nn import init\n",
    "import pdb\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch_geometric.data import Data\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.ticker as mticker\n",
    "from torch.autograd import Function\n",
    "import pytorch_optimizer as p_optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "ced79a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data(file_path):\n",
    "    file = open(file_path,\"rb\")\n",
    "    raw_data = pickle.load(file)  \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "005261bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearOperate(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen):\n",
    "        super(NonLinearOperate, self).__init__()  #类NonLinearLayer继承父类nn.Module的初始化方法\n",
    "        self.layer_1 = nn.Linear(input_dimen, hidden_dimen)\n",
    "        self.layer_2 = nn.Linear(hidden_dimen, output_dimen)\n",
    "        self.acti_func = nn.ReLU()\n",
    "        #for m in self.modules():#遍历所有子模块\n",
    "        #Check if each sub-module is an example of the class nn.Linear\n",
    "            #if isinstance(m, nn.Linear):\n",
    "                #m.weight.data = init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "                #if m.bias is not None:\n",
    "                    #m.bias.data = init.constant_(m.bias.data, 0.0)\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.acti_func(x)\n",
    "        x = self.layer_2(x)\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "958888b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGNN_Layer(nn.Module):\n",
    "    def __init__(self, input_dimen, output_dimen, max_ach_num):\n",
    "        super(PGNN_Layer, self).__init__()\n",
    "        self.input_dimen = input_dimen\n",
    "        self.output_dimen = output_dimen\n",
    "        self.distance_calculate = NonLinearOperate(1, output_dimen, 1)\n",
    "        self.acti_func = nn.ReLU()\n",
    "        self.linear_hidden = nn.Linear(2*input_dimen, output_dimen)\n",
    "        self.out_transition = nn.Linear(output_dimen,1)\n",
    "        self.linear_out_position = nn.Linear(max_ach_num,input_dimen)\n",
    "        \n",
    "        #for m in self.modules():\n",
    "            #if isinstance(m, nn.Linear):\n",
    "                #m.weight.data = init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "                #if m.bias is not None:\n",
    "                    #m.bias.data = init.constant_(m.bias.data, 0.0)                \n",
    "                    \n",
    "    def forward(self, node_features, dists_max, dists_argmax):\n",
    "        dists_max = self.distance_calculate(dists_max.unsqueeze(-1)).squeeze()\n",
    "        subset_features = node_features[dists_argmax.flatten(), :]\n",
    "        subset_features = subset_features.reshape(dists_argmax.shape[0], dists_argmax.shape[1], subset_features.shape[1])\n",
    "        messages = subset_features * dists_max.unsqueeze(-1)\n",
    "        feature_self = node_features.unsqueeze(1).repeat(1, dists_max.shape[1],1)\n",
    "        messages = torch.concat((messages, feature_self), dim = -1) #N行M列D维\n",
    "        messages = self.linear_hidden(messages).squeeze()#将输出维度改为Output Dimen,即n*m*output_dimen\n",
    "        messages = self.acti_func(messages) \n",
    "        output_transition = self.out_transition(messages).squeeze(-1) #n * m * output_dimen to n * m\n",
    "        output_position = self.linear_out_position(output_transition)\n",
    "        output_structure = torch.mean(messages, dim=1)#n*output_dimen\n",
    "        \n",
    "        return output_position, output_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "770a8458",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGNN(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen, max_ach_num, layer_num = 1, drop_out = True):\n",
    "        super(PGNN, self).__init__()\n",
    "        self.drop_out = drop_out\n",
    "        self.layer_num = layer_num\n",
    "        self.input_layer = nn.Linear(input_dimen, hidden_dimen)\n",
    "        self.last_layer = nn.Linear(hidden_dimen, input_dimen)\n",
    "        self.max_ach_num = max_ach_num\n",
    "        if self.layer_num == 1:\n",
    "            self.gnn_operate_1 = PGNN_Layer(hidden_dimen, output_dimen, max_ach_num)#输出维度是node_num * hidden_dimen\n",
    "            \n",
    "        if self.layer_num > 1:\n",
    "            self.gnn_hidden = nn.ModuleList([PGNN_Layer(hidden_dimen, hidden_dimen, max_ach_num) for i in range(0, layer_num)])\n",
    "            self.gnn_output_layer = PGNN_Layer(hidden_dimen, output_dimen)\n",
    "                \n",
    "        \n",
    "    def forward(self, x, dist_max_sets, dist_argmax_sets):\n",
    "        \n",
    "        x = self.input_layer(x)\n",
    "        if self.layer_num == 1:\n",
    "            x_position, x = self.gnn_operate_1(x, dist_max_sets[0,:,:], dist_argmax_sets[0,:,:])\n",
    "            if self.drop_out:\n",
    "                x = F.dropout(x, training=self.training)\n",
    "            x_position = self.last_layer(x_position)\n",
    "            #print(f\"pgnn output_layer size: {x_position.shape}\")\n",
    "            return x_position\n",
    "    \n",
    "        if self.layer_num > 1:\n",
    "            for i in range(self.layer_num):\n",
    "                _, x = self.gnn_hidden[i](x, dist_max_sets[i,:,:], dist_argmax_sets[i,:,:])\n",
    "                if self.drop_out:\n",
    "                    x = F.dropout(x, training=self.training)\n",
    "                  \n",
    "            _ = F.normalize(_, p=2, dim=-1)\n",
    "            x_position = self.last_layer(_)    \n",
    "            \n",
    "            \n",
    "            return x_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "61361329",
   "metadata": {},
   "outputs": [],
   "source": [
    "class P_GCN(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen, max_ach_num, layer_num = 1, drop_out = True):\n",
    "        super(P_GCN, self).__init__()\n",
    "        self.max_ach_num = max_ach_num\n",
    "        self.layer_num = layer_num\n",
    "        self.drop_out = drop_out\n",
    "        self.input_layer = nn.Linear(input_dimen, hidden_dimen)\n",
    "        self.p_gcn_block = nn.Sequential(PGNN_Layer(hidden_dimen, hidden_dimen, max_ach_num), \n",
    "                                         GCNConv(hidden_dimen, hidden_dimen, add_self_loops=True))\n",
    "        self.acti_func = nn.ReLU()   \n",
    "        \n",
    "        if layer_num == 1:\n",
    "            self.gcn_p_layers = self.p_gcn_block\n",
    "        if layer_num > 1:\n",
    "            self.gcn_p_layers = nn.ModuleList([self.p_gcn_block for i in range(layer_num)])\n",
    "            \n",
    "        self.output_layer = nn.Linear(hidden_dimen, output_dimen)\n",
    "        \n",
    "        #for m in self.modules():\n",
    "            #if isinstance(m, nn.Linear):\n",
    "                #m.weight.data = init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "                #if m.bias is not None:\n",
    "                    #m.bias.data = init.constant_(m.bias.data, 0.0)   \n",
    "        \n",
    "        \n",
    "    def forward(self, x, edge_index, dist_max, dist_argmax):#GCN_P_input shape: node_num, 4\n",
    "        \n",
    "        x_ = self.input_layer(x)\n",
    "        \n",
    "        if self.layer_num == 1:\n",
    "            x_position, _ = self.gcn_p_layers[0](x_, dist_max[0,:,:], dist_argmax[0,:,:])\n",
    "            if self.drop_out:\n",
    "                x_position = F.dropout(x_position, training=self.training)\n",
    "            x = self.gcn_p_layers[1](x_position, edge_index)\n",
    "            if self.drop_out:\n",
    "                x = F.dropout(x, training=self.training)\n",
    "            x = self.acti_func(x + x_)\n",
    "            \n",
    "        else:\n",
    "            for i in range(self.layer_num):\n",
    "                x_position, _ = self.gcn_p_layers[i][0](x_, dist_max[i,:,:], dist_argmax[i,:,:])\n",
    "                if self.drop_out:\n",
    "                    x_position = F.dropout(x_position, training=self.training)\n",
    "                x = self.gcn_p_layers[i][1](x_position, edge_index)\n",
    "                if self.drop_out:\n",
    "                    x = F.dropout(x, training=self.training)\n",
    "                x = self.acti_func(x + x_)\n",
    "                x_ = x\n",
    "                    \n",
    "        x = self.acti_func(self.output_layer(x))\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "6bfc4a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入数据shape: node_num, 4, time_step(20)\n",
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels_1, hidden_channels_2, out_channels, output_dimen):\n",
    "        super(CNN_1D, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = input_channels, out_channels = hidden_channels_1, kernel_size = 3, padding=1),\n",
    "            nn.BatchNorm1d(hidden_channels_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels = hidden_channels_1, out_channels = hidden_channels_2, kernel_size = 3, padding=0),\n",
    "            nn.BatchNorm1d(hidden_channels_2),\n",
    "            nn.ReLU(), #len: 18\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),#(18-2)/2 +1 =9\n",
    "            nn.ConvTranspose1d(in_channels= hidden_channels_2,\n",
    "                               out_channels=out_channels,\n",
    "                               kernel_size=4,\n",
    "                               stride= 2, \n",
    "                               padding=0))\n",
    "\n",
    "        self.fc1 = nn.Linear(out_channels, output_dimen)\n",
    "        self.acti_func = nn.ReLU()  \n",
    "        \n",
    "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        #output shape: change from batch_num, out_channels, t-step to batch_num, t-step, out_channels\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = self.acti_func(self.fc1(out))\n",
    "        #output shape: batch_num, t-step, output_dimen\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "248cd318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, pred_len, output_dimen = 4, num_layers = 2):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_dimen = hidden_dimen\n",
    "        self.output_dimen = output_dimen\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_layer_1 = nn.Linear(input_dimen, hidden_dimen)\n",
    "        self.linear_layer_2 = nn.Linear(20, pred_len)\n",
    "        self.gru_layers = nn.GRU(hidden_dimen, output_dimen, num_layers, batch_first = True)\n",
    "        self.acti_func = nn.ReLU()\n",
    "         \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = init.constant_(m.bias.data, 0.0)               \n",
    "            \n",
    "            \n",
    "    #gru输入格式：node_num, t-steps, hidden_dimen\n",
    "    def forward(self, pgnn_t_step_outs, extractor_outputs):  \n",
    "        x = torch.cat((pgnn_t_step_outs, extractor_outputs), dim = -1)\n",
    "        #x shape: node_num, t-steps, 2 * 4\n",
    "        batch_size, seq_len, feat_dim = x.size()\n",
    "        x = self.acti_func(self.linear_layer_1(x))\n",
    "        x = self.acti_func(self.linear_layer_2(x.permute(0,2,1)))\n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.output_dimen)\n",
    "        outputs, _ = self.gru_layers(x.permute(0,2,1), h_0)#outputs shape: batch_size, pred_len, output_dimen. \n",
    "        x = self.acti_func(outputs)\n",
    "    \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "9121dede-32cc-483e-b371-59d73abf21b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T_Step_PGNN(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen, max_ach_num, layer_num = 1):\n",
    "        super(T_Step_PGNN, self).__init__()\n",
    "        self.pgnn_model = PGNN(input_dimen, hidden_dimen, output_dimen, max_ach_num, layer_num)\n",
    "        \n",
    "    def forward(self, t_step_inputs, subgraph_nodes, dist_max, dist_argmax): #t-step-inputs shape: 20, max_subgraph_nodenumber,4\n",
    "        pgnn_template = torch.zeros((t_step_inputs.shape[0], t_step_inputs.shape[1], t_step_inputs.shape[2]))\n",
    "        \n",
    "        pgnn_outputs = torch.empty((0,subgraph_nodes.shape[0],t_step_inputs.shape[2]))\n",
    "        subgraph_node_num = subgraph_nodes.shape[0]\n",
    "            \n",
    "        for t in range(t_step_inputs.shape[0]):\n",
    "            pgnn_t_step = self.pgnn_model(t_step_inputs[t,:subgraph_node_num,:], dist_max[:,:,:], dist_argmax[:,:,:])\n",
    "            pgnn_outputs = torch.cat((pgnn_outputs, pgnn_t_step.unsqueeze(0)), dim=0)\n",
    "        \n",
    "        pgnn_template[:,:subgraph_node_num,:] = pgnn_outputs\n",
    "        \n",
    "        return pgnn_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "e1b5730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T_Step_PGCN(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen, max_ach_num, layer_num = 1):\n",
    "        super(T_Step_PGCN, self).__init__()\n",
    "        self.pgcn_model = P_GCN(input_dimen, hidden_dimen, output_dimen, max_ach_num, layer_num)\n",
    "        self.output_dimen = output_dimen\n",
    "        \n",
    "    def forward(self, t_step_inputs, subgraph_nodes, edge_index, dist_max, dist_argmax): #t-step-inputs shape: 20, max_subgraph_nodenumber,4\n",
    "        pgcn_template = torch.zeros((t_step_inputs.shape[0], t_step_inputs.shape[1], self.output_dimen))\n",
    "        pgcn_outputs = torch.empty((0,subgraph_nodes.shape[0],self.output_dimen))\n",
    "        #pgcn_template = torch.zeros((t_step_inputs.shape[0], t_step_inputs.shape[1], t_step_inputs.shape[2]))\n",
    "        #pgcn_outputs = torch.empty((0,subgraph_nodes.shape[0],t_step_inputs.shape[2]))\n",
    "        subgraph_node_num = subgraph_nodes.shape[0]\n",
    "            \n",
    "        for t in range(t_step_inputs.shape[0]):\n",
    "            pgcn_t_step = self.pgcn_model(t_step_inputs[t,:subgraph_node_num,:], edge_index, dist_max[:,:,:], dist_argmax[:,:,:])\n",
    "            pgcn_outputs = torch.cat((pgcn_outputs, pgcn_t_step.unsqueeze(0)), dim=0)\n",
    "        \n",
    "        pgcn_template[:,:subgraph_node_num,:] = pgcn_outputs\n",
    "        \n",
    "        return pgcn_template     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "5f5886a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Extractor(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen, max_ach_num,\n",
    "                input_channels, hidden_channels_1, hidden_channels_2, out_channels, layer_num = 1):\n",
    "        super(Feature_Extractor, self).__init__()\n",
    "        self.pgcn_model = P_GCN(input_dimen, hidden_dimen, output_dimen, max_ach_num, layer_num = layer_num)\n",
    "        self.cnn_1D = CNN_1D(input_channels, hidden_channels_1, hidden_channels_2, out_channels, output_dimen)\n",
    "        self.output_dimen = output_dimen\n",
    "         \n",
    "    def forward(self, t_step_inputs, edge_index, subgraph_nodes, dist_max, dist_argmax): \n",
    "        pgcn_template = torch.zeros((t_step_inputs.shape[0], t_step_inputs.shape[1], self.output_dimen))\n",
    "        pgcn_outputs = torch.empty((0, subgraph_nodes.shape[0], self.output_dimen))\n",
    "       \n",
    "        \n",
    "        subgraph_node_num = subgraph_nodes.shape[0]\n",
    "            \n",
    "            \n",
    "        for t in range(t_step_inputs.shape[0]):\n",
    "            pgcn_t_step = self.pgcn_model(t_step_inputs[t,:subgraph_node_num,:], edge_index, dist_max[:,:,:], dist_argmax[:,:,:])\n",
    "            pgcn_outputs = torch.cat((pgcn_outputs, pgcn_t_step.unsqueeze(0)), dim=0)\n",
    "            \n",
    "        pgcn_template[:,:subgraph_node_num,:] = pgcn_outputs\n",
    "            \n",
    "        #shape changed as: node_num, 4, time_step\n",
    "        pgcn_template =pgcn_template.permute(1,2,0)\n",
    "        extractor_outputs = self.cnn_1D(pgcn_template).permute(1, 0, 2)\n",
    "        #cnn_output shape changed from node_num, t-step, 4 to t-step, node_num, 4 \n",
    "    \n",
    "        return extractor_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "48624601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_pooling(extractor_outputs, k):\n",
    "    norms = torch.norm(extractor_outputs, p=2, dim= -1)\n",
    "    #print(\"norm shape\", norms.shape)\n",
    "    _, sorted_indices = torch.sort(norms, dim= -1, descending=True)\n",
    "    sorted_outputs = torch.gather(extractor_outputs, dim=1, index=sorted_indices.unsqueeze(-1).expand(-1, -1, extractor_outputs.size(-1)))\n",
    "    k_nodes_outputs = sorted_outputs[:, : k , :]\n",
    "    \n",
    "    return k_nodes_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "65e3e4af-ee3f-4e85-bc00-be1f7ad8e2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalLayer(Function):\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "    \n",
    "    \n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha  # 反转梯度\n",
    "        return output, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "56690bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Domain_Classifier(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, k, seq_len=20, drop_out = True):\n",
    "        super(Domain_Classifier, self).__init__()\n",
    "        self.k = k\n",
    "        self.drop_out = drop_out\n",
    "        self.acti_func = nn.Sigmoid()\n",
    "        self.input_dimen = input_dimen\n",
    "\n",
    "        \n",
    "        self.input_layer = nn.Sequential(nn.Linear(input_dimen, hidden_dimen), nn.BatchNorm1d(num_features = hidden_dimen), nn.Tanh())\n",
    "        #self.linear_layer_1 = nn.Sequential(nn.Linear(hidden_dimen, hidden_dimen), nn.BatchNorm1d(num_features = hidden_dimen), nn.Tanh())    \n",
    "        self.linear_layer_2 = nn.Sequential(nn.Linear(hidden_dimen,1), nn.Tanh())  \n",
    "        self.output_layer = nn.Sequential(nn.Linear(k*seq_len, hidden_dimen), nn.Tanh(), nn.Linear(hidden_dimen, 1))\n",
    "\n",
    "        \n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                module.weight.data = init.xavier_uniform_(module.weight.data)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data = init.constant_(module.bias.data, 0.0)                    \n",
    "        \n",
    "        \n",
    "    def forward(self, extractor_outputs):\n",
    "        k_nodes_outputs = sort_pooling(extractor_outputs, self.k) #Shape Changed as: 20, K, input_dimen\n",
    "        k_nodes_outputs = k_nodes_outputs.reshape(-1, self.input_dimen)\n",
    "        #k_nodes_outputs #Shape: 20*K, input_dimen\n",
    "        x = self.input_layer(k_nodes_outputs)\n",
    "        if self.drop_out:\n",
    "            x = F.dropout(x, training=self.training, p = 0.3)\n",
    "        x = self.linear_layer_2(x) #Shape from 20*K, hidden_dimen TO 20*K, 1\n",
    "        if self.drop_out:\n",
    "            x = F.dropout(x, training=self.training, p = 0.3)\n",
    "        x = self.output_layer(x.squeeze(-1))#Shape: 20*K,\n",
    "        if self.drop_out:\n",
    "            x = F.dropout(x, training=self.training, p = 0.3)\n",
    "        x = self.acti_func(x)\n",
    "        #Shape: 1,\n",
    "     \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "ad999bfd-b8f2-4070-a22c-3e55aef8f686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_data(root_path, purpose):\n",
    "    tar_path = root_path + f'Barcelona/input_target/{purpose}_regional_level.pt'\n",
    "    sour_path = root_path + f'SourceDomain/regional_loaders/fake_{purpose}.pt'\n",
    "    source_data = torch.load(sour_path)\n",
    "    target_data = torch.load(tar_path)\n",
    "\n",
    "    return source_data, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "d6aeeb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomData(Data):\n",
    "    def __init__(self, trend, period, target_volume, target_label, edge_pairs, subgraph_node_num, subgraph_nodes, city_node_num, \n",
    "                 dist_max, dist_argmax, min_vals, max_vals):\n",
    "        super(CustomData, self).__init__()\n",
    "        self.trend = trend\n",
    "        self.period = period\n",
    "        self.target_volume = target_volume\n",
    "        self.target_label = target_label\n",
    "        self.edge_pairs = edge_pairs\n",
    "        self.subgraph_node_num = subgraph_node_num\n",
    "        self.subgraph_nodes = subgraph_nodes\n",
    "        self.city_node_num = city_node_num\n",
    "        self.dist_max = dist_max\n",
    "        self.dist_argmax = dist_argmax\n",
    "        self.min_vals = min_vals\n",
    "        self.max_vals = max_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "ddad7009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PGCN:\n",
    "input_dimen, hidden_dimen, output_dimen = 4, 16 ,8\n",
    "max_ach_num = 50\n",
    "\n",
    "#1-d Conv\n",
    "input_channels, hidden_channels_1, hidden_channels_2, out_channels = 8, 32, 16, 8\n",
    "\n",
    "#GRU\n",
    "gru_input_dimen = 16\n",
    "\n",
    "#classifier\n",
    "classifier_hidden_dimen = 64\n",
    "\n",
    "k = 60\n",
    "pred_len = 10\n",
    "seq_len =20\n",
    "beta = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "a92d7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------PGNN----------------------------\n",
    "pgcn_model = T_Step_PGCN(input_dimen, hidden_dimen, output_dimen, max_ach_num, layer_num = 2)                      \n",
    "for para in pgcn_model.parameters():\n",
    "    para.requires_grad = False\n",
    "\n",
    "    \n",
    "#-------------------Feature Extractor----------------------   \n",
    "feature_extractor = Feature_Extractor(input_dimen, hidden_dimen, output_dimen, max_ach_num,\n",
    "                                      input_channels, hidden_channels_1, hidden_channels_2, out_channels, layer_num = 1)\n",
    "\n",
    "#-------------------Discriminator-----------------------------\n",
    "classifier = Domain_Classifier(input_dimen = output_dimen, hidden_dimen = classifier_hidden_dimen, k=k, seq_len = seq_len)\n",
    "\n",
    "#------------------Predictor-----------------------------------\n",
    "predictor = GRU(gru_input_dimen, hidden_dimen, pred_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "f0bb88f1-c98e-451b-88a7-ed398cc75a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载模型和优化器参数\n",
    "checkpoint = torch.load('D:/ThesisData/processed data/ModelPara/model_para.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "500bbb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss = nn.BCELoss()\n",
    "criterion_regression_MSE = nn.MSELoss(reduction='sum')\n",
    "optimizer_extractor = optim.Adam(list(feature_extractor.parameters())+list(pgcn_model.parameters())+list(predictor.parameters()), lr=0.0006, weight_decay=0.00001)\n",
    "optimizer_extractor.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "optimizer_classifier = p_optim.RAdam(classifier.parameters(), lr=4e-4, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer_classifier, step_size = 5, gamma = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "5c7eba79-baec-47a6-9229-7fb5e21aa961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading model Parameters\n",
    "pgcn_model.load_state_dict(checkpoint['pgcn_state_dict'])\n",
    "feature_extractor.load_state_dict(checkpoint['feature_extractor_state_dict'])\n",
    "predictor.load_state_dict(checkpoint['predictor_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "a525d9e2-8160-48be-8374-de398ef7c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"D:/ThesisData/processed data/\"\n",
    "purposes = [\"train\", \"vali\",\"test\"]\n",
    "source_train, target_train = loading_data(root_path, purposes[0])\n",
    "source_vali, target_vali = loading_data(root_path, purposes[1])\n",
    "source_test, target_test = loading_data(root_path, purposes[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "5acde23f-8db9-46b2-bb0d-56bd9d874e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source train len: 336----target train len: 168\n",
      "Training set batch number  36\n",
      "Validation set batch number:  4\n",
      "Test set batch number:  5\n"
     ]
    }
   ],
   "source": [
    "print(f\"source train len: {len(source_train)}----target train len: {len(target_train)}\")\n",
    "train_data = source_train + target_train\n",
    "shuffle(train_data)\n",
    "batch_size = 14\n",
    "batch_num = len(train_data)//batch_size\n",
    "vali_batch_num = len(target_vali)//batch_size\n",
    "test_batch_num = len(target_test) // batch_size\n",
    "print(\"Training set batch number \", batch_num)\n",
    "print(\"Validation set batch number: \", vali_batch_num)\n",
    "print(\"Test set batch number: \", test_batch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "556fbdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 0, regression loss: 0.0617988146841526; domain classifier loss: 0.8797403573989868\n",
      "After batch 1, regression loss: 0.061779431998729706; domain classifier loss: 0.7679815292358398\n",
      "After batch 2, regression loss: 0.05472857877612114; domain classifier loss: 0.7762728929519653\n",
      "After batch 3, regression loss: 0.04675084725022316; domain classifier loss: 0.9242613911628723\n",
      "After batch 4, regression loss: 0.02646695263683796; domain classifier loss: 0.4969669282436371\n",
      "After batch 5, regression loss: 0.025378329679369926; domain classifier loss: 0.6383566856384277\n",
      "After batch 6, regression loss: 0.03332853689789772; domain classifier loss: 0.5507872104644775\n",
      "After batch 7, regression loss: 0.02643711306154728; domain classifier loss: 0.5581260323524475\n",
      "After batch 8, regression loss: 0.042385831475257874; domain classifier loss: 0.7486862540245056\n",
      "After batch 9, regression loss: 0.05795174464583397; domain classifier loss: 0.9554055333137512\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[648], line 144\u001b[0m\n\u001b[0;32m    142\u001b[0m optimizer_extractor\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    143\u001b[0m loss_feat_ext \u001b[38;5;241m=\u001b[39m loss_mse \u001b[38;5;241m+\u001b[39m beta \u001b[38;5;241m*\u001b[39m loss_classifier\n\u001b[1;32m--> 144\u001b[0m \u001b[43mloss_feat_ext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m optimizer_extractor\u001b[38;5;241m.\u001b[39mstep()      \n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m one_batch_predictor\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\Cuda_env\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\Cuda_env\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\Cuda_env\\lib\\site-packages\\torch\\autograd\\function.py:277\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m         backward_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mbackward  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    281\u001b[0m         vjp_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mvjp  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_num = 40\n",
    "\n",
    "\n",
    "plt.ion() \n",
    "#---------collect train set value-------------\n",
    "train_mse_losses = []\n",
    "train_mae_losses = []\n",
    "train_class_losses = []\n",
    "#--------collect vali set value---------------\n",
    "vali_mse_losses = []\n",
    "vali_mae_losses = []\n",
    "#collect test set value--------------------\n",
    "test_mse_losses = []\n",
    "test_mae_losses = []\n",
    "\n",
    "#---------------------------------------------\n",
    "epoch_numbers = []\n",
    "\n",
    "for h in range(epoch_num):\n",
    "    \n",
    "    train_mse_loss =0.0\n",
    "    train_mae_loss = 0.0\n",
    "    train_class_loss = 0.0\n",
    "\n",
    "    feature_extractor.train()\n",
    "    pgcn_model.train()\n",
    "    predictor.train()   \n",
    "    classifier.train()\n",
    "    \n",
    "    for b in range(batch_num):\n",
    "        mini_batch = train_data[b: b + batch_size]\n",
    "        all_subgraph_nodes = sum([mini_batch[i].subgraph_node_num for i in range(len(mini_batch))])\n",
    "\n",
    "        \n",
    "        #-----------------------------------------\n",
    "        one_batch_extractor = []\n",
    "        one_batch_predictor = []\n",
    "        one_batch_classifier = []\n",
    "        #-----------------------------------------\n",
    "     \n",
    "    \n",
    "        #-----------------------------------------\n",
    "        target_volumes = []\n",
    "        target_labels = []\n",
    "\n",
    "        \n",
    "        #Each Batch Covers batch_size data files\n",
    "        #for i in range(batch_size):\n",
    "            #extractor outputs Shape: T-step, Node_num, 8\n",
    "            #------------target city--------------------\n",
    "        i = 0\n",
    "        while i < int(batch_size * 2/3):\n",
    "            extractor_outputs = feature_extractor(mini_batch[i].trend, mini_batch[i].edge_pairs, mini_batch[i].subgraph_nodes, mini_batch[i].dist_max, mini_batch[i].dist_argmax).detach()\n",
    "            classifier_out = classifier(extractor_outputs)\n",
    "           \n",
    "                \n",
    "            #-------------------------------------------------\n",
    "            target_labels.append(mini_batch[i].target_label)\n",
    "            one_batch_classifier.append(classifier_out)\n",
    "            #-------------------------------------------------    \n",
    "            i += 1\n",
    "       \n",
    "        \n",
    "        #----------------------------------------------------------------\n",
    "        one_batch_classifier = torch.stack(one_batch_classifier)\n",
    "        target_labels = torch.tensor([ 0.9 if label == 1.0 else 0.1 for label in target_labels]).unsqueeze(-1)\n",
    "        #target_labels = torch.tensor(target_labels).unsqueeze(-1)\n",
    "        \n",
    "        #---------------------------------------------------------------- \n",
    "        \n",
    "        \n",
    "        #---------Update the Para of Domain Classifier-----------\n",
    "        optimizer_classifier.zero_grad()\n",
    "        loss_classifier = bce_loss(one_batch_classifier, target_labels)\n",
    "        loss_classifier.backward()\n",
    "        #Gradien Clip\n",
    "        torch.nn.utils.clip_grad_value_(classifier.parameters(), clip_value=0.5)\n",
    "        optimizer_classifier.step()\n",
    "        #------------------------------------------------------\n",
    "        del one_batch_classifier\n",
    "        del target_labels\n",
    "\n",
    "        \n",
    "        one_batch_classifier= []\n",
    "        target_labels = []\n",
    "\n",
    "        \n",
    "        while i >= int(batch_size * 2/3) and i < batch_size:\n",
    "            #--------Update Feature Extractor Para-------------\n",
    "            #Classifier_out Shape: out_dimen\n",
    "   \n",
    "            #for i in range(batch_size):\n",
    "            #extractor outputs Shape: T-step, Node_num, 8\n",
    "            #------------target city--------------------\n",
    "            extractor_outputs = feature_extractor(mini_batch[i].trend, mini_batch[i].edge_pairs, mini_batch[i].subgraph_nodes, mini_batch[i].dist_max, mini_batch[i].dist_argmax)    \n",
    "            pgcn_period_outputs = pgcn_model(mini_batch[i].period, mini_batch[i].subgraph_nodes, mini_batch[i].edge_pairs, mini_batch[i].dist_max, mini_batch[i].dist_argmax)\n",
    "            predictor_output = predictor(extractor_outputs.permute(1, 0, 2), pgcn_period_outputs.permute(1, 0, 2))\n",
    "            predictor_output = predictor_output.permute(1, 0, 2)\n",
    "            alpha = (b + 1) / epoch_num\n",
    "            #alpha = 0.01\n",
    "            classifier_input = GradientReversalLayer.apply(extractor_outputs, alpha)\n",
    "            classifier_out = classifier(classifier_input)\n",
    "            #-----------------------------------------------\n",
    "\n",
    "            \n",
    "            #-------------------------------------------------\n",
    "            one_batch_extractor.append(extractor_outputs)\n",
    "            one_batch_classifier.append(classifier_out)\n",
    "            one_batch_predictor.append(predictor_output)\n",
    "            #-------------------------------------------------    \n",
    "            \n",
    "            #--------------------------------------------------\n",
    "            target_volumes.append(mini_batch[i].target_volume) \n",
    "            target_labels.append(mini_batch[i].target_label)\n",
    "            i += 1\n",
    "\n",
    "        \n",
    "        #--------------------------------------------------\n",
    "        target_volumes = torch.stack(target_volumes)      \n",
    "        target_labels = torch.tensor([ 0.9 if label == 1.0 else 0.1 for label in target_labels]).unsqueeze(-1)\n",
    "        #target_labels = torch.tensor(target_labels).unsqueeze(-1)\n",
    "    \n",
    "\n",
    "        #----------------------------------------------------------------\n",
    "        one_batch_extractor = torch.stack(one_batch_extractor)  \n",
    "        one_batch_classifier = torch.stack(one_batch_classifier)\n",
    "        one_batch_predictor = torch.stack(one_batch_predictor)\n",
    "        #----------------------------------------------------------------\n",
    "\n",
    "        \n",
    "        loss_classifier = bce_loss(one_batch_classifier, target_labels)\n",
    "        loss_mse = (criterion_regression_MSE(one_batch_predictor, target_volumes)) / all_subgraph_nodes\n",
    "        loss_mae = torch.sum(torch.abs(one_batch_predictor - target_volumes)) / all_subgraph_nodes\n",
    "        #----------------------------------------------\n",
    "        train_mse_loss += loss_mse.item()\n",
    "        train_mae_loss += loss_mae.item()\n",
    "        train_class_loss += loss_classifier.item()\n",
    "        #-----------------------------------------------\n",
    "    \n",
    "        print(f\"After batch {b}, regression loss: {loss_mse}; domain classifier loss: {loss_classifier}\")      \n",
    "        \n",
    "        \n",
    "        optimizer_extractor.zero_grad()\n",
    "        loss_feat_ext = loss_mse + beta * loss_classifier\n",
    "        loss_feat_ext.backward(retain_graph=True)\n",
    "        optimizer_extractor.step()      \n",
    "\n",
    "\n",
    "        del one_batch_predictor\n",
    "        del target_volumes\n",
    "        del one_batch_classifier\n",
    "   \n",
    "        \n",
    "    scheduler.step()\n",
    "    \n",
    "    train_mse_loss /= batch_num\n",
    "    train_mae_loss /= batch_num   \n",
    "    train_class_loss /= batch_num   \n",
    "\n",
    "\n",
    "    train_mse_losses.append(train_mse_loss)\n",
    "    train_mae_losses.append(train_mae_loss)\n",
    "    train_class_losses.append(train_class_loss)\n",
    "        \n",
    "    \n",
    "\n",
    "#---------------------------------------------------------\n",
    "#-----------验证集---------------------------------------\n",
    "    feature_extractor.eval()\n",
    "    pgcn_model.eval()\n",
    "    classifier.eval()\n",
    "    predictor.eval()\n",
    "\n",
    "    vali_mse_loss = 0.0\n",
    "    vali_mae_loss = 0.0    \n",
    "\n",
    "    test_mse_loss = 0.0\n",
    "    test_mae_loss = 0.0  \n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for b in range(vali_batch_num):\n",
    "            target_batch = target_vali[b: b + batch_size]\n",
    "            all_subgraph_nodes = sum([target_batch[i].subgraph_node_num for i in range(len(target_batch))]) \n",
    "            #-----------------------------------------\n",
    "            one_batch_predictor = []\n",
    "            #-----------------------------------------\n",
    "\n",
    "            \n",
    "            #-----------------------------------------\n",
    "            target_volumes = []\n",
    "       \n",
    "        \n",
    "            for i in range(batch_size):\n",
    "                #extractor outputs Shape: T-step, Node_num, 8\n",
    "                #------------target city--------------------\n",
    "                extractor_outputs = feature_extractor(target_batch[i].trend, target_batch[i].edge_pairs, target_batch[i].subgraph_nodes, target_batch[i].dist_max, target_batch[i].dist_argmax)    \n",
    "                pgcn_period_outputs = pgcn_model(target_batch[i].period, target_batch[i].subgraph_nodes, target_batch[i].edge_pairs, target_batch[i].dist_max, target_batch[i].dist_argmax)\n",
    "                predictor_output = predictor(extractor_outputs.permute(1, 0, 2), pgcn_period_outputs.permute(1, 0, 2))\n",
    "                predictor_output = predictor_output.permute(1, 0, 2)\n",
    "                \n",
    "                #将结果反归一化-------------------------------------\n",
    "                min_vals = torch.round(target_batch[i].min_vals.unsqueeze(0).unsqueeze(0))\n",
    "                max_vals = torch.round(target_batch[i].max_vals.unsqueeze(0).unsqueeze(0))  \n",
    "                predictor_output[:,:target_batch[i].subgraph_node_num,:] = torch.round(predictor_output[:,:target_batch[i].subgraph_node_num,:]*(max_vals- min_vals) + min_vals)     \n",
    "                #-------------------------------------------------\n",
    "                one_batch_predictor.append(predictor_output)\n",
    "\n",
    "                \n",
    "                #--------------------------------------------------\n",
    "                target_volume = torch.zeros_like(target_batch[i].target_volume)  \n",
    "                target_volume[:,:target_batch[i].subgraph_node_num,:] = torch.round(target_batch[i].target_volume[:,:target_batch[i].subgraph_node_num,:]*(max_vals- min_vals) + min_vals) \n",
    "                target_volumes.append(target_volume)                \n",
    "                    \n",
    "            #---------------------------------------------------------------- \n",
    "            one_batch_predictor = torch.stack(one_batch_predictor)\n",
    "            #----------------------------------------------------------------\n",
    "            target_volumes = torch.stack(target_volumes)                  \n",
    "          \n",
    "         \n",
    "            loss_mse = criterion_regression_MSE(one_batch_predictor, target_volumes) / all_subgraph_nodes\n",
    "            loss_mae =torch.sum(torch.abs(one_batch_predictor - target_volumes)) / all_subgraph_nodes\n",
    "            vali_mse_loss +=  loss_mse.item()\n",
    "            vali_mae_loss += loss_mae.item()\n",
    "            print(f\"After batch {b}, Vali MSE loss: {loss_mse}, Vali MAE loss: {loss_mae}\")  \n",
    "\n",
    "\n",
    "        vali_mse_loss /= vali_batch_num\n",
    "        vali_mae_loss /= vali_batch_num\n",
    "\n",
    "    \n",
    "        vali_mse_losses.append(vali_mse_loss)\n",
    "        vali_mae_losses.append(vali_mae_loss)\n",
    "\n",
    "\n",
    "#---------------------Test set--------------------------------------------------\n",
    "        \n",
    "        for b in range(test_batch_num):\n",
    "            target_batch = target_test[b: b + batch_size]\n",
    "            all_subgraph_nodes = sum([target_batch[i].subgraph_node_num for i in range(len(target_batch))]) \n",
    "            #-----------------------------------------\n",
    "            one_batch_predictor = []\n",
    "            #-----------------------------------------\n",
    "         \n",
    "            #-----------------------------------------\n",
    "            target_volumes = []      \n",
    "            for i in range(batch_size):\n",
    "                extractor_outputs = feature_extractor(target_batch[i].trend, target_batch[i].edge_pairs, target_batch[i].subgraph_nodes, target_batch[i].dist_max, target_batch[i].dist_argmax)    \n",
    "                pgcn_period_outputs = pgcn_model(target_batch[i].period, target_batch[i].subgraph_nodes, target_batch[i].edge_pairs, target_batch[i].dist_max, target_batch[i].dist_argmax)\n",
    "                predictor_output = predictor(extractor_outputs.permute(1, 0, 2), pgcn_period_outputs.permute(1, 0, 2))\n",
    "                predictor_output = predictor_output.permute(1, 0, 2)\n",
    "                \n",
    "                #将结果反归一化-------------------------------------\n",
    "                min_vals = torch.round(target_batch[i].min_vals.unsqueeze(0).unsqueeze(0))\n",
    "                max_vals = torch.round(target_batch[i].max_vals.unsqueeze(0).unsqueeze(0))  \n",
    "                predictor_output[:,:target_batch[i].subgraph_node_num,:] = torch.round(predictor_output[:,:target_batch[i].subgraph_node_num,:]*(max_vals- min_vals) + min_vals)     \n",
    "                #-------------------------------------------------\n",
    "                one_batch_predictor.append(predictor_output)\n",
    "\n",
    "                \n",
    "                #--------------------------------------------------\n",
    "                target_volume = torch.zeros_like(target_batch[i].target_volume)  \n",
    "                target_volume[:,:target_batch[i].subgraph_node_num,:] = torch.round(target_batch[i].target_volume[:,:target_batch[i].subgraph_node_num,:]*(max_vals- min_vals) + min_vals) \n",
    "                target_volumes.append(target_volume)                \n",
    "                    \n",
    "            #---------------------------------------------------------------- \n",
    "            one_batch_predictor = torch.stack(one_batch_predictor)\n",
    "            #----------------------------------------------------------------\n",
    "            target_volumes = torch.stack(target_volumes)                  \n",
    "                          \n",
    "            loss_mse = criterion_regression_MSE(one_batch_predictor, target_volumes) / all_subgraph_nodes\n",
    "            loss_mae =torch.sum(torch.abs(one_batch_predictor - target_volumes)) / all_subgraph_nodes\n",
    "            test_mse_loss +=  loss_mse.item()\n",
    "            test_mae_loss += loss_mae.item()\n",
    "            print(f\"After batch {b}, Test MSE loss: {loss_mse}, Test MAE loss: {loss_mae}\")  \n",
    "    \n",
    "    \n",
    "        test_mse_loss /= test_batch_num\n",
    "        test_mae_loss /= test_batch_num\n",
    "\n",
    "    \n",
    "        test_mse_losses.append(test_mse_loss)\n",
    "        test_mae_losses.append(test_mae_loss)            \n",
    "\n",
    "    \n",
    "    #------------------------------------------------------------------\n",
    "    epoch_numbers.append(h+1)\n",
    "    \n",
    "    clear_output(wait=True)  #清除上一次的输出\n",
    "    fig, axs = plt.subplots(1,2, figsize=(16,6))\n",
    "    plt.subplots_adjust(wspace=0.35)\n",
    "    line1, = axs[0].plot(epoch_numbers, train_mse_losses, label = \"MSE\", color = 'g') \n",
    "    axs[0].tick_params(axis='both',labelsize=8)\n",
    "    axs[0].set_ylim(0,max(train_mse_losses)+0.1)\n",
    "    axs[0].set_xlabel('Epoch number', fontsize = 11)\n",
    "    axs[0].set_ylabel('MSE loss', fontsize = 11)\n",
    "    axs[0].set_title('Normalized training data', fontsize = 13)\n",
    "\n",
    "    \n",
    "    ax_2 = axs[0].twinx()\n",
    "    line2, = ax_2.plot(epoch_numbers, train_class_losses, label = \"Binary cross-entropy loss\", color = 'b')\n",
    "    ax_2.set_ylim(0,max(train_class_losses)+0.1)\n",
    "    ax_2.set_ylabel('Binary cross-entropy loss', fontsize = 11)\n",
    "    ax_2.tick_params(axis='y',labelsize=8)\n",
    "    lines = [line1, line2] \n",
    "    labels = [line.get_label() for line in lines]\n",
    "    axs[0].legend(lines, labels, loc='upper right') \n",
    "\n",
    "    \n",
    "    formatter = mticker.ScalarFormatter(useMathText=True)\n",
    "    formatter.set_scientific(False)\n",
    "\n",
    "    axs[1].plot(epoch_numbers, vali_mse_losses, label = \"Validation set\", color = 'y')\n",
    "    axs[1].plot(epoch_numbers, test_mse_losses, label = \"Test set\", color = 'b')\n",
    "    axs[1].set_ylim(0,max(max(vali_mse_losses), max(test_mse_losses)) + 1000)\n",
    "    axs[1].tick_params(axis='both',labelsize=8)\n",
    "    axs[1].yaxis.set_major_formatter(formatter)\n",
    "    axs[1].yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, pos: f'{x / 1000:.0f}'))\n",
    "    axs[1].set_xlabel('Epoch number', fontsize = 11)\n",
    "    axs[1].set_ylabel('Vehicles * day (*10³)', fontsize = 11)\n",
    "    axs[1].set_title('MSE value (inverse-normalized data)', fontsize = 13)    \n",
    "    axs[1].legend(loc='upper right')\n",
    "    \n",
    "    plt.show()\n",
    "    plt.pause(0.1)\n",
    "    \n",
    "        \n",
    "plt.ioff()  #关闭交互模式\n",
    "plt.show()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb084f-667d-4ee4-be43-24bd7b6878c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
