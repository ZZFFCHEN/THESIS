{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94cc25ff-e681-4d4b-88c0-eaac11cd70f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import GCNConv, ChebConv\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch.nn import init\n",
    "import pdb\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch_geometric.utils import to_dense_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "319627b8-9d48-4363-a392-d08b9b1656f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e037b565-a94a-4f40-9aeb-a775c3185743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, file_path):\n",
    "    with open(file_path , 'wb') as f:\n",
    "        pickle.dump(data,f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8352b57-6759-40a1-bb57-67a8d2e36d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGraphDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_list (list): 包含多个 CustomGraphData 对象的列表\n",
    "        \"\"\"\n",
    "        super(CustomGraphDataset, self).__init__()\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.data_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc780de9-b736-48b8-8e79-16c6194a94cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGraphData(Data):\n",
    "    def __init__(self, x=None, edge_index=None, edge_attr=None, y=None, class_label=None, min_vals=None, max_vals=None, coord_list=None):\n",
    "        # 正确调用父类的构造函数\n",
    "        super(CustomGraphData, self).__init__(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "        \n",
    "        # 添加自定义属性\n",
    "        self.class_label = class_label\n",
    "        self.min_vals = min_vals\n",
    "        self.max_vals = max_vals\n",
    "        self.coord_list = coord_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf803a02-3d6f-43c4-ac1d-5079f860b9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_Model(nn.Module):\n",
    "    def __init__(self, input_dimen, hidden_dimen, output_dimen, drop_out=True, K= 4, layer_num = 1):\n",
    "        super(GCN_Model, self).__init__()\n",
    "        self.conv1 = ChebConv(in_channels=input_dimen, out_channels=hidden_dimen, K=K)\n",
    "        self.fc = nn.Linear(hidden_dimen, output_dimen)\n",
    "        self.acti_func = nn.ReLU()\n",
    "        #-----------------------------------------------------------------------\n",
    "        self.drop_out = drop_out\n",
    "        self.output_dimen = output_dimen\n",
    "\n",
    "        \n",
    "        for m in self.children():\n",
    "            #if isinstance(m, GCNConv):\n",
    "                #m.lin.weight = init.kaiming_uniform_(m.lin.weight, a=0, nonlinearity='leaky_relu')\n",
    "                #if m.lin.bias is not None:\n",
    "                    #m.lin.bias = nn.init.constant_(m.lin.bias, 0)\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = init.constant_(m.bias.data, 0.0)       \n",
    "                    \n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # ----------->TOTAL NODE_NUMBER, 28\n",
    "        # x: TOTAL NODE_NUMBER, hidden_dimen\n",
    "        \n",
    "        gcn_out = self.conv1(x, edge_index, edge_attr)\n",
    "        if self.drop_out:\n",
    "            gcn_out = F.dropout(gcn_out, training=self.training, p=0.2)\n",
    "        gcn_out = self.acti_func(gcn_out)\n",
    "        gcn_out = self.fc(gcn_out)\n",
    "        if self.drop_out:\n",
    "            gcn_out = F.dropout(gcn_out, training=self.training, p=0.2)\n",
    "        gcn_out = self.acti_func(gcn_out)\n",
    "\n",
    "        \n",
    "        return gcn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f219fd7a-58ba-4e63-9bac-8baf5f80980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self, output_dimen, hidden_channels_1, hidden_channels_2, drop_out = True):\n",
    "        super(CNN_1D, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = output_dimen, out_channels = hidden_channels_1, kernel_size=7,  stride=1, padding=0),\n",
    "            nn.LeakyReLU(), #len 22 days\n",
    "            nn.Conv1d(in_channels = hidden_channels_1, out_channels = hidden_channels_2, kernel_size = 6, stride = 3, padding=1),\n",
    "            nn.LeakyReLU())  #len: 7 days\n",
    "        self.acti_func = nn.LeakyReLU()\n",
    "        self.drop_out = drop_out \n",
    "\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d) or isinstance(m, nn.ConvTranspose1d) or isinstance(m, nn.Linear):\n",
    "                # 对卷积层使用 Kaiming 正态初始化\n",
    "                nn.init.kaiming_uniform_(m.weight.data, nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "       \n",
    "        \n",
    "#PGNN处理后数据形状 : TOTAL NODE_NUMBER, hidden_dimen, 28 ------> (Batch_num * Max_node_num), output_dimen, 28\n",
    "    \n",
    "    def forward(self, x):\n",
    "      \n",
    "        #output shape: TOTAL NODE_NUMBER, hidden_channels_2, 7\n",
    "        out = self.conv_layer(x) \n",
    "        #SHAPE: TOTAL NODE_NUMBER, 7, hidden_channels_2 \n",
    "        out = out.permute(0,2,1)\n",
    " \n",
    "        #----------------------------------------------------\n",
    "  \n",
    "       \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "472349bb-c9c0-4811-8b2a-5acfa8614986",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, output_seq_len, drop_out = True):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers = 1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size * output_seq_len)\n",
    "        self.output_seq_len = output_seq_len\n",
    "        self.output_size = output_size\n",
    "        self.acti_func = nn.ReLU()\n",
    "        self.drop_out = drop_out\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = init.constant_(m.bias.data, 0.0) \n",
    "          \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch_size * NODE_NUM), 28, output_dimen\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        if self.drop_out:\n",
    "            lstm_out = F.dropout(lstm_out, training=self.training, p=0.2)\n",
    "        lstm_out = self.acti_func(lstm_out)\n",
    "        # lstm_out shape: (Batch_num * Max_node_num), hidden_size \n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        out = self.fc(lstm_out)  \n",
    "        out= self.acti_func(out)\n",
    "        #out shape: Batch_num, Max_node_num, output_seq_len\n",
    "\n",
    "        #Batch_num * Max_node_num), output_seq_len\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da471026-c866-44f0-80cc-4fc690864a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, output_seq_len, drop_out = True):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size * output_seq_len)\n",
    "        self.output_seq_len = output_seq_len\n",
    "        self.output_size = output_size\n",
    "        self.acti_func = nn.ReLU()\n",
    "        self.drop_out = drop_out\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = init.constant_(m.bias.data, 0.0) \n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch_size * NODE_NUM), 28, output_dimen\n",
    "        lstm_out, _ = self.rnn(x)\n",
    "        if self.drop_out:\n",
    "            lstm_out = F.dropout(lstm_out, training=self.training, p=0.2)\n",
    "        lstm_out = self.acti_func(lstm_out)\n",
    "        # lstm_out shape: (Batch_num * Max_node_num), hidden_size \n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        out = self.fc(lstm_out)  \n",
    "        out= self.acti_func(out)\n",
    "        #out shape: Batch_num, Max_node_num, output_seq_len\n",
    "\n",
    "        #Batch_num * Max_node_num), output_seq_len\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e16b0d13-2795-4924-96ce-b3f19cb5cba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.load(\"D:/ThesisData/processed data/TargetDomain/NEW/train_GCN_data_14days.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b593efd-7cb5-4fa6-b6ab-34bf22f806bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vali_barcelona = torch.load(\"D:/ThesisData/processed data/TargetDomain/NEW/Barcelona_vali_GCN_data_14days.h5\")\n",
    "test_barcelona = torch.load(\"D:/ThesisData/processed data/TargetDomain/NEW/Barcelona_test_GCN_data_14days.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30e06abf-ae6d-4c86-9f73-d817198d188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 14\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "Barcelona_vali_loader = DataLoader(vali_barcelona, batch_size=batch_size, shuffle=False)\n",
    "Barcelona_test_loader = DataLoader(test_barcelona, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85d107be-e87f-4a5a-b3c0-3d050a87ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GCN\n",
    "input_dimen, hidden_dimen, output_dimen = 1, 128, 64\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "output_seq_len = 14  # 预测14天\n",
    "output_size = 1\n",
    "#LSTM\n",
    "input_size, hidden_size = output_dimen, 128\n",
    "#-------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49a5b9d6-db19-4874-aa92-f59d04ded20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm = LSTMModel(input_size=input_size, hidden_size=hidden_size, output_size=output_size, output_seq_len=output_seq_len)\n",
    "lstm = RNNModel(input_size=input_size, hidden_size=hidden_size, output_size=output_size, output_seq_len=output_seq_len)\n",
    "lstm_optimizer = optim.NAdam(lstm.parameters(), lr=3e-4, weight_decay=1e-5)  #4e-4\n",
    "lstm_scheduler = optim.lr_scheduler.StepLR(lstm_optimizer, step_size=12, gamma=0.6)\n",
    "#------------------------------------------------------------------------------------------\n",
    "gcn = GCN_Model(input_dimen, hidden_dimen, output_dimen)\n",
    "gcn_optimizer = optim.NAdam(gcn.parameters(), lr=4e-4, weight_decay=1e-5)  #5e-4\n",
    "gcn_scheduler = optim.lr_scheduler.StepLR(gcn_optimizer, step_size=12, gamma=0.55)\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "criterion_MSE = nn.MSELoss(reduction='none')  # 使用均方误差作为损失函数\n",
    "criterion_MAE = nn.L1Loss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78bf200a-3acb-46dc-a1f5-232ac72d0585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('D:/ThesisData/processed data/ModelPara/source_Cheb_rnn_14days.pth')\n",
    "lstm.load_state_dict(checkpoint['lstm_state_dict'])\n",
    "gcn.load_state_dict(checkpoint['gcn_state_dict'])\n",
    "#gcn_optimizer.load_state_dict(checkpoint['gcn_optimizer_state_dict'])\n",
    "#lstm_optimizer.load_state_dict(checkpoint['lstm_optimizer_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac11e88f-384a-4e36-a53e-9e8e336fb948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c367bde7-a2d8-4574-9bf1-71a52eccf00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num= 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6466556c-1f59-4c08-b5e9-33c55c77399d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, Train Set Inversed Values: MSE=2580.6, MAE=39.2\n",
      "----Vali Barcelona: MSE=1245.5, MAE=28.1\n",
      "----Test Barcelona: MSE=1649.2, MAE=30.5\n",
      "--------------------------------------------\n",
      "epoch 2, Train Set Inversed Values: MSE=2622.0, MAE=39.7\n",
      "----Vali Barcelona: MSE=1259.8, MAE=28.2\n",
      "----Test Barcelona: MSE=1699.0, MAE=30.8\n",
      "--------------------------------------------\n",
      "epoch 3, Train Set Inversed Values: MSE=2550.0, MAE=39.0\n",
      "----Vali Barcelona: MSE=1268.0, MAE=28.6\n",
      "----Test Barcelona: MSE=1633.2, MAE=30.6\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# SHAPE: TOTAL NODE_NUMBER, 14\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#----------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     21\u001b[0m gcn_out \u001b[38;5;241m=\u001b[39m gcn_out\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m lstm_out \u001b[38;5;241m=\u001b[39m \u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgcn_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#----------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#-----------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     27\u001b[0m batch_info \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mbatch\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\Cuda_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\Cuda_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 22\u001b[0m, in \u001b[0;36mRNNModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(x)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_out:\n\u001b[1;32m---> 22\u001b[0m     lstm_out \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m lstm_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macti_func(lstm_out)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# lstm_out shape: (Batch_num * Max_node_num), hidden_size \u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\Cuda_env\\lib\\site-packages\\torch\\nn\\functional.py:1268\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for h in range(epoch_num):\n",
    "    lstm.train()\n",
    "    gcn.train()\n",
    "\n",
    "    \n",
    "    for batch in train_loader:\n",
    "\n",
    "        x, edge_index, edge_attr, y = batch.x, batch.edge_index, batch.edge_attr, batch.y\n",
    "        batch_min, batch_max = batch.min_vals, batch.max_vals\n",
    "        #--------------------------------------------------------\n",
    "\n",
    "        for i in range(x.shape[-1]):\n",
    "            if i == 0:\n",
    "                # SHAPE: TOTAL NODE_NUMBER, hidden_dimen, 1\n",
    "                gcn_out = gcn(x[:, i].unsqueeze(-1), edge_index, edge_attr).unsqueeze(-1)\n",
    "            else:\n",
    "                gcn_out = torch.cat((gcn_out, gcn(x[:, i].unsqueeze(-1), edge_index, edge_attr).unsqueeze(-1)), dim=-1)\n",
    "\n",
    "        # SHAPE: TOTAL NODE_NUMBER, 14\n",
    "        #----------------------------------------------------------\n",
    "        gcn_out = gcn_out.permute(0,2,1)\n",
    "        lstm_out = lstm(gcn_out)\n",
    "        #----------------------------------------------------------\n",
    "        \n",
    "\n",
    "        #-----------------------------------------------------------\n",
    "        batch_info = batch.batch\n",
    "        #SHAPE: batch_size, max_num_nodes, 14\n",
    "        batch_out, mask = to_dense_batch(lstm_out, batch_info)\n",
    "        #------------------------------------------------------------\n",
    "        \n",
    "      \n",
    "        # batch_labels SHAPE: batch_size, max_num_nodes, 14\n",
    "        batch_labels, mask = to_dense_batch(y, batch_info)\n",
    "        # mask SHAPE: batch_size, max_num_nodes\n",
    "        batch_masks = mask.unsqueeze(-1).repeat(1, 1, 14)\n",
    "        #-----------------------------------------------------------\n",
    "\n",
    "        \n",
    "        loss_mse = criterion_MSE(batch_out, batch_labels) * batch_masks\n",
    "        loss_mae = criterion_MAE(batch_out, batch_labels) * batch_masks\n",
    "        loss_mse = loss_mse.sum() / batch_masks.sum()\n",
    "        loss_mae = loss_mae.sum() / batch_masks.sum()    \n",
    "\n",
    "        lstm_optimizer.zero_grad()\n",
    "        gcn_optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        loss_mse.backward()\n",
    "\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(lstm.parameters(), 1.0)\n",
    "        torch.nn.utils.clip_grad_norm_(gcn.parameters(), 1.0)\n",
    "\n",
    "        lstm_optimizer.step()\n",
    "        gcn_optimizer.step()\n",
    "\n",
    "        del loss_mse\n",
    "        del loss_mae\n",
    "        #----------------------------------------------------------------------\n",
    "        \n",
    "        inverse_outputs = batch_out * (batch_max - batch_min) + batch_min\n",
    "        inverse_labels = batch_labels * (batch_max - batch_min) + batch_min\n",
    "\n",
    "        loss_mse = criterion_MSE(inverse_outputs, inverse_labels) * batch_masks\n",
    "        loss_mae = criterion_MAE(inverse_outputs, inverse_labels) * batch_masks\n",
    "        loss_mse = loss_mse.sum() / batch_masks.sum()\n",
    "        loss_mae = loss_mae.sum() / batch_masks.sum() \n",
    "    \n",
    "    \n",
    "    print(f\"epoch {h + 1}, Train Set Inversed Values: MSE={loss_mse.item():.1f}, MAE={loss_mae.item():.1f}\")\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------\n",
    "    lstm.eval()\n",
    "    gcn.eval()  \n",
    "\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch in Barcelona_vali_loader:\n",
    "            x, edge_index, edge_attr, y = batch.x, batch.edge_index, batch.edge_attr, batch.y\n",
    "            batch_min, batch_max = batch.min_vals, batch.max_vals\n",
    "            #--------------------------------------------------------\n",
    "\n",
    "            for i in range(x.shape[-1]):\n",
    "                if i == 0:\n",
    "                    # SHAPE: TOTAL NODE_NUMBER, hidden_dimen, 1\n",
    "                    gcn_out = gcn(x[:, i].unsqueeze(-1), edge_index, edge_attr).unsqueeze(-1)\n",
    "                else:\n",
    "                    gcn_out = torch.cat((gcn_out, gcn(x[:, i].unsqueeze(-1), edge_index, edge_attr).unsqueeze(-1)), dim=-1)\n",
    "    \n",
    "            # SHAPE: TOTAL NODE_NUMBER, hidden_dimen, 28\n",
    "            #----------------------------------------------------------\n",
    "            gcn_out = gcn_out.permute(0,2,1)\n",
    "            lstm_out = lstm(gcn_out)\n",
    "            #----------------------------------------------------------\n",
    "        \n",
    "            \n",
    "            #-----------------------------------------------------------\n",
    "            batch_info = batch.batch\n",
    "            #SHAPE: Batch_size, NODE_NUM, 7\n",
    "            batch_out, mask = to_dense_batch(lstm_out, batch_info)\n",
    "            #------------------------------------------------------------\n",
    "        \n",
    "      \n",
    "            # batch_labels SHAPE: batch_size, max_num_nodes, 7\n",
    "            batch_labels, mask = to_dense_batch(y, batch_info)\n",
    "            # mask SHAPE: batch_size, max_num_nodes\n",
    "            batch_masks = mask.unsqueeze(-1).repeat(1, 1, 14)     \n",
    "\n",
    "            \n",
    "            Barcelona_vali_outputs = batch_out * (batch_max - batch_min) + batch_min\n",
    "            Barcelona_vali_labels = batch_labels * (batch_max - batch_min) + batch_min\n",
    "\n",
    "            loss_mse = criterion_MSE(Barcelona_vali_outputs, Barcelona_vali_labels) * batch_masks\n",
    "            loss_mae = criterion_MAE(Barcelona_vali_outputs, Barcelona_vali_labels) * batch_masks\n",
    "            loss_mse = loss_mse.sum() / batch_masks.sum()\n",
    "            loss_mae = loss_mae.sum() / batch_masks.sum()  \n",
    "\n",
    "        print(f\"----Vali Barcelona: MSE={loss_mse.item():.1f}, MAE={loss_mae.item():.1f}\")\n",
    "\n",
    "\n",
    "#--------------------------------------------------\n",
    "        for batch in Barcelona_test_loader:\n",
    "            x, edge_index, edge_attr, y = batch.x, batch.edge_index, batch.edge_attr, batch.y\n",
    "            batch_min, batch_max = batch.min_vals, batch.max_vals\n",
    "            #--------------------------------------------------------\n",
    "\n",
    "            for i in range(x.shape[-1]):\n",
    "                if i == 0:\n",
    "                    # SHAPE: TOTAL NODE_NUMBER, hidden_dimen, 1\n",
    "                    gcn_out = gcn(x[:, i].unsqueeze(-1), edge_index, edge_attr).unsqueeze(-1)\n",
    "                else:\n",
    "                    gcn_out = torch.cat((gcn_out, gcn(x[:, i].unsqueeze(-1), edge_index, edge_attr).unsqueeze(-1)), dim=-1)\n",
    "    \n",
    "            # SHAPE: TOTAL NODE_NUMBER, hidden_dimen, 28\n",
    "            #----------------------------------------------------------\n",
    "            gcn_out = gcn_out.permute(0,2,1)\n",
    "            lstm_out = lstm(gcn_out)\n",
    "            #----------------------------------------------------------\n",
    "        \n",
    "            \n",
    "            #-----------------------------------------------------------\n",
    "            batch_info = batch.batch\n",
    "            #SHAPE: Batch_size, NODE_NUM, 7\n",
    "            batch_out, mask = to_dense_batch(lstm_out, batch_info)\n",
    "            #------------------------------------------------------------\n",
    "        \n",
    "      \n",
    "            # batch_labels SHAPE: batch_size, max_num_nodes, 7\n",
    "            batch_labels, mask = to_dense_batch(y, batch_info)\n",
    "            # mask SHAPE: batch_size, max_num_nodes\n",
    "            batch_masks = mask.unsqueeze(-1).repeat(1, 1, 14)     \n",
    "\n",
    "            \n",
    "            Barcelona_test_outputs = batch_out * (batch_max - batch_min) + batch_min\n",
    "            Barcelona_test_labels = batch_labels * (batch_max - batch_min) + batch_min\n",
    "\n",
    "            loss_mse = criterion_MSE(Barcelona_test_outputs, Barcelona_test_labels) * batch_masks\n",
    "            loss_mae = criterion_MAE(Barcelona_test_outputs, Barcelona_test_labels) * batch_masks\n",
    "            loss_mse = loss_mse.sum() / batch_masks.sum()\n",
    "            loss_mae = loss_mae.sum() / batch_masks.sum()  \n",
    "\n",
    "        print(f\"----Test Barcelona: MSE={loss_mse.item():.1f}, MAE={loss_mae.item():.1f}\")\n",
    "        #------------------------------------------------------------\n",
    "        \n",
    "\n",
    "\n",
    "    if h == epoch_num-1:\n",
    "        cheb_rnn_results = {\"Barcelona_vali\": Barcelona_vali_outputs.detach().numpy(), \"Barcelona_vali_label\": Barcelona_vali_labels.detach().numpy(), \n",
    "                            \"Barcelona_test\": Barcelona_test_outputs.detach().numpy(), \"Barcelona_test_label\": Barcelona_test_labels.detach().numpy(),\n",
    "                            \"Barcelona_node_num\": 1273}\n",
    "\n",
    "        \n",
    "        save_data(cheb_rnn_results,\"D:/ThesisData/processed data/TargetDomain/NEW/results/cheb_lstm_results.h5\") \n",
    "        break\n",
    "        \n",
    "    gcn_optimizer.step()\n",
    "    lstm_optimizer.step()\n",
    "    \n",
    "    \n",
    "    print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e252d957-108c-4dd6-ae10-2f53b5481f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb57d0-a91c-4a0e-bf88-facd40bcacc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
